Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Duplicate),Outward issue link (Reference),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Tags),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
SingularValueDecomposition constructor blocks with NaN in the matrix,MATH-947,12637012,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,mafa,mafa,14/Mar/13 13:00,07/Apr/13 09:20,20/Mar/20 20:32,15/Mar/13 12:37,3.0,,,,,,,3.2,,,0,,,,,,"//Create coefficient (A) Matrix
        RealMatrix coefficients =
            new Array2DRowRealMatrix(coeffs);

               SingularValueDecomposition svd = new SingularValueDecomposition(coefficients);

//When coeffs is a 2x2 matrix with all elements Nan, the constructor blocks indefinitely /w 100% CPU usage","jdk1.7.0_13 32bit, Netbeans 7.2.1, Win7 x64",,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-03-14 13:45:21.01,,,false,,,,,,,,,,,,,,,,,317504,,,Sun Apr 07 09:20:41 UTC 2013,,,,,,,"0|i1isan:",317845,,,,,,,,,,,,,,,,"14/Mar/13 13:45;erans;Obviously, you can't hope to get a useful result from such an input.

There is probably a loop with an exit condition that's never satisfied...
One way to avoid it would be to check the validity of the input. This could be done either in Commons Math or in the user's code.","15/Mar/13 12:37;luc;Fixed in subversion repository as of r1456931.

Thanks for the report.","07/Apr/13 09:20;luc;Closing issue as version 3.2 has been released on 2013-04-06.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Constructor parameter not used,MATH-572,12506688,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,aploese,aploese,10/May/11 08:07,24/Mar/12 16:16,20/Mar/20 20:33,10/May/11 18:09,3.0,,,,,,,3.0,,,0,,,,,,"the constructor public ArrayFieldVector(Field<T> field, T[] v1, T[] v2)
sets this
""this.field = data[0].getField();""
in the fast line...

""this.field = field;""

would be right - field was explicitly provided.
",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-05-10 18:09:45.165,,,false,,,,,,,,,,,,,,,,,150671,,,Tue May 10 18:09:45 UTC 2011,,,,,,,"0|i0rtov:",160454,,,,,,,,,,,,,,,,"10/May/11 18:09;luc;Fixed in subversion repository as of r1101575.

Thanks for reporting the issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix and then deprecate isSupportXxxInclusive in RealDistribution interface,MATH-859,12606729,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,08/Sep/12 18:15,24/Feb/15 22:09,20/Mar/20 20:33,24/Feb/15 22:09,3.0,,,,,,,4.0,,,0,,,,,,"The conclusion from [1] was never implemented. We should deprecate these
properties from the RealDistribution interface, but since removal
will have to wait until 4.0, we should agree on a precise
definition and fix the code to match it in the mean time.

The definition that I propose is that isSupportXxxInclusive means
that when the density function is applied to the upper or lower
bound of support returned by getSupportXxxBound, a finite (i.e. not
infinite), not NaN value is returned.

[1] http://markmail.org/message/dxuxh7eybl7xejde
",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-03-10 17:14:19.246,,,false,,,,,,,,,,,,,,,,,241570,,,Tue Feb 24 22:09:34 UTC 2015,,,,,,,"0|i028pr:",11013,,,,,,,,,,,,,,,,"08/Sep/12 22:17;psteitz;Fixes and deprecation done in r1382380.","10/Mar/13 17:14;tn;Can this issue be resolved?
It is already contained in the 3.0 release notes.","10/Mar/13 17:21;psteitz;The methods can't actually be removed until 4.0, so this should stay open until we cut 4.0 with them removed.","10/Mar/13 17:30;tn;But I thought it is common practice to only remove deprecated methods/classes in the next major release, and there is no need to keep the corresponding issue open till then?","11/Mar/13 04:51;psteitz;Personally, I think it is better to put fix version at the release that will actually resolve the issue by removing the methods, as it is in this case (fix version 4.0).  When we decide to remove or refactor something, I think it is best to keep the issue open until it is fully resolved, which to me means deprecated methods have been removed.  This helps us remember to actually do it when we cut the major release and jogs memory when prepping that release what exactly we had in mind / why methods were deprecated.","24/Feb/15 22:09;tn;Removed deprecated methods in commit ece7c6fc67c0d584f4884c5b17ddf491a397fdfe.",,,,,,,,,,,,,,,,,,,,,,,,,
SparseRealVectorTest.testMap and testMapToSelf fail because zero entries lose their sign,MATH-821,12598560,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,celestin,celestin,12/Jul/12 14:01,19/May/14 15:13,20/Mar/20 20:33,20/Feb/14 16:17,3.0,,,,,,,3.3,,,0,linear,sparse,,,,"Mapping {{Inverse}} to an {{OpenMapRealVector}} can lead to wrong answers, because {{1.0 / 0.0}} should return {{+/-Infinity}} depending on the sign of the zero entry. Since the sign is lost in {{OpenMapRealVector}}, the answer must be wrong if the entry is truly {{-0.0}}.

This is a difficult bug, because it potentially affects any function passed to {{OpenMapRealVector.map()}} or {{mapToSelf()}}. I would recommend we relax the requirements in the unit tests of this class, and make people aware of the issue in the class documentation.",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-07-13 11:22:56.681,,,false,,,,,,,,,,,,,,,,,241587,,,Mon May 19 15:13:35 UTC 2014,,,,,,,"0|i028tj:",11030,,,,,,,,,,,,,,,,"13/Jul/12 11:22;erans;In revision 1361164, I disabled the two unit tests that demonstrate this bug.
When the bug is fixed, the tests should be made active again.
","22/Oct/12 13:51;erans;I guess that this will be automatically fixed when the sparse vector implementation is deleted. :)
","20/Feb/14 16:17;luc;Unit tests have been relaxed.

Comments have been added in the javadoc for SparseVector interface and OpenMapRealVector class.","19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Bugs in RealVector.ebeMultiply(RealVector) and ebeDivide(RealVector),MATH-803,12560058,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,celestin,celestin,09/Jun/12 18:51,19/May/14 15:13,20/Mar/20 20:33,20/Feb/14 15:58,3.0,,,,,,,3.3,,,0,,,,,,"{{OpenMapRealVector.ebeMultiply(RealVector)}} and {{OpenMapRealVector.ebeDivide(RealVector)}} return wrong values when one entry of the specified {{RealVector}} is nan or infinity. The bug is easy to understand. Here is the current implementation of {{ebeMultiply}}

{code:java}
    public OpenMapRealVector ebeMultiply(RealVector v) {
        checkVectorDimensions(v.getDimension());
        OpenMapRealVector res = new OpenMapRealVector(this);
        Iterator iter = entries.iterator();
        while (iter.hasNext()) {
            iter.advance();
            res.setEntry(iter.key(), iter.value() * v.getEntry(iter.key()));
        }
        return res;
    }
{code}

The assumption is that for any double {{x}}, {{x * 0d == 0d}} holds, which is not true. The bug is easy enough to identify, but more complex to solve. The only solution I can come up with is to loop through *all* entries of v (instead of those entries which correspond to non-zero entries of this). I'm afraid about performance losses.

",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-06-10 14:01:18.224,,,false,,,,,,,,,,,,,,,,,241596,,,Mon May 19 15:13:24 UTC 2014,,,,,,,"0|i028vj:",11039,,,,,,,,,,,,,,,,"09/Jun/12 19:03;celestin;In {{r1348485}}, {{RealVectorAbstractTest}} include unit tests illustrating this bug.","10/Jun/12 14:01;luc;Perhaps we should use a post-processing branch to handle infinite or spacial values:

{code}
// current implementation looping over non-zero instance elements goes here

if (v.isNaN() || v.isInfinite()) {
  // post-processing loop, to handle 0 * infinity and 0 * NaN cases
  for (int i = 0; i < v.getDimension(); ++v) {
    if (Double.isInfinite(v.getElement(i)) || Double.isNaN(v.getElement(i)) {
       res.setEntry(i, Double.NaN);
    }
  }
}
{code}

This could be fast only if isNaN() and isInfinite() results are cached and the same vector is reused, otherwise the outer if statement should be removed and the post-processing should be done in all cases.","11/Jun/12 05:12;celestin;I'm going to implement the suggested post-processing for the time being; this should solve the bug. Caching of isNaN() and isInfinite() is postponed.","11/Jun/12 05:43;celestin;I think this does not work for {{ebeDivide(RealVector)}}. In this case, I suggest to revert to naive implementation (loop through all entries).","22/Jun/12 06:05;celestin;Changed the title of the ticket, since the scope of this bug is much broader. Indeed, it affects {{RealVector.ebeMultiply(RealVector)}} and {{RealVector.ebeDivide(RealVector)}} as soon as the {{RealVector}} passed as a parameter is sparse, see examples below
# for {{ebeMultiply()}}
{code:java}
final RealVector v1 = new ArrayRealVector(new double[] { 1d });
final RealVector v2 = new OpenMapRealVector(new double[] { -0d });
final RealVector w = v1.ebeMultiply(v2);
System.out.println(1d / w.getEntry(0));
{code}
prints {{Infinity}}, instead of {{-Infinity}} (because the sign is lost in {{v2}}). This means that {{w}} holds {{+0d}} instead of {{-0d}}.
# for {{ebeDivide()}}
{code:java}
final RealVector v1 = new ArrayRealVector(new double[] { 1d });
final RealVector v2 = new OpenMapRealVector(new double[] { -0d });
final RealVector w = v1.ebeDivide(v2);
System.out.println(w.getEntry(0));
{code}
prints {{Infinity}}, instead of {{-Infinity}}.
","22/Jun/12 06:09;celestin;According to this [thread|http://markmail.org/thread/4evd6dcyrh2yc2bs], {{ebeMultiply}} and {{ebeDivide}} are deprecated.","22/Jun/12 07:08;celestin;Methods are deprecated in {{r1352782}}, and unit tests are skipped (they are kept for reference). Issue is to remain open until the methods are removed in version 4.0.","20/Feb/14 15:58;luc;Methods undeprecated in r1570246.
The test cases have been adapted, and the fact we force 0 * x = 0 even for NaNs and infinities is considered acceptable now, as it is similar to standard practive for sparse linear algebra libraries.","19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.",,,,,,,,,,,,,,,,,,,,,,
NPE when calling SubLine.intersection() with non-intersecting lines,MATH-988,12650507,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,andreashuber,andreashuber,02/Jun/13 13:06,19/May/14 15:13,20/Mar/20 20:33,03/Jun/13 07:17,3.0,3.1,3.1.1,3.2,,,,3.3,,,0,,,,,,"When calling SubLine.intersection() with two lines that not intersect, then a NullPointerException is thrown in Line.toSubSpace(). This bug is in the twod and threed implementations.

The attached patch fixes both implementations and adds the required test cases.

",,,,,,,,,,,,,,"02/Jun/13 13:07;andreashuber;SubLineIntersection.patch;https://issues.apache.org/jira/secure/attachment/12585757/SubLineIntersection.patch",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2013-06-03 07:17:01.331,,,false,,,,,,,,,,,,,,,,,330834,,,Mon May 19 15:13:20 UTC 2014,,,,,,,"0|i1l2qf:",331167,,,,,,,,,,,,,,,,"02/Jun/13 13:07;andreashuber;This patch fixes both implementations and adds test cases.","03/Jun/13 07:17;luc;Fixed in subversion repository as of r1488866.
Patch applied with minor whitespace changes.

Thanks for the report and for the patch.","19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
GammaDistribution returns infinite density when cdf = 1,MATH-888,12614053,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,zhochberg,zhochberg,30/Oct/12 17:08,02/May/13 02:29,20/Mar/20 20:33,23/Nov/12 11:42,3.0,,,,,,,3.1,,,0,,,,,,"When GammaDistribution.cumulativeProbability = 1, then GammaDistribution.density may return Infinite - but it should return 0.
Here is sample java code which demonstrates this issue, with results. Note that when pass the point at which cumulativeProbability = 1, the density stays > 0, though very small (pdf should be 0 when cdf = 1) - but as continue, then cdf stays 1, but pdf becomes infinite. This is seen most clearly in the second set of data below.


package ApacheTester;

import org.apache.commons.math3.distribution.GammaDistribution;

public class GammaDistributionTester
{
    public static void main(String[] args)
    {
        double mean = 10.118627813856065;
        double std  = 0.8934747186204953;
        double alpha = (mean*mean)/(std*std);
        double beta = (std*std)/mean;
        System.out.println(""alpha: "" + alpha);
        System.out.println(""beta: "" + beta);
        
        GammaDistribution gd = new GammaDistribution(alpha, beta);
        
        for (int i=0; i<25; i++)
        {
            double pdf = gd.density(i);
            double cdf = gd.cumulativeProbability(i);
            System.out.println(""i="" + i + ""; pdf="" + pdf + ""; cdf="" + cdf);            
        }
        System.out.println();
        
        for (int i=0; i<20; i++)
        {
            double x = 19.0 + (.1 * i);
            double pdf = gd.density(x);
            double cdf = gd.cumulativeProbability(x);
            System.out.println(""x="" + x + ""; pdf="" + pdf + ""; cdf="" + cdf);            
        }
    }
}

Here's results of running above test:

alpha: 128.25629996917283
beta: 0.07889380729280462
i=0; pdf=0.0; cdf=0.0
i=1; pdf=8.663477060371509E-79; cdf=7.488555717749243E-81
i=2; pdf=5.5062704634862175E-46; cdf=1.0676382015037016E-47
i=3; pdf=4.413483488439264E-29; cdf=1.4607150065658644E-30
i=4; pdf=1.0945225959157887E-18; cdf=5.599255664602861E-20
i=5; pdf=7.35929647596093E-12; cdf=5.590805261026197E-13
i=6; pdf=2.7437881760964153E-7; cdf=3.0732983691238435E-8
i=7; pdf=2.8376132639470645E-4; cdf=4.7826733420766874E-5
i=8; pdf=0.021281179160656388; cdf=0.005695853765034336
i=9; pdf=0.2151187723352627; cdf=0.10175065472250033
i=10; pdf=0.44751658755472723; cdf=0.4586536845007119
i=11; pdf=0.259114059558244; cdf=0.8385256983930766
i=12; pdf=0.05218255030958943; cdf=0.9781383380332821
i=13; pdf=0.004329365023697574; cdf=0.9986058018679428
i=14; pdf=1.6878342979402664E-4; cdf=0.9999549514385939
i=15; pdf=3.431947471368873E-6; cdf=0.9999992047520807
i=16; pdf=3.9588967227014906E-8; cdf=0.9999999917873332
i=17; pdf=2.7752325350928474E-10; cdf=0.9999999999473412
i=18; pdf=1.2515801802459923E-12; cdf=0.9999999999997794
i=19; pdf=3.808710263354571E-15; cdf=0.9999999999999993
i=20; pdf=8.143130333262255E-18; cdf=1.0
i=21; pdf=Infinity; cdf=1.0
i=22; pdf=Infinity; cdf=1.0
i=23; pdf=Infinity; cdf=1.0
i=24; pdf=Infinity; cdf=1.0

x=19.0; pdf=3.808710263354571E-15; cdf=0.9999999999999993
x=19.1; pdf=2.0912827149653448E-15; cdf=0.9999999999999997
x=19.2; pdf=1.144280754920214E-15; cdf=0.9999999999999998
x=19.3; pdf=6.239549206021272E-16; cdf=0.9999999999999999
x=19.4; pdf=3.3907057624200243E-16; cdf=1.0
x=19.5; pdf=1.8363629496980252E-16; cdf=1.0
x=19.6; pdf=9.912278358035803E-17; cdf=1.0
x=19.7; pdf=5.332732516340122E-17; cdf=1.0
x=19.8; pdf=2.8595785047900534E-17; cdf=1.0
x=19.9; pdf=1.5284263208928833E-17; cdf=1.0
x=20.0; pdf=8.143130333262255E-18; cdf=1.0
x=20.1; pdf=4.324705899342647E-18; cdf=1.0
x=20.2; pdf=2.2895693394216285E-18; cdf=1.0
x=20.3; pdf=1.2083606178165914E-18; cdf=1.0
x=20.4; pdf=6.357672846520565E-19; cdf=1.0
x=20.5; pdf=Infinity; cdf=1.0
x=20.6; pdf=Infinity; cdf=1.0
x=20.7; pdf=Infinity; cdf=1.0
x=20.8; pdf=Infinity; cdf=1.0
x=20.9; pdf=Infinity; cdf=1.0

",Windows 7,,,,,,,,,,,,MATH-849,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-11-09 20:59:20.724,,,false,,,,,,,,,,,,,,,,,253177,,,Fri Nov 23 11:42:39 UTC 2012,,,,,,,"0|i0dc8n:",75912,,,,,,,,,,,,,,,,"09/Nov/12 20:59;tn;Several improvements to the Gamma function have already been implemented in trunk (see MATH-849).

Running your test program with the latest trunk gives the following results:

{noformat}
alpha: 128.25629996917283
beta: 0.07889380729280462
i=0; pdf=0.0; cdf=0.0
i=1; pdf=8.663477060372497E-79; cdf=7.488555717749243E-81
i=2; pdf=5.506270463486845E-46; cdf=1.0676382015037016E-47
i=3; pdf=4.4134834884397667E-29; cdf=1.4607150065658644E-30
i=4; pdf=1.0945225959159135E-18; cdf=5.599255664602861E-20
i=5; pdf=7.359296475961767E-12; cdf=5.590805261026197E-13
i=6; pdf=2.7437881760967277E-7; cdf=3.0732983691238435E-8
i=7; pdf=2.837613263947388E-4; cdf=4.7826733420766874E-5
i=8; pdf=0.021281179160658817; cdf=0.005695853765034336
i=9; pdf=0.21511877233528723; cdf=0.10175065472250033
i=10; pdf=0.4475165875547783; cdf=0.4586536845007119
i=11; pdf=0.25911405955827355; cdf=0.8385256983930767
i=12; pdf=0.052182550309595385; cdf=0.9781383380332821
i=13; pdf=0.004329365023698068; cdf=0.9986058018679428
i=14; pdf=1.6878342979404588E-4; cdf=0.9999549514385939
i=15; pdf=3.431947471369265E-6; cdf=0.9999992047520807
i=16; pdf=3.958896722701942E-8; cdf=0.9999999917873332
i=17; pdf=2.775232535093164E-10; cdf=0.9999999999473412
i=18; pdf=1.2515801802472468E-12; cdf=0.9999999999997794
i=19; pdf=3.808712290749614E-15; cdf=0.9999999999999993
i=20; pdf=8.266386718783014E-18; cdf=1.0
i=21; pdf=1.2660914223523828E-20; cdf=1.0
i=22; pdf=1.4746029859879733E-23; cdf=1.0
i=23; pdf=1.320017264979955E-26; cdf=1.0
i=24; pdf=9.28776702592028E-30; cdf=1.0

x=19.0; pdf=3.808712290749614E-15; cdf=0.9999999999999993
x=19.1; pdf=2.091282124070138E-15; cdf=0.9999999999999997
x=19.2; pdf=1.1442807873491891E-15; cdf=0.9999999999999998
x=19.3; pdf=6.239511895039934E-16; cdf=0.9999999999999999
x=19.4; pdf=3.3905614583343015E-16; cdf=1.0
x=19.5; pdf=1.8362803668472756E-16; cdf=1.0
x=19.6; pdf=9.92380512634114E-17; cdf=1.0
x=19.7; pdf=5.314805483145551E-17; cdf=1.0
x=19.8; pdf=2.8759196357817904E-17; cdf=1.0
x=19.9; pdf=1.528822526909328E-17; cdf=1.0
x=20.0; pdf=8.266386718783014E-18; cdf=1.0
x=20.1; pdf=7.797057331677706E-18; cdf=1.0
x=20.2; pdf=0.0; cdf=1.0
x=20.3; pdf=0.0; cdf=1.0
x=20.4; pdf=0.0; cdf=1.0
x=20.5; pdf=0.0; cdf=1.0
x=20.6; pdf=0.0; cdf=1.0
x=20.7; pdf=0.0; cdf=1.0
x=20.8; pdf=0.0; cdf=1.0
x=20.9; pdf=2.4498611525242454E-20; cdf=1.0
{noformat}

Could you please test yourself and report if the result is as expected?

Thanks,

Thomas","14/Nov/12 19:12;zhochberg;I've built trunk, and I do get the results Thomas Neidhart shows below - problem is fixed.

One odd thing: for all unit tests to run correctly, I needed to download trunk to root of c:\ drive.
If I copied elsewhere, I would get a unit test failure - once, for example, of FixedElapsedTimeTest.java in genetics package.","14/Nov/12 19:14;tn;Hi Zev,

thanks for the feedback.

Could you attach a log of the mentioned test failure?

Thanks,

Thomas","14/Nov/12 19:42;zhochberg;Here's a unit test error log, when I copy trunk to c:\ACM\commons-math3, rather than to c:\commons-math3
There was also a unit test error when trunk was much ""deeper"" in the drive; in that case failed to open a file, which was in fact present.
I'll try and find my notes so I reproduce that one.

-------------------------------------------------------------------------------
Test set: org.apache.commons.math3.genetics.FixedElapsedTimeTest
-------------------------------------------------------------------------------
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.488 sec <<< FAILURE!
testIsSatisfied(org.apache.commons.math3.genetics.FixedElapsedTimeTest)  Time elapsed: 3.487 sec  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:92)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertTrue(Assert.java:54)
	at org.apache.commons.math3.genetics.FixedElapsedTimeTest.testIsSatisfied(FixedElapsedTimeTest.java:72)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:236)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:134)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:113)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:103)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:74)
","14/Nov/12 20:10;tn;ok, the test should be fixed, the sleep was larger than the test condition which could lead to a race condition.","23/Nov/12 11:42;erans;Resolving, as the reporter confirmed that the issue had already been fixed in ""trunk"".
",,,,,,,,,,,,,,,,,,,,,,,,,
SpearmansCorrelation fails when using NaturalRanking together with NaNStrategy.REMOVED,MATH-891,12615360,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tn,tn,08/Nov/12 18:31,07/Apr/13 09:17,20/Mar/20 20:33,27/Mar/13 19:44,3.0,,,,,,,3.2,,,0,,,,,,"As reported by Martin Rosellen on the users mailinglist:

Using a NaturalRanking with a REMOVED NaNStrategy can result in an exception when NaN are contained in the input arrays.

The current implementation just removes the NaN values where they occur, without taken care to remove the corresponding values in the other array.",,,,,,,,,,,,MATH-958,,"18/Feb/13 13:22;tn;MATH-891.patch;https://issues.apache.org/jira/secure/attachment/12569809/MATH-891.patch",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2013-03-27 17:04:24.713,,,false,,,,,,,,,,,,,,,,,256499,,,Sun Apr 07 09:17:10 UTC 2013,,,,,,,"0|i0hd67:",99396,,,,,,,,,,,,,,,,"08/Nov/12 18:38;tn;Test case:

{noformat}
        double[] column1 = new double[] { Double.NaN, 1, 2 };
        double[] column2 = new double[] { 10, 2, 10 };

        Array2DRowRealMatrix mydata = new Array2DRowRealMatrix(column1.length, 2);
        for (int i = 0; i < column1.length; i++) {
            mydata.addToEntry(i, 0, column1[i]);
            mydata.addToEntry(i, 1, column2[i]);
        }

        // compute correlation
        NaturalRanking ranking = new NaturalRanking(NaNStrategy.REMOVED);
        SpearmansCorrelation spearman = new SpearmansCorrelation(mydata, ranking);
        double correlations = spearman.correlation(column1, column2);
        System.out.println(""correlations "" + correlations);
{noformat}","18/Feb/13 13:22;tn;The attached patch does the following:

 * checks if the ranking algorithm is NaturalRanking with NaNStrategy set to REMOVED
 ** finds all indices with NaNs in the input arrays
 ** removes the corresponding rows in *all* input data
 ** passes the modified data to the ranking algorithm

This should ideally be handled by the RankingAlgorithm, but updating the interface would break compatibility afaik.

The SpearmanCorrelation class is the only one in CM that uses a RankingAlgorithm to rank correlated data, so it is a kind of compromise imho.","27/Mar/13 17:04;psteitz;I am OK with committing this patch, but lets keep the issue open, or open another one for handling missing data in multivariate stats.  I think it is OK to leave the RankingAlgorithm interface and impls as is - they are doing what they should be doing by contract.  I think multivariate stats should just not allow the REMOVED NAN strategy (i.e. throw in this case).  Once we agree on how to implement and represent missing data strategies at least just for this class, the Spearman's constructor should then be modified to include specification of missing data strategy.

I think it is better to commit the workaround now, since behavior is currently broken; but note in the javadoc and release notes that as of 4.0, the constructor will throw on REMOVED NaNStrategy and NANs should not be used to represent missing data.  Practical advice to users is to preprocess data to remove / replace / impute missing data in preparation for this.","27/Mar/13 19:44;tn;Applied changes in r1461822 and created MATH-958.","07/Apr/13 09:17;luc;Closing issue as version 3.2 has been released on 2013-04-06.",,,,,,,,,,,,,,,,,,,,,,,,,,
OLSMultipleRegression seems to fail on the Filippelli Data,MATH-615,12513813,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Not A Problem,,gsteri1,gsteri1,12/Jul/11 16:08,04/Mar/13 18:58,20/Mar/20 20:33,26/Nov/11 22:13,3.0,,,,,,,,,,0,data,Filippelli,NIST,OLSMutlipleRegression,QR,"Running the Filipelli data results in an exception being thrown by OLSMutlipleRegression. The exception states that the matrix is singular. 
http://www.itl.nist.gov/div898/strd/lls/data/Filip.shtml

I have added the data to the OLSMutlipleRegressionTest file. 

Unless I screwed something up in the passing of the data, it looks like the QR decomposition is failing.",Java,,,,,,,,,,,,,"20/Jul/11 02:46;gsteri1;filippelli2;https://issues.apache.org/jira/secure/attachment/12487100/filippelli2","12/Jul/11 16:11;gsteri1;tstdiff;https://issues.apache.org/jira/secure/attachment/12486197/tstdiff",,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-08-10 06:12:12.375,,,false,,,,,,,,,,,,,,,,,2208,,,Wed Aug 10 06:21:27 UTC 2011,,,,,,,"0|i0rtjj:",160430,,,,,,,,,,,,,,,,"12/Jul/11 16:11;gsteri1;The OLSMutlipleRegressionTest changes with Filipelli included... ","20/Jul/11 02:46;gsteri1;The situation is not as dire as I first thought. The original test I uploaded had a bug which resulted in a singular matrix, correctly. This current test still fails, but the failure occurs with a tolerance of 1.0e-5 for the parameters. ","10/Aug/11 06:12;psteitz;I am tempted to close this as not a problem.  What do you think, Greg?","10/Aug/11 06:17;gsteri1;Yes,  the major issue of singularity was one where I had a bug in the test.



","10/Aug/11 06:21;gsteri1;Do check in the Filipelli test though.


",,,,,,,,,,,,,,,,,,,,,,,,,,
Polygon difference function produces erroneous results with certain polygons,MATH-668,12522754,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Not A Problem,luc,jensenc,jensenc,12/Sep/11 15:18,04/Mar/13 18:58,20/Mar/20 20:33,20/Apr/12 19:19,3.0,,,,,,,3.0,,,0,difference,"math,","polygon,",,,"For some polygons, the difference function produces erroneous results.  This appears to happen when one polygon is completely encompassed in another, and the outer has multiple concave sections.",,604800,604800,,0%,604800,604800,,,,,,,"14/Sep/11 15:14;jensenc;PolygonsSetCircleTest.java;https://issues.apache.org/jira/secure/attachment/12494455/PolygonsSetCircleTest.java","12/Sep/11 15:32;jensenc;PolygonsSetTest.java;https://issues.apache.org/jira/secure/attachment/12494047/PolygonsSetTest.java",,,,,2.0,,,,,,,,,,,,,,,,,,,,2012-01-24 11:49:28.411,,,false,,,,,,,,,,,,,,,,,2184,,,Fri Apr 20 19:19:52 UTC 2012,,,,,,,"0|i0rtg7:",160415,,,,,,,,,,,,,,,,"12/Sep/11 15:32;jensenc;There are two test cases here.  The first ""testdifference1"" produces 3 polygons, two of which are correct (one outer polygon and one inner hole polygon).  The third polygon is a rectangular region in the lower left corner of the outer polygon and should not be there.

The second test case ""testdifference2"" is a simpler example.  Technically, the results are not incorrect, but they are strange.  This example is included because it seems interesting and may help understand the problem.  The inner resultant polygon has two extra points that are collinear with two edges on the polygon.  The extra points don't change the shape of the polygon, but they don't need to be there.","12/Sep/11 15:38;jensenc;Never mind on testdifference1.  I think I see the problem (bottom left corner of outer polygon is ambiguous).  I'll work on creating a better example.  Though, testdifference2 is still an interesting case.","14/Sep/11 15:14;jensenc;This circle test case takes the difference of two concentric circles.  The results produces is an empty list of vertices, when it should be the outer circle with a hole where the inner circle is.","24/Jan/12 11:49;tn;I have checked the attached test cases:

 - circle test: you remove the outer circle from the inner one, resulting in an empty polygon set, which is correct behaviour. If you switch the two circles, the correct result is returned: a PolygonsSet with two loops: the two circles itself.

 - testdifference2: the described behaviour is implementation specific due to the way the hyperplane is cut when subtracting the two regions.","26/Jan/12 19:21;luc;The fix would probably not imply API change, so it can be delayed to 3.1.","20/Apr/12 19:19;luc;I agree with Thomas analyses.

Concerning the difference2 case, the two points are explained by the vertical line at x = 5.0 which comes from the outer shape. The internal representation is a BSP tree and one of this part of the outer boundary creates an hyperplane that splits the inner triangle. When the boundary representation is rebuilt, the two segments are glued together and the points appear there. There is no post-processing that simplifies the representation afterwards.

Concerning the circle test, I guess you mixed the arrays. What is really in the code is that the vertices2 array is build first from outer circle and the vertices1 array is built afterwards from inner circle. So you are really subtracting a big disk from a smaller one. As Thomas explained, computing set2 minus set1 give the expected two boundaries. Another possible change is to build the circles clockwise instead of counter-clockwise, and in this case the two regions are infinite wich a whole at the center, then subtracting set2 from set1 returns a disk with a hole.",,,,,,,,,,,,,,,,,,,,,,,,,
Brent solver calculates incorrect root (namley Double.MAX_VALUE),MATH-832,12600065,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,alex87,alex87,24/Jul/12 20:56,04/Mar/13 18:57,20/Mar/20 20:33,27/Jul/12 15:10,3.0,,,,,,,,,,0,,,,,,"*Wolfram-Alpha-Solution:*
[http://www.wolframalpha.com/input/?i=min+100*sqrt%28x%29%2B1000000%2Fx%2B10000%2Fsqrt%28x%29+with+x%3E0]
{code:borderStyle=solid}min{100 sqrt(x)+1000000/x+10000/sqrt(x)|x>0}~~4431.94 at x~~804.936{code}

*Java-Input:*{code:borderStyle=solid}
int startValue1 = 100 + 1000000 + 10000;
int startValue2 = 100;

UnivariateFunction uf = new UnivariateFunction() {
    @Override
    public double value(double x) {
        return 100/(2*Math.sqrt(x)) - 1000000/Math.pow(x,2) - 10000/(2*Math.pow(x,(double) 3/2));
    }
};

System.out.println(
    (new BrentSolver()).solve(Integer.MAX_VALUE, uf, 1/Double.MAX_VALUE, Double.MAX_VALUE, startValue1)
);
System.out.println(
    (new BrentSolver()).solve(Integer.MAX_VALUE, uf, 1/Double.MAX_VALUE, Double.MAX_VALUE, startValue2)
);{code}

*Java-Output:*{code:borderStyle=solid}
804.9355821866686
1.7976931348623157E308 (= Double.MAX_VALUE){code} ","Netbeans 7.1.2
",,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-07-24 22:33:45.567,,,false,,,,,,,,,,,,,,,,,292286,,,Fri Jul 27 15:10:32 UTC 2012,,,,,,,"0|i0rt1r:",160350,,,,,,,,,,,,,,,,"24/Jul/12 22:33;erans;bq. min{100 sqrt(x)+1000000/x+10000/sqrt(x)|x>0}

Are you using the right class?
Optimizing (a.k.a. finding the minimum) and solving (a.k.a. finding the root) are not the same thing.

Please try this:
{code}
public void testMath832() {
    final UnivariateFunction f = new UnivariateFunction() {
            public double value(double x) {
                final double sqrtX = FastMath.sqrt(x);
                final double a = 1e2 * sqrtX;
                final double b = 1e6 / x;
                final double c = 1e4 / sqrtX;

                return a + b + c;
            }
        };

    UnivariateOptimizer optimizer = new BrentOptimizer(1e-10, 1e-8);
    final double result = optimizer.optimize(1483,
                                             f,
                                             GoalType.MINIMIZE,
                                             Double.MIN_VALUE,
                                             Double.MAX_VALUE).getPoint();

    Assert.assertEquals(804.935582, result, 1e-6);
}
{code}
","26/Jul/12 09:25;alex87;I tried testMath832() and there was no error (although i'm not sure if i did it right - i never worked with JUnit before). But now i think i know where the problem is: It isn't the difference between optimizing and solving (minimizing a function (that derivative has only one root) => finding the root of the derivative) but a numerical issue:

http://www.wolframalpha.com/input/?i=Plot(100*sqrt%28x%29%2B1000000%2Fx%2B10000%2Fsqrt%28x%29%2C+{x%2C+0%2C+1000})
http://www.wolframalpha.com/input/?i=Plot(d%2Fdx+100*sqrt%28x%29%2B1000000%2Fx%2B10000%2Fsqrt%28x%29%2C+{x%2C+0%2C+1000})
[http://www.wolframalpha.com/input/?i=Plot(d%2Fdx+100*sqrt%28x%29%2B1000000%2Fx%2B10000%2Fsqrt%28x%29%2C+{x%2C+10^10%2C+10^100})]

An upper bound for the interval sufficiently smaller than infinity causes no problems:{code:borderStyle=solid}
System.out.println(
        (new BrentSolver()).solve(Integer.MAX_VALUE, uf, 1/Double.MAX_VALUE, 1.0e10, startValue2)
);{code}","26/Jul/12 11:30;erans;bq. [...] But now i think i know where the problem is [...]

I do not. Is there a problem in Commons Math?

bq. An upper bound for the interval sufficiently smaller than infinity causes no problems

The initial report seems to indicate that you wanted to obtain the minimum of ""uf"". Then, in your last comment, you still use ""BrentSolver"" which will search for the root (of that same ""uf"").
Which is it?

Also, please correct the links in your previous comment; they seem broken (parts not being interpreted as links). Even better would be to upload files (either figures, or source code to reproduce the problem) to this page, instead of those unreadable links!
Thanks.
","26/Jul/12 13:38;erans;""BrentSolver"" (on the derivative of ""uf"") and ""Brentoptimizer"" (on ""uf"") both return the same value.

What you encountered is probably a limitation of finite precision: large values (on the right of the search interval) caused the solver to stop iterating too early: You might want to try with other values of the tolerances (arguments to the constructor).
","27/Jul/12 15:10;erans;If you still consider that there is a problem, please file another report, with a more precise description of the issue.
",,,,,,,,,,,,,,,,,,,,,,,,,,
LP on android devices,MATH-794,12558559,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,sylvain.rousse,sylvain.rousse,30/May/12 08:51,04/Mar/13 18:57,20/Mar/20 20:33,31/May/12 12:46,3.0,,,,,,,,,,0,android,,,,,"When I express a Linear Programming problem in a Pure Java environment, I get a solution A.

When I express thje same linear programming problem in a Java Android environment using the same problem modelling classes, I get a solution A', which doesn't match the constraints I express.

For instance, If I have a set of Xi variables, with each Xi >= 0, I can get some Xj < 0.

Or if I have Xi >= a, then in the solution, I can have Xi < a.

I noticed that for freechart library, they have made a special library for android (Jfreecharrtt for pure javan Afreechart for Android).

I have tried to compile the source with my android app, but the problem is the same.

Is it necessary to have a library built up for android devices ?

Is it possible to have commons-math for android ?

In advance thanks.

Regards.

Sylvain.",Android 2.2 JDK 1.6,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-05-30 11:37:12.163,,,false,,,,,,,,,,,,,,,,,292307,,,Thu May 31 12:46:15 UTC 2012,,,,,,,"0|i0rt6f:",160371,,,,,,,,,,,,,,,,"30/May/12 11:37;luc;Apache Commons Math should run on Android, as long as you package it as a dex file.
There is an exemple here: [https://www.orekit.org/forge/projects/socis-2011/wiki/HowToBuildToAndroid],
based on another Java library (Orekit), which depends on Apache Commons Math.

Can you reproduce the error with a really simple (say less than 100 lines of code) standalone application that could be run both on Android from the command line with dalvikvm (look at the exemple in the wiki page above, in the ""easy way"" section) and on a desktop environment?","31/May/12 12:46;luc;According to user, a simpler problem provided the same result on Android and desktop environment. So the problem was elsewhere.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not expected UnboundedSolutionException,MATH-828,12599477,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,tn,alexeyslepov,alexeyslepov,19/Jul/12 15:07,04/Mar/13 18:53,20/Mar/20 20:33,05/Aug/12 16:33,3.0,,,,,,,3.1,,,0,linear,math,programming,,,"SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables.

In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions.
First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result.

The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values.

What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem.

The problem is formulated as
min(1*t + 0*L) (for every r-th subject)
s.t.
-q(r) + QL >= 0
x(r)t - XL >= 0
L >= 0
where 
r = 1..R, 
L = {l(1), l(2), ..., l(R)} (vector of R rows and 1 column),
Q - coefficients matrix MxR
X - coefficients matrix NxR ",Intel Core i5-2300 Windows XP SP3,,,,,,,,,,,MATH-842,,"19/Jul/12 15:09;alexeyslepov;ApacheSimplexWrapper.java;https://issues.apache.org/jira/secure/attachment/12537184/ApacheSimplexWrapper.java","19/Jul/12 15:09;alexeyslepov;ApacheSimplexWrapperTest.java;https://issues.apache.org/jira/secure/attachment/12537185/ApacheSimplexWrapperTest.java","19/Jul/12 15:09;alexeyslepov;Entity.java;https://issues.apache.org/jira/secure/attachment/12537183/Entity.java","19/Jul/12 15:09;alexeyslepov;commons-math3-3.0.jar;https://issues.apache.org/jira/secure/attachment/12537186/commons-math3-3.0.jar",,,4.0,,,,,,,,,,,,,,,,,,,,2012-07-19 17:24:24.445,,,false,,,,,,,,,,,,,,,,,292289,,,Sun Aug 05 16:39:47 UTC 2012,,,,,,,"0|i0rt2f:",160353,,,,,,,,,,,,,,,,"19/Jul/12 15:09;alexeyslepov;ApacheSimplexWrapperTest.java is the entry point
ApacheSimplexWrapper.java is the main class

commons-math3-3.0.jar is that same Apache Math 3 Lib I use

Source code contatins come auxiliary method, almost all for printing. Just run ApacheSimplexWrapperTest.java with a debugger and run straight forward to the line 160 in ApacheSimplexWrapper.java where things happen","19/Jul/12 15:19;alexeyslepov;And there is one more strange thing I see. Solutions of the problem that are not unbounded give value of t = 1 every single time while they are supposed to be within [0, 1]. E.g. in the predifend variables values case in the first iteration t1 is 1 and t2 is 0.25 (see Test #1)","19/Jul/12 17:24;erans;Did you test with a recent development snapshot?
","19/Jul/12 19:18;tn;Hi Alexey,

I have looked at your updated test case, and my observation is as follows:

You create lots of constraints (L >= 0) that are unnecessary as the solver is already configured to restrict variables to non-negative values.

I also think you use the objective function in a wrong way. It is defined as:

{noformat}
c1*x1 + ... cn*xn + d
{noformat}

so at index 0 you have the coefficient for the first variable, .... and the last index is for the constant term. Now you use something called theta, which you put on index 0 which is wrong imho.

If I remove all the unnecessary constraints, and move the theta variable to the end of the objective function vector, the tests run through successfully.

Thomas","19/Jul/12 19:20;tn;I close this issue also as invalid, as there is nothing wrong with the solver itself. You may also ask questions regarding the use of the simplex solver on the commons-user mailinglist.

Thomas","20/Jul/12 09:00;alexeyslepov;Thomas, thanks for reference that the solver is already configured to restrict variables to non-negative values. That signally decreased time the solver needed to find solution as much as decreased the number of UnboundedSolutionExceptions.

As to the objective, there is a misunderstanding I believe.
I use the definition of the objective as it's declared in http://commons.apache.org/math/apidocs/org/apache/commons/math/optimization/linear/LinearObjectiveFunction.html
and theta is x1 and there are R+1 variables. R is the number of objects the omtimum is to find for (ENTITIES_COUNT) that is equal to the number of lambdas l(i). And as it shown in the very first test of the JUnit test where Q = [1,2], X = [2,1] and L = T[l1, l2] the solver gives an expected result. That is the indicator that equations are written properly. In other words for every single of N entities there has to be an objective with N + 1 variables. For the case of 2 entities the 3 dimension space is used to build a surface that has it's theta or x(1) coordinate set to minimum, for the case of 3 entities the 4 dimension space is used to build a shape that has it's theta or x(1) coordinate set to minimum and etc.

Here is how it looks for the simplest case (JUnit test #0)

Model name: DEA problem
                 t       L1       L2 
Minimize         1        0        0 
R1               0        2        1 >=        1
R2               2       -1       -2 >=        0
R3               0        1        0 >=        0
R4               0        0        1 >=        0
Type          Real     Real     Real 
upbo           Inf      Inf      Inf 
lowbo            0        0        0 

the objective is to be 0.25 and theta = 0.25 and L1 = 0.5, L2 = 0
The solver gives the same result for the case.
But only I add more entities to find minimum for as the same add more lambdas the solver gives back wrong answer, unbounded solution or theta greater than 1 (that is wrong due to the problem condition)

I'm sure it's been really too early to close the issue :( ","20/Jul/12 19:37;tn;Hi Alexey,

you are right, I was too quick to draw a conclusion, the way you setup the problem is indeed correct.

What I have seen is that you use a very small maxUlps setting in your solver. The default it 10 and should work better atm. I will further look into it, it seems to be related to numerical instabilities.

Solving the same problems with glpk seems to be more robust, which maybe due to the scaling that is applied there to improve numerical properties of the constraint matrix.","23/Jul/12 09:12;alexeyslepov;Thank you Thomas for reopenning and your confirmation that I use math3 properly. I'll play around with maxUIps setting and will try GLPK that I've heard about but haven't tryed yet.

Gilles am I right thinking that I need to use SVN to get 3.1 (r12317576)
svn checkout http://svn.apache.org/repos/asf/commons/proper/math/trunk commons-math3
?
or if I'm wrong please refer me to the proper link.
Thanks","23/Jul/12 09:34;erans;bq. [...] I need to use SVN to get 3.1 [...]

The link refers to the development branch (it's not yet 3.1).
Yes, that's the way to get the up-to-date version of the code.

If you don't want to compile the code (which requires the ""maven"" software), you could also download a snapshot JAR:
 https://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math3/3.1-SNAPSHOT/

","23/Jul/12 10:02;alexeyslepov;Now with 3.1-SNAPSHOT things are going better. Number of unbounded exceptions and unexpected theta values (> 1) are fewer versus math 3.0.
Now the picture of failures and successes for
25 entities (i.e. 25 + 1 variables) with 7 constraint equations, 3 for -q(r) + QL >= 0 and 4 for x(r)*t - XL >= 0
looks like

Iteration 1 of 64
Iteration 2 of 64
Iteration 3 of 64
Iteration 4 of 64
Iteration 5 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 6 of 64
Iteration 7 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 8 of 64
Iteration 9 of 64
Iteration 10 of 64
Iteration 11 of 64
Iteration 12 of 64
Iteration 13 of 64
Iteration 14 of 64
Iteration 15 of 64
Iteration 16 of 64
Iteration 17 of 64
Iteration 18 of 64
Iteration 19 of 64
Iteration 20 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 21 of 64
Iteration 22 of 64
Iteration 23 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 24 of 64
Iteration 25 of 64
Iteration 26 of 64
EXCEPTION: unbounded solution
Iteration 27 of 64
Iteration 28 of 64
Iteration 29 of 64
Iteration 30 of 64
Iteration 31 of 64
Iteration 32 of 64
Iteration 33 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 34 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 35 of 64
Iteration 36 of 64
Iteration 37 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 38 of 64
Iteration 39 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 40 of 64
Iteration 41 of 64
Iteration 42 of 64
Iteration 43 of 64
Iteration 44 of 64
Iteration 45 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 46 of 64
Iteration 47 of 64
Iteration 48 of 64
Iteration 49 of 64
Iteration 50 of 64
Iteration 51 of 64
Iteration 52 of 64
Iteration 53 of 64
Iteration 54 of 64
Iteration 55 of 64
Iteration 56 of 64
Iteration 57 of 64
Iteration 58 of 64
Iteration 59 of 64
Iteration 60 of 64
Iteration 61 of 64
Iteration 62 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 63 of 64
Iteration 64 of 64

for the code
SimplexSolver solver = new SimplexSolver(epsilon, 15);
try
{
	solver.setMaxIterations(32768);
	PointValuePair optimum = solver.optimize(objectiveFunction, constraints, GoalType.MINIMIZE, true);
...

It's much better but it's still to risky to use math 3 to solve problems of this kind in real-world projects.","25/Jul/12 10:46;alexeyslepov;I've made several launches of the program with different values. Here is the result
These
final int INPUT_ARGUMENTS_COUNT = 4;
final int OUTPUT_ARGUMENTS_COUNT = 3;
final int MIN_ARGUMENT_VALUE = 1;
final int MAX_ARGUMENT_VALUE = 100;
final int ITERATIONS_COUNT = 512;
and
maxIterationsCount = 65536;
stay the same over all experiments


Experiment 1
final int ENTITIES_COUNT = 20;
final double EPSILON = 1E-5;
final boolean IS_INTEGER = false;
/*
IS_INTEGER is using in the source code as
value = MIN_ARGUMENT_VALUE + rand.nextInt(MAX_ARGUMENT_VALUE);
if(!IS_INTEGER){
     value += rand.nextDouble();
}
where value is an entity' coefficient value that is a cell of Q and X matrices
*/
gives
13 unbounded solutions of 512 iterations
0 nofeasible solutions of 512 iterations
34 maxcount exceeded exception of 512 iterations
Total 47.0 failures of 512 iterations ( = 0.091796875 of 1)

Experiment 2
final int ENTITIES_COUNT = 20;
final double EPSILON = 1E-8;
final boolean IS_INTEGER = false;
gives
13 unbounded solutions of 512 iterations
0 nofeasible solutions of 512 iterations
37 maxcount exceeded exception of 512 iterations
Total 50.0 failures of 512 iterations ( = 0.09765625 of 1)

Experiment 3
final int ENTITIES_COUNT = 20;
final double EPSILON = 1E-8;
final boolean IS_INTEGER = true;
gives
11 unbounded solutions of 512 iterations
3 nofeasible solutions of 512 iterations
33 maxcount exceeded exception of 512 iterations
Total 47.0 failures of 512 iterations ( = 0.091796875 of 1)

Experiment 4
final int ENTITIES_COUNT = 15;
final double EPSILON = 1E-8;
final boolean IS_INTEGER = false;
gives
10 unbounded solutions of 512 iterations
0 nofeasible solutions of 512 iterations
18 maxcount exceeded exception of 512 iterations
Total 28.0 failures of 512 iterations ( = 0.0546875 of 1)

Experiment 5
final int ENTITIES_COUNT = 10;
final double EPSILON = 1E-8;
final boolean IS_INTEGER = false;
gives
7 unbounded solutions of 512 iterations
1 nofeasible solutions of 512 iterations
16 maxcount exceeded exception of 512 iterations
Total 24.0 failures of 512 iterations ( = 0.046875 of 1)

Experiment 6
final int ENTITIES_COUNT = 5;
final double EPSILON = 1E-8;
final boolean IS_INTEGER = false;
gives
3 unbounded solutions of 512 iterations
0 nofeasible solutions of 512 iterations
0 maxcount exceeded exception of 512 iterations
Total 3.0 failures of 512 iterations ( = 0.005859375 of 1)


As you can see the most influence to the amount of failures gives the number of variables. When there are 5 of them the amount of failure is about a half of a precent which is satisfyingly. When there are 10 or more variables the amount of failures becomes unacceptable.

Please pay attention to the dependence of the amount of failures on the number of variables that is shown through experiments 3, 4, 5, 6
variables     failures
20            47
15            28
10            24
5             3
These failures numbers would change from one experiment launch to another of course but not much, +/- 2 failures","27/Jul/12 13:47;alexeyslepov;Another parameter I've playd around with is maxUlps
As far as I see it's Precision.equals(double x, double y, int maxUlps) where maxUlps is used

  public static boolean equals(double x, double y, int maxUlps) {
        long xInt = Double.doubleToLongBits(x);
        long yInt = Double.doubleToLongBits(y);

        // Make lexicographically ordered as a two's-complement integer.
        if (xInt < 0) {
            xInt = SGN_MASK - xInt;
        }
        if (yInt < 0) {
            yInt = SGN_MASK - yInt;
        }

        final boolean isEqual = FastMath.abs(xInt - yInt) <= maxUlps;

        return isEqual && !Double.isNaN(x) && !Double.isNaN(y);
    }

I've made several more experiments. Notice that on each iteration 

SimplexSolver solver = new SimplexSolver(epsilon, maxUlps);
		try
		{
			solver.setMaxIterations(maxIterationsCount);
			PointValuePair optimum = solver.optimize(objectiveFunction, constraints, GoalType.MINIMIZE, true);

code is called for each entity. I.e. when 1024 iterations there are 1024*ENTITIES_COUNT = 1024*15 calls of new instances of SimplexSolver
Sum of input and output arguments is equal to number of variables - 1. In this case there were 8 variables for the objective function.

These below are results of experiments with different maxUlps values

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 0
8 unbounded solutions of 1024 iterations ( = 0.0078125 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
48 maxcount exceeded exception of 1024 iterations ( = 0.046875 of 1)
Total 56.0 failures of 1024 iterations ( = 0.0546875 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 2
13 unbounded solutions of 1024 iterations ( = 0.0126953125 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
67 maxcount exceeded exception of 1024 iterations ( = 0.0654296875 of 1)
Total 80.0 failures of 1024 iterations ( = 0.078125 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 4
16 unbounded solutions of 1024 iterations ( = 0.015625 of 1)
1 nofeasible solutions of 1024 iterations ( = 9.765625E-4 of 1)
45 maxcount exceeded exception of 1024 iterations ( = 0.0439453125 of 1)
Total 62.0 failures of 1024 iterations ( = 0.060546875 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 6
21 unbounded solutions of 1024 iterations ( = 0.0205078125 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
50 maxcount exceeded exception of 1024 iterations ( = 0.048828125 of 1)
Total 71.0 failures of 1024 iterations ( = 0.0693359375 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 8
13 unbounded solutions of 1024 iterations ( = 0.0126953125 of 1)
1 nofeasible solutions of 1024 iterations ( = 9.765625E-4 of 1)
39 maxcount exceeded exception of 1024 iterations ( = 0.0380859375 of 1)
Total 53.0 failures of 1024 iterations ( = 0.0517578125 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 25
24 unbounded solutions of 1024 iterations ( = 0.0234375 of 1)
1 nofeasible solutions of 1024 iterations ( = 9.765625E-4 of 1)
44 maxcount exceeded exception of 1024 iterations ( = 0.04296875 of 1)
Total 69.0 failures of 1024 iterations ( = 0.0673828125 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 30
19 unbounded solutions of 1024 iterations ( = 0.0185546875 of 1)
2 nofeasible solutions of 1024 iterations ( = 0.001953125 of 1)
43 maxcount exceeded exception of 1024 iterations ( = 0.0419921875 of 1)
Total 64.0 failures of 1024 iterations ( = 0.0625 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 35
15 unbounded solutions of 1024 iterations ( = 0.0146484375 of 1)
1 nofeasible solutions of 1024 iterations ( = 9.765625E-4 of 1)
36 maxcount exceeded exception of 1024 iterations ( = 0.03515625 of 1)
Total 52.0 failures of 1024 iterations ( = 0.05078125 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 40
33 unbounded solutions of 1024 iterations ( = 0.0322265625 of 1)
1 nofeasible solutions of 1024 iterations ( = 9.765625E-4 of 1)
44 maxcount exceeded exception of 1024 iterations ( = 0.04296875 of 1)
Total 78.0 failures of 1024 iterations ( = 0.076171875 of 1)

It seems that maxUlps gives not much to the state of the problem.
I used https://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math3/3.1-SNAPSHOT/commons-math3-3.1-20120726.144152-154.jar SNAPSHOT for this expirement set","27/Jul/12 14:09;erans;Lines 74-78 in {{SimplexSolver}} look strange:
{code}
if (Precision.compareTo(entry, minValue, maxUlps) < 0) {
    minValue = entry;
    minPos = i;
}
{code}

Its seems like the ""minValue"" will not really be most negative negative coefficient, as claimed in the doc.
The larger ""maxUlps"", the most likely it will not be, in line with what you observe...
","28/Jul/12 16:42;tn;In r1366707, I committed the following changes to the SimplexSolver:

 * do not use maxUlps in getPivotColumn and getPivotRow when trying to find a minimum
   this is contra-productive, and the check should be strict

 * implement Bland's rule to prevent cycling when selecting the pivot row in case of multiple candidates

 * to improve numerical stability, introduce a CUTOFF_THRESHOLD (currently 1e-12) to zero-out
   values that are smaller than this threshold in SimplexTableau.subtractRow

With these changes your tests run through without any errors. Could you please verify yourself and confirm.

Thanks, 

Thomas
","30/Jul/12 08:53;alexeyslepov;Thank you very much, Thomas! Now it works well! 

5 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)
Total 0.0 failures of 1024 iterations ( = 0.0 of 1)

10 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)
Total 0.0 failures of 1024 iterations ( = 0.0 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)
Total 0.0 failures of 1024 iterations ( = 0.0 of 1)

20 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
2 maxcount exceeded exception of 1024 iterations ( = 0.001953125 of 1)
Total 2.0 failures of 1024 iterations ( = 0.001953125 of 1)

25 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
2 maxcount exceeded exception of 1024 iterations ( = 0.001953125 of 1)
Total 2.0 failures of 1024 iterations ( = 0.001953125 of 1)

30 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
3 maxcount exceeded exception of 1024 iterations ( = 0.0029296875 of 1)
Total 3.0 failures of 1024 iterations ( = 0.0029296875 of 1)

https://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math3/3.1-SNAPSHOT/commons-math3-3.1-20120729.124827-157.jar","30/Jul/12 13:24;tn;Thanks for the feedback, I will further investigate the remaining maxcount exceptions.
In my dev environment I mainly tested with 15 entities.","30/Jul/12 19:45;tn;I further investigated the remaining problems and came up with an empirical heuristic:

 * If we have not found a solution after half of maxIterations, ignore Bland's rule and revert to the top-most row

I did extensive tests with your provided test case and did not receive any exceptions anymore.","31/Jul/12 09:38;alexeyslepov;I confirm that those new improvements erased errors were remaining.
Tests of
https://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math3/3.1-SNAPSHOT/commons-math3-3.1-20120730.205101-159.jar
gives

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)
Total 0.0 failures of 1024 iterations ( = 0.0 of 1)

25 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)
Total 0.0 failures of 1024 iterations ( = 0.0 of 1)

35 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)
Total 0.0 failures of 1024 iterations ( = 0.0 of 1)

Excellent! Thank you, Thomas!","01/Aug/12 21:38;tn;Hmm, I am no so sure if the last fix is the right way to go. I will leave this issue open and will investigate more on the topic.","02/Aug/12 09:45;alexeyslepov;What can go wrong with the last fix?","02/Aug/12 11:52;tn;The first fix introduced Bland's rule to prevent cycling. In fact cycling still occurs, thats why I added the additional heuristic.
We need to better understand why cycling still occurs with Bland's rule and better fix this problem, than trying to circumvent it with a rule like it is implemented now.

It should hopefully not affect you, as with the latest version your problems seem to work pretty well, but I want a more general solution. The current fix may break for other type of problems.","02/Aug/12 12:51;erans;Hi Thomas.

Wouldn't it be clearer (cf. description and subject of this issue) to open a new issue for the problem which you identified (cycling) together with a test case that induces it?
The new issue can reference this report so that the history can be recovered easily.

This will allow to resolve this issue, which accurately reflects its state from the perspective of the problem raised.
","03/Aug/12 09:01;tn;Hi Gilles,

the cycling problem has been fixed (+ test case) but in a way that may be very specific to the problem definition of Alexey.
But I am fine with closing this issue and creating a new one to further investigate the issue.

Thomas","03/Aug/12 09:17;alexeyslepov;Thomas, when you'll open a new thread for further investigation of the issue, post here a link to that thread, please","05/Aug/12 16:39;tn;Hi Alexey,

I linked the newly created issue to this one.

Thomas",,,,,,
SimplexSolver bug?,MATH-813,12598195,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,vidyaraghu,vidyaraghu,10/Jul/12 14:58,04/Mar/13 18:53,20/Mar/20 20:33,10/Jul/12 20:05,3.0,,,,,,,3.1,,,0,,,,,,"I am trying to use the SimplexSolver in commons-math3-3.0 and am getting unpredictable results. I am pasting the problem code below. Basically swapping the sequence of the last two constraints results in two different results (of which one is pure sub-optimal). Am I not using the solver correctly?

------------------------------
import java.util.ArrayList;
import java.util.Collection;

import org.apache.commons.math3.optimization.*;
import org.apache.commons.math3.optimization.linear.*;

public class Commons_Solver {
  public static void main(String[] args) {

 // describe the optimization problem
    
    LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 1, 1, 1, 1, 1, 1, 0, 0 }, 0);
    Collection <LinearConstraint>constraints = new ArrayList<LinearConstraint>();
    
    //variables upper bounds
    constraints.add(new LinearConstraint(new double[] { 1, 0, 0, 0, 0, 0, 0, 0 }, Relationship.LEQ, 38));
    constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 0, 0, 0, 0, 0 }, Relationship.LEQ, 34));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 1, 0, 0, 0, 0, 0 }, Relationship.LEQ, 1));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 1, 0, 0, 0, 0 }, Relationship.LEQ, 6));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 1, 0, 0, 0 }, Relationship.LEQ, 17));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 1, 0, 0 }, Relationship.LEQ, 11));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 0, 1, 0 }, Relationship.LEQ, 101));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 0, 0, 1 }, Relationship.LEQ, 1e10));

    //variables lower bounds
    constraints.add(new LinearConstraint(new double[] { 1, 0, 0, 0, 0, 0, 0, 0 }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 0, 0, 0, 0, 0 }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 1, 0, 0, 0, 0, 0 }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 1, 0, 0, 0, 0 }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 1, 0, 0, 0 }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 1, 0, 0 }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 0, 1, 0 }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 0, 0, 1 }, Relationship.GEQ, 0));


    constraints.add(new LinearConstraint(new double[] { -1,-1, -1, -1, -1, -1, 1, 0 }, Relationship.EQ, 0));

    constraints.add(new LinearConstraint(new double[] { -1, -1, -1, -1, -1, -1,0 , 1 }, Relationship.EQ, 0));

    constraints.add(new LinearConstraint(new double[] { 1, 0, 0, 0, 0, 0, 0, -0.2841121495327103  }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 0, 0, 0, 0, -0.25420560747663556  }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 1, 0, 0, 0, -0.04485981308411215 }, Relationship.GEQ, 0));
    
    /*---------------
    Swapping the sequence of the below two constraints produces two different results 
    ------------------*/
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 1, 0, 0, -0.12710280373831778  }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 1, 0, -0.08224299065420561  }, Relationship.GEQ, 0));
    /*------------------*/
    
    PointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, false);

    // get the solution
    for (int i = 0 ; i < solution.getPoint().length; i++)      
      System.out.println(""x["" + i + ""] = "" +  solution.getPoint()[i]);
    
    System.out.println(""value = "" + solution.getValue());
  }
}
----------------------------------","Windows 7, JDK 1.7.0_03",,,,,,,,,,MATH-781,,,"10/Jul/12 14:59;vidyaraghu;Commons_Solver.java;https://issues.apache.org/jira/secure/attachment/12535845/Commons_Solver.java",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-07-10 17:59:38.444,,,false,,,,,,,,,,,,,,,,,292294,,,Tue Jul 10 20:06:48 UTC 2012,,,,,,,"0|i0rt3j:",160358,,,,,,,,,,,,,,,,"10/Jul/12 17:59;tn;Hi,

I quickly checked your test case with the latest trunk and I get exactly the same result. Most likely your observed problem has been fixed already as part of MATH-781. Could you please check yourself with the latest trunk and confirm that the problem is not existent anymore?

Thanks,

Thomas","10/Jul/12 19:58;vidyaraghu;Hi Thomas,

I too am able to get the right results with the latest trunk code. Thanks for looking into this. 

Raghu","10/Jul/12 20:05;tn;Duplicate of MATH-781","10/Jul/12 20:06;tn;Thanks for the report anyway!

I marked the issue as a duplicate of MATH-781 and changed the fix version to 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,
BSPTree class and recovery of a Euclidean 3D BRep,MATH-780,12551802,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,arwillis,arwillis,20/Apr/12 15:11,04/Mar/13 18:53,20/Mar/20 20:33,13/May/12 15:56,3.0,,,,,,,3.1,,,0,BSPTree,euclidean.threed,,,,"New to the work here. Thanks for your efforts on this code.

I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem.

Any ideas?
",Linux,,,,,,,,,,,,,"22/Apr/12 15:44;arwillis;BSPMesh2.java;https://issues.apache.org/jira/secure/attachment/12523688/BSPMesh2.java","22/Apr/12 15:38;arwillis;BSPMesh2.java;https://issues.apache.org/jira/secure/attachment/12523685/BSPMesh2.java","20/Apr/12 20:55;arwillis;BSPMesh2.java;https://issues.apache.org/jira/secure/attachment/12523563/BSPMesh2.java","20/Apr/12 20:52;arwillis;BSPMesh2.java;https://issues.apache.org/jira/secure/attachment/12523561/BSPMesh2.java","20/Apr/12 16:40;arwillis;BSPMesh2.java;https://issues.apache.org/jira/secure/attachment/12523518/BSPMesh2.java",,5.0,,,,,,,,,,,,,,,,,,,,2012-04-20 18:16:12.575,,,false,,,,,,,,,,,,,,,,,236609,,,Sun May 13 15:56:34 UTC 2012,,,,,,,"0|i0rt93:",160383,,,,,,,,,,,,,,,,"20/Apr/12 16:08;arwillis;Looks like I had an index flipped resulting in an improperly oriented face for the tetrahedron. I did get the tetrahedron to work. However, I am now struggling with getting a cube to work and I have verified the normals/orientations of the planes to be correct.","20/Apr/12 16:40;arwillis;The code in BSPMesh2.java produces the following error when I run it. If you comment in the line that re-assigns the coordinate data to be cubeCoords1, then the code works fine. The only difference in the two data sets is that one coordinate has changed by a small amount.

Exception in thread ""main"" java.lang.ClassCastException: org.apache.commons.math3.geometry.partitioning.BoundaryAttribute cannot be cast to java.lang.Boolean
	at org.apache.commons.math3.geometry.euclidean.twod_exact.PolygonsSet.computeGeometricalProperties(PolygonsSet.java:135)
	at org.apache.commons.math3.geometry.partitioning.AbstractRegion.getSize(AbstractRegion.java:380)
	at org.apache.commons.math3.geometry.euclidean.threed_exact.PolyhedronsSet$FacetsContributionVisitor.addContribution(PolyhedronsSet.java:171)
	at org.apache.commons.math3.geometry.euclidean.threed_exact.PolyhedronsSet$FacetsContributionVisitor.visitInternalNode(PolyhedronsSet.java:153)
	at org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:262)
	at org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:261)
	at org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:261)
	at org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:263)
	at org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:261)
	at org.apache.commons.math3.geometry.euclidean.threed_exact.PolyhedronsSet.computeGeometricalProperties(PolyhedronsSet.java:118)
	at org.apache.commons.math3.geometry.partitioning.AbstractRegion.getSize(AbstractRegion.java:380)
	at datastructures.j3d.bsptree.BSPMesh.<init>(BSPMesh.java:130)
	at datastructures.j3d.bsptree.BSPMesh2.main(BSPMesh2.java:206)
","20/Apr/12 18:16;luc;Hi Andrew,

The file BSPMesh2.java you attached to the issue is not sufficient to reproduce the error. It refers to a BSPMesh class and to a PolygonOfPoints class. It seems changing BSPMesh to BSPMesh2 is sufficient to solver the references in the main method (but I am not sure this class is a drop-in replacement for your issue), but I have no clue about the other class. Could you attach it to the issue too ?","20/Apr/12 20:52;arwillis;New version. Hopefully no missing references. I have removed references to the PolygonOfPoints class and fixed the constructor error. I have created a class specifically for the bug report and apologies for not making it cleaner/more clear.

I'm working on an exact arithmetic version of BSPTrees for CSG operations. I hope to contribute the code if it's of value to the project. Initially i'll use Java's BigDecimal classes for arbitrary arithmetic but I plan on performance enhancements per JR Shewchuk's work (orient3d() etc., http://www.cs.cmu.edu/~quake/robust.html).

hope this code (latest version of BSPMesh2.java) helps.","22/Apr/12 13:10;luc;Thanks for the new version. I was able to reproduce the problem. However, I don't understand what you are trying to achieve here.

In the double loop that creates the 2D SubLine instances, I have added some print statement to see the indices and the start and end points of the built lines. Here is the output I get:

{noformat}
idxA = 0, idxB = 2, idxC = 4
   adding SubLine: {1; -1} {-1; 1}
   adding SubLine: {-1; 1} {-1; 1}
   adding SubLine: {-1; 1} {1; -1}
idxA = 2, idxB = 4, idxC = 6
   adding SubLine: {-1; 1} {-1; 1}
   adding SubLine: {-1; 1} {-1; -1}
   adding SubLine: {-1; -1} {-1; 1}
idxA = 8, idxB = 14, idxC = 12
   adding SubLine: {1; -1} {-1; 1}
   adding SubLine: {-1; 1} {1; 1}
   adding SubLine: {1; 1} {1; -1}
idxA = 14, idxB = 12, idxC = 10
   adding SubLine: {-1; 1} {1; 1}
   adding SubLine: {1; 1} {-1; -1}
   adding SubLine: {-1; -1} {-1; 1}
idxA = 0, idxB = 8, idxC = 10
   adding SubLine: {1; -1} {1; -1}
   adding SubLine: {1; -1} {-1; -1}
   adding SubLine: {-1; -1} {1; -1}
idxA = 8, idxB = 10, idxC = 2
   adding SubLine: {1; -1} {-1; -1}
   adding SubLine: {-1; -1} {-1; 1}
   adding SubLine: {-1; 1} {1; -1}
idxA = 2, idxB = 10, idxC = 12
   adding SubLine: {-1; 1} {-1; -1}
   adding SubLine: {-1; -1} {1; 1}
   adding SubLine: {1; 1} {-1; 1}
idxA = 10, idxB = 12, idxC = 4
   adding SubLine: {-1; -1} {1; 1}
   adding SubLine: {1; 1} {-1; 1}
   adding SubLine: {-1; 1} {-1; -1}
idxA = 4, idxB = 12, idxC = 14
   adding SubLine: {-1; 1} {1; 1}
   adding SubLine: {1; 1} {-1; 1}
   adding SubLine: {-1; 1} {-1; 1}
idxA = 12, idxB = 14, idxC = 6
   adding SubLine: {1; 1} {-1; 1}
   adding SubLine: {-1; 1} {-1; -1}
   adding SubLine: {-1; -1} {1; 1}
idxA = 8, idxB = 0, idxC = 6
   adding SubLine: {1; -1} {1; -1}
   adding SubLine: {1; -1} {-1; -1}
   adding SubLine: {-1; -1} {1; -1}
idxA = 0, idxB = 6, idxC = 14
   adding SubLine: {1; -1} {-1; -1}
   adding SubLine: {-1; -1} {-1; 1}
   adding SubLine: {-1; 1} {1; -1}
lines.size() = 36
{noformat}

Once the 36 sublines have been created, one polygon is created from them.
It seems for each indices triplets, 3 lines are created representing a semi-infinite stripe. As they are all packed togeteher before building the single polygon, I guess the algorithm gets lots in all redundant boundary lines.

Are you sure the inner loop should build only 3 Subline instance and not 4, and are you sure only one polygon should be built from all these lines ?","22/Apr/12 15:38;arwillis;By the output, the index values displayed are all factors of 2. This seems to indicate you are running the constructor for 2D shapes which is commented out with an if (false) {} statement in the main() function.
I you use the constructor:

public BSPMesh2(float[] coords, int[] indices)

Then your indices should be factors of 3.

If you run the 2D shape constructor on the 3D points (cubeIndices, cubeCoords) strange things will occur as you mention.

I've attached yet another BSPMesh2.java with all extraneous code stripped out.

The code constructs a cube from a set of triangles. Each face (having 4 vertices) is triangulated by adding an edge across the diagonal of the face, i.e., a quad face with vertex indices 0 1 2 3 becomes the following two triplets of indices {0 1 2} {2 1 3} as two triangles.
Hence each face (normally 4 SubLines has 6 subLines (one redundant across the diagonal and these redundant lines have opposing directions (subline 1 2 and subline 2 1)). I am working towards converting generic triangular meshes to BSPTrees (and back to meshes). This is a standard for triangular mesh representations.

I hope this helps clarify.","22/Apr/12 16:37;arwillis;Sorry, the indices should be {0 1 2} {1 2 3} above and the subline {1 2} will be redundant. I checked the orientation of the planes for the indices above and one of the two triangles is flipped orientation (incorrectly specified indices).","23/Apr/12 06:24;luc;Hi Andrew,

Thanks for the explanation, now I understand your code. It would be a nice enhancement to have this in Apache Commons Math!

I think there are still some indices issues. As far as I understand, for each facet of the cube, the current indices do not define two triangles sharing one diagonal, but rather two triangles with the crossing diagonals (i.e. they overlap on some part of the fact, and a quarter of the facet is missing. If I replace the indices array from:
{noformat}
{0, 1, 2, 1, 2, 3,
 4, 7, 6, 7, 6, 5,
 0, 4, 5, 4, 5, 1,
 1, 5, 6, 5, 6, 2,
 2, 6, 7, 6, 7, 3,
 4, 0, 3, 0, 3, 7}
{noformat}

to
{noformat}
{0, 1, 2, 2, 3, 0,
 4, 7, 6, 6, 5, 4,
 0, 4, 5, 5, 1, 0,
 1, 5, 6, 6, 2, 1,
 2, 6, 7, 7, 3, 2,
 4, 0, 3, 3, 7, 4}
{noformat}

it seems to work (I had no time for thorough testing though). Note the pattern of the change on the last three columns.

Could you tell me if this work for you ang give expected results ?
","24/Apr/12 02:14;arwillis;The indices you have above work. Thanks. 
Try these
{noformat}
            float[] coordVals = {
1.000000f, -1.000000f, -1.000000f, 
1.000000f, -1.000000f, 1.000000f, 
-1.000000f, -1.000000f, 1.000000f, 
-1.000000f, -1.000000f, -1.000000f, 
1.000000f, 1.000000f, -1f, 
0.999999f, 1.000000f, 1.000000f, 
-1.000000f, 1.000000f, 1.000000f, 
-1.000000f, 1.000000f, -1.000000f};
int[] coordIdxs = {
0, 1, 2, 0, 2, 3, 
4, 7, 6, 4, 6, 5, 
0, 4, 5, 0, 5, 1, 
1, 5, 6, 1, 6, 2, 
2, 6, 7, 2, 7, 3, 
4, 0, 3, 4, 3, 7};
{noformat}
Then change the coord 0.999999f to 1.0f as follows:
{noformat}
float[] coordVals = {
1.000000f, -1.000000f, -1.000000f, 
1.000000f, -1.000000f, 1.000000f, 
-1.000000f, -1.000000f, 1.000000f, 
-1.000000f, -1.000000f, -1.000000f, 
1.000000f, 1.000000f, -1.000000f, 
1.000000f, 1.000000f, 1.000000f, 
-1.000000f, 1.000000f, 1.000000f, 
-1.000000f, 1.000000f, -1.000000f};
coordIdxs = {
0, 1, 2, 0, 2, 3, 
4, 7, 6, 4, 6, 5, 
0, 4, 5, 0, 5, 1, 
1, 5, 6, 1, 6, 2, 
2, 6, 7, 2, 7, 3, 
4, 0, 3, 4, 3, 7};
{noformat}

I get an error on the first set of coordinates but not on the second. The indices are the same. This is the original data which gave rise to the bug report.

Let me know what you find.
thanks,
andrew
","29/Apr/12 20:09;luc;I have made some progress analyzing this issue.
First, I confirm there is a problem in the code.
It appears that during the build, a very thin triangle occurs on a facet. In the facet plane, the coordinates of the three vertices of this triangle are (0.9999999997583233 -0.999998986721039), (-1.0000000000000002 -0.9999989867210387) and (-1.0000000000000002 -1.0). While extracting the vertices of this triangle, the public getVertices method in PolygonsSet first build the segments and then calls the private followLoop method to identify their topology (i.e. how they connect to each other). The segments are properly built and identified, but the followLoop method fails to connect them. It hits a dedicated conditional statement considering the loop is below some predefined threshold (despite it really is above this threshold) and it ignores the triangle completely. Later, this breaks things as a node in the tree without a boundary must be a leaf node which must contain a boolean attribute. However, here we are not a a leaf node, we are at an internal node with an ignored boundary.

So I think there are two bugs here. One bug for not identifying correctly the triangle, and one bug for misclassifying an internal node as a leaf node.

The triangle identification bug will probably take some time to fix. The internal/leaf node bug will probably be easy to fix.

I'll look into that, hopefully in the next few days.
","03/May/12 14:53;arwillis;Thanks for your input. I am still very much interested in resolving this. I'll keep an eye out for your reply.","13/May/12 15:56;luc;Fixed in subversion repository as of r1337929.

The fix is an ugly one. I have only prevented the wrong cast since boolean attributes occur only at leaf nodes in the tree (internal nodes have different attributes, related to the boundary).

The roots of the problem are much deeper than that, and need more thoughts.",,,,,,,,,,,,,,,,,,,
PolynomialFitter.fit() stalls,MATH-798,12558857,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,kpp,kpp,31/May/12 20:22,04/Mar/13 18:53,20/Mar/20 20:33,04/Jul/12 18:01,3.0,,,,,,,3.1,,,0,,,,,,"Hi, in certain cases I ran into the problem that the PolynomialFitter.fit() method stalls, meaning that it does not return, nor throw an Exception (even if it runs for 90 min). Is there a way to tell the PolynomialFitter to iterate only N-times to ensure that my program does not stall?",Mac OS 10.6 and Win XP,,,,,,,,,,,,,"04/Jul/12 02:57;kpp;PolynomialFitterTest.java;https://issues.apache.org/jira/secure/attachment/12535032/PolynomialFitterTest.java",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-07-04 12:57:24.223,,,false,,,,,,,,,,,,,,,,,292303,,,Wed Jul 04 18:01:53 UTC 2012,,,,,,,"0|i0rt5j:",160367,,,,,,,,,,,,,,,,"31/May/12 20:24;kpp;JUnit Test case which fits a PolynomialFitter using a GaussNewtonOptimiaer (stalls) and a LevenbergMarquardtOptimizer (works).","04/Jul/12 02:57;kpp;Test case for Math-798","04/Jul/12 12:57;erans;Your use-case had already been included in the unit test suite (albeit in the ""CurveFitterTest"" class because I initially wanted to remove the ""PolynomialFitter"" class).

MATH-799 discusses that the problem you reported here happens because the default tolerances were much too small. This has been solved (cf. unit test method ""testMath798"" in ""CurveFitterTest"").

However, since the mistake of setting the tolerances at too low values could still happen, I'm going to add a new (overridden) method in ""PolynomialFitter"", where you can explicitly set the number of allowed evaluations of the polynomial during the fit process. This will make it fail early instead of running ""forever"" (not really: the default number of evaluation is ""Integer.MAX_VALUE"").
","04/Jul/12 18:01;erans;Changes performed in revision 1357353.
The unit test ""testMath798"" is now in ""PolynomialFitterTest"".
",,,,,,,,,,,,,,,,,,,,,,,,,,,
Quaternion not normalized after construction,MATH-801,12559346,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,romdicos,romdicos,05/Jun/12 09:31,04/Mar/13 18:53,20/Mar/20 20:33,05/Jun/12 18:29,3.0,,,,,,,3.1,,,0,,,,,,"The use of the Rotation(Vector3D u1,Vector3D u2,Vector3D v1,Vector3D v2) constructor with normalized angle can apparently lead to un-normalized quaternion.
This case appeared to me with the following data :
u1 = (0.9999988431610581, -0.0015210774290851095, 0.0)
u2 = (0.0, 0.0, 1.0)
and 
v1 = (0.9999999999999999, 0.0, 0.0)
v2 = (0.0, 0.0, -1.0)

This lead to the following quaternion :
q0 = 225783.35177064248
q1 = 0.0
q2 = 0.0
q3 = -3.3684446110762543E-9

I was expecting to have a normalized quaternion, as input vector's are normalized. Does the quaternion shouldn't be normalized ?
I've joined the corresponding piece of code as JUnit Test case",,,,,,,,,,,,,,"05/Jun/12 09:33;romdicos;RotationNotNormalised.txt;https://issues.apache.org/jira/secure/attachment/12530927/RotationNotNormalised.txt",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-06-05 18:29:16.194,,,false,,,,,,,,,,,,,,,,,292301,,,Tue Jun 05 18:29:16 UTC 2012,,,,,,,"0|i0rt53:",160365,,,,,,,,,,,,,,,,"05/Jun/12 18:29;luc;Fixed in subversion repository as of r1346513.

Thanks for the report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wide bounds to CMAESOptimizer result in NaN parameters passed to fitness function,MATH-865,12608333,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,fhess,fhess,19/Sep/12 21:10,04/Mar/13 18:53,20/Mar/20 20:33,22/Sep/12 10:04,3.0,,,,,,,3.1,,,0,,,,,,"If you give large values as lower/upper bounds (for example -Double.MAX_VALUE as a lower bound), the optimizer can call the fitness function with parameters set to NaN.  My guess is this is due to FitnessFunction.encode/decode generating NaN when normalizing/denormalizing parameters.  For example, if the difference between the lower and upper bound is greater than Double.MAX_VALUE, encode could divide infinity by infinity.",,,,,,,,,,,,,,"19/Sep/12 22:16;fhess;Math865Test.java;https://issues.apache.org/jira/secure/attachment/12545815/Math865Test.java",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-09-21 15:33:23.163,,,false,,,,,,,,,,,,,,,,,292268,,,Sat Sep 29 22:50:47 UTC 2012,,,,,,,"0|i0rsxr:",160332,,,,,,,,,,,,,,,,"19/Sep/12 21:33;fhess;Also, if the first call to the fitness function returns NaN and gets stored in the bestValue local variable in doOptimize() before the generationLoop, then the optimization will completely fail.  This is because the later comparison if(bestValue > bestFitness) will always return false if bestValue is NaN and no more optimal result will ever be found.","19/Sep/12 22:16;fhess;Attached test program that demonstrates the problem.","19/Sep/12 22:41;fhess;Another issue is, if you use bounds then the bounded range is mapped onto the interval [0,1] where the fit is performed, and then blown back up to your specified bounds after getting a result.  This means if you use a large range, the discreteness of Double variables becomes very noticeable in the fit results.  For example, if you set the boundsMagnitude to 5e15 (approx. 1./Math.ulp(1.)) in the Math865Test program I attached, the fitter is unable to fit anything to a precision smaller than 1.","21/Sep/12 15:33;erans;bq. My guess is this is due to FitnessFunction.encode [...] generating NaN

Good guess.

I've introduced a check on the bounds that now throws an exception in case of overflow.
Please test revision 1388552.

Thanks for the report.
","21/Sep/12 22:47;fhess;Yes, it throws the exception for me now (revision 1388555), thanks.  The secondary issue i mentioned in the comment is still a problem, but I'll open a new bug for that.","29/Sep/12 22:50;erans;This issue is superseded by MATH-867 whose fix entails that an overflow cannot happen anymore.
",,,,,,,,,,,,,,,,,,,,,,,,,
Dfp Dfp.multiply(int x) does not comply with the general contract FieldElement.multiply(int n),MATH-778,12550319,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,celestin,celestin,celestin,10/Apr/12 06:06,04/Mar/13 18:53,20/Mar/20 20:33,21/Oct/12 16:23,3.0,,,,,,,3.1,,,0,,,,,,"In class {{org.apache.commons.math3.Dfp}},  the method {{multiply(int n)}} is limited to {{0 <= n <= 9999}}. This is not consistent with the general contract of {{FieldElement.multiply(int n)}}, where there should be no limitation on the values of {{n}}.",,,,,,,,,,,,,,"23/Sep/12 20:26;tn;MATH-778.patch;https://issues.apache.org/jira/secure/attachment/12546220/MATH-778.patch",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-09-23 20:26:44.629,,,false,,,,,,,,,,,,,,,,,235184,,,Sun Oct 21 16:23:33 UTC 2012,,,,,,,"0|i028wf:",11043,,,,,,,,,,,,,,,,"23/Sep/12 20:26;tn;Hi,

I looked at this issue, and if I understand it correctly, the current multiply(int) method is using a performance shortcut for values of x between 0 and RADIX.

I did a very simple patch to implement the following logic:

 * if 0<=x<RADIX: call multiplyFast with x
 * otherwise create a new Dfp instance with x and call multiply(Dfp) with it","19/Oct/12 15:36;erans;Thomas, Sébastien,

Any reluctance to apply this patch?
","20/Oct/12 08:55;tn;I looked at it as it is one of the few open issues for 3.1. As I am no expert for the Dfp implementation, I did not want to commit it without at least a comment from Sebastien or somebody else who is more proficient in this area than I am.","20/Oct/12 09:08;luc;The patch seems good to me.","21/Oct/12 09:06;celestin;Seems good to me too. Using shift-right operations would result in faster implementations. I remember having tried that at the time I created this issue, but it seems to me I met with an issue, I can't remember what...
So for the time being, let's stick with this patch, and once we get some time, we can look into optimizations.","21/Oct/12 16:23;tn;Applied patch in r1400671.",,,,,,,,,,,,,,,,,,,,,,,,,
"FastMath.[cosh, sinh] do not support the same range of values as the Math counterparts",MATH-905,12617008,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tn,tn,20/Nov/12 20:54,04/Mar/13 18:53,20/Mar/20 20:33,26/Nov/12 13:25,3.0,,,,,,,3.1,,,0,,,,,,"As reported by Jeff Hain:

cosh(double) and sinh(double):
Math.cosh(709.783) = 8.991046692770538E307
FastMath.cosh(709.783) = Infinity
Math.sinh(709.783) = 8.991046692770538E307
FastMath.sinh(709.783) = Infinity
===> This is due to using exp( x )/2 for values of |x|
above 20: the result sometimes should not overflow,
but exp( x ) does, so we end up with some infinity.
===> for values of |x| >= StrictMath.log(Double.MAX_VALUE),
exp will overflow, so you need to use that instead:
for x positive:
double t = exp(x*0.5);
return (0.5*t)*t;
for x negative:
double t = exp(-x*0.5);
return (-0.5*t)*t;",,,,,,,,,,,,,,"23/Nov/12 14:28;erans;MATH-905.diff;https://issues.apache.org/jira/secure/attachment/12554729/MATH-905.diff",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-11-23 14:28:51.706,,,false,,,,,,,,,,,,,,,,,258925,,,Mon Nov 26 13:25:55 UTC 2012,,,,,,,"0|i0l5sv:",121590,,,,,,,,,,,,,,,,"23/Nov/12 14:28;erans;Here is a patch for this issue.

The additional test methods ""testMath905..."" explore the range where the previous code would overflow (around [709, 710] and [-710, -709]).

Please note that the tolerance for those tests had to be increased to ""3"" (whereas it was ""2"" for the range [-30, 30]).

OK to commit?
","25/Nov/12 21:32;tn;Looks good to me.

Any specific reason why you prefer multiplication by 0.5 over division by 2, or just personal preference?
Searching in the FastMath source, there are several occurrences of both variants.","26/Nov/12 00:03;erans;bq. Any specific reason why you prefer multiplication by 0.5 over division by 2, or just personal preference?

I've doing that since having read that multiplication is simpler (thus potentially faster) than division.
Never tried to benchmark.
","26/Nov/12 13:25;erans;Committed revision 1413600.",,,,,,,,,,,,,,,,,,,,,,,,,,,
"In v3, Bundle-SymbolicName should be org.apache.commons.math3 (not org.apache.commons.math as currently)",MATH-876,12610805,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mwebber,mwebber,08/Oct/12 09:22,04/Mar/13 18:53,20/Mar/20 20:33,15/Jan/13 12:23,3.0,,,,,,,3.1,,,0,build,,,,,"In Commons Math 3.0, all package names start with {{org.apache.commons.math3}}, to distinguish them from packages in the previous (2.2) - issue MATH-444.

However, the name of the bundle itself was not similarly changed - in the MANIFEST.MF from 3.0.0, we have this line:
{{Bundle-SymbolicName: org.apache.commons.math}}

This should be changed in 3.1 to:
{{Bundle-SymbolicName: org.apache.commons.math3}}

As an example, Apache Commons Lang changed their bundle name when they moved from v2 to v3 - exactly what I am proposing for Commons Math.

For various reasons, the existing plugin naming is a problem for us in our environment, where our code uses a mixture of 2.2 and 3.0 classes (there are too many references to quickly change).

",,,,,,,,,,,,,,"06/Jan/13 16:47;niallp;MATH876-bundle-name.patch;https://issues.apache.org/jira/secure/attachment/12563490/MATH876-bundle-name.patch",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-10-08 13:18:03.721,,,false,,,,,,,,,,,,,,,,,245519,,,Sun Jan 06 22:48:28 UTC 2013,,,,,,,"0|i06bdz:",34778,,,,,,,,,,,,,,,,"08/Oct/12 13:18;erans;Property ""commons.componentid"" changed to ""math3"" in revision 1395545.
","08/Oct/12 13:52;mwebber;Brilliant - thanks for the prompt fix. I look forward to the 3.1 release!","08/Oct/12 14:27;erans;You could already test with the snapshot that will be generated tonight:

https://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math3/3.1-SNAPSHOT/","18/Oct/12 10:43;mwebber;We are now building against a 3.1 snapshot (with the new bundle name), and have not had any issues so far (based on limited testing).

Many thanks for promptly fixing this ticket.","18/Oct/12 11:01;erans;Thanks for the feedback.
I didn't think using the new ""MANIFEST.MF"" would create any problem; it's rather the release process which might be impacted, so I leave this open until we try it out (hopefully pretty soon).
","06/Jan/13 16:47;niallp;It would be better to override the commons.osgi.symbolicName property rather than changing the componentid to do this - attaching a patch","06/Jan/13 17:48;psteitz;+1 to apply Niall's patch","06/Jan/13 22:48;erans;Changed in revision 1429607.",,,,,,,,,,,,,,,,,,,,,,,
BrentOptimizer: User-defined check block is badly placed,MATH-782,12552120,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,erans,erans,23/Apr/12 12:36,04/Mar/13 18:53,20/Mar/20 20:33,27/Apr/12 23:31,3.0,,,,,,,3.1,,,0,,,,,,"The CM implementation of Brent's original algorithm was supposed to allow for a user-defined stopping criterion (in addition to Brent's default one).
However, it turns out that this additional block of code is not at the right location, implying an unwanted early exit.
",,,,,,,,,,,,,,"23/Apr/12 12:38;erans;MATH-782.patch;https://issues.apache.org/jira/secure/attachment/12523785/MATH-782.patch",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-04-23 14:59:36.073,,,false,,,,,,,,,,,,,,,,,237299,,,Fri Apr 27 23:31:21 UTC 2012,,,,,,,"0|i0rt8n:",160381,,,,,,,,,,,,,,,,"23/Apr/12 12:38;erans;Please have a look at the proposed patch.","23/Apr/12 14:59;luc;I have no specific comment about this fix, just a question.

The previous implementation, check user case after an if/else structure, so it was checked each time. Now it is checked only in one branch. Are we sure we would not always end up in the other branch and never user user check ?","23/Apr/12 18:20;erans;I saw that too. And I'm not sure.
Clearly, what was there before was wrong: If there is no update of ""x"" and ""fx"" before the user-defined check is performed, the ""previous"" and ""current"" will hold the same values, leading to an early exit. The test which I've added fails miserably with the check statement located at its original place.
I don't know the details of the algorithm, but there is only one block where ""x"" and ""fx"" are updated; thus it would seem safe that the user's check is placed there.
","25/Apr/12 14:50;erans;Change applied in revision 1330321.","27/Apr/12 23:19;erans;Luc,

You were right, the check is still not at the right place.
I'll try to fix it shortly.
","27/Apr/12 23:31;erans;Hopefully fixed this time (revision 1331635).",,,,,,,,,,,,,,,,,,,,,,,,,
ListPopulation Iterator allows you to remove chromosomes from the population.,MATH-779,12550553,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,reidhoch,reidhoch,11/Apr/12 15:28,04/Mar/13 18:53,20/Mar/20 20:33,12/Apr/12 18:34,3.0,,,,,,,3.1,,,0,genetics,,,,,Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.,,3600,3600,,0%,3600,3600,,,,,,,"11/Apr/12 15:29;reidhoch;MATH-779.txt;https://issues.apache.org/jira/secure/attachment/12522262/MATH-779.txt",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-04-12 18:34:06.133,,,false,,,,,,,,,,,,,,,,,235417,,,Thu Apr 12 18:34:06 UTC 2012,,,,,,,"0|i0rt9b:",160384,,,,,,,,,,,,,,,,"11/Apr/12 15:29;reidhoch;Proposed patch.","12/Apr/12 18:34;tn;Fixed in r1325427 with minor modifications (updated javadoc, use getChromosomes() instead of Collections.unmodifiableList).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexSolver gives bad results,MATH-781,12551973,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,scheiber@unitbv.ro,scheiber@unitbv.ro,21/Apr/12 07:11,04/Mar/13 18:53,20/Mar/20 20:33,02/May/12 18:29,3.0,,,,,,,3.1,,,0,,,,,,"Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0
in a simple test problem. It works well in commons-math-2.2. ","Windows 7 (64), jdk1.7.0_03",,,,,,,,,,,,,"22/Apr/12 11:29;scheiber@unitbv.ro;ASF.LICENSE.NOT.GRANTED--LinearProgCM.java;https://issues.apache.org/jira/secure/attachment/12523679/ASF.LICENSE.NOT.GRANTED--LinearProgCM.java","22/Apr/12 11:29;scheiber@unitbv.ro;ASF.LICENSE.NOT.GRANTED--LinearProgCM2.java;https://issues.apache.org/jira/secure/attachment/12523680/ASF.LICENSE.NOT.GRANTED--LinearProgCM2.java",,,,,2.0,,,,,,,,,,,,,,,,,,,,2012-04-22 09:03:32.841,,,false,,,,,,,,,,,,,,,,,236782,,,Wed May 02 18:28:52 UTC 2012,,,,,,,"0|i0rt8v:",160382,,,,,,,,,,,,,,,,"22/Apr/12 09:03;luc;Could you give some more information ?

What is the simple problem ? What are the expected results ? What are the results returned by Apache Commons Math 3.0 ?
Could we have some example code so we can reproduce the problem ?","22/Apr/12 11:29;scheiber@unitbv.ro;Hi,

I attached the test codes LinearProgCM.java for commons-math3-3.0 and 
LinearProgCM2.java for commons-math-2.2.

Best regards,
E.Scheiber



","22/Apr/12 14:48;tn;I did a quick check and found no obvious reason for the different behavior. I will do a more thorough check when I am back in 2 weeks.","01/May/12 17:55;tn;in MATH-434, floating-point comparisons have been changed to use either an:

 * epsilon for any comparisons related to algorithm convergence
 * ulp for any other comparisons

Now, when dropping the objective function of the first phase, the comparison is done using ulp which is wrong imho as it is basically a convergence check. When changing this back to an epsilon check like before, the test runs through as expected.","02/May/12 08:06;tn;The algorithm flow is as follows:

 * perform phase 1:
 ** iterate while !optimal
 ** isOptimal uses epsilon
 * after phase 1, drop phase 1 objective function
 ** drop columns using the same criteria as in isOptimal but with ulp instead of epsilon
 * perform phase 2
 ** ..

After finishing phase 1, we end up dropping columns based on a different epsilon (ulp) as in the convergence check of the iteration for phase 1.","02/May/12 18:28;tn;Fixed in r1333146.",,,,,,,,,,,,,,,,,,,,,,,,,
"In RealVector, dotProduct and outerProduct return wrong results due to misuse of sparse iterators",MATH-812,12597900,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,celestin,celestin,celestin,07/Jul/12 12:01,04/Mar/13 18:53,20/Mar/20 20:33,12/Jul/12 13:38,3.0,,,,,,,3.1,,,0,linear,sparse,,,,"In class {{RealVector}}, the default implementation of {{RealMatrix outerProduct(RealVector)}} uses sparse iterators on the entries of the two vectors. The rationale behind this is that {{0d * x == 0d}} is {{true}} for all {{double x}}. This assumption is in fact false, since {{0d * NaN == NaN}}.

Proposed fix is to loop through *all* entries of both vectors. This can have a significant impact on the CPU cost, but robustness should probably be preferred over speed in default implementations.

Same issue occurs with {{double dotProduct(RealVector)}}, which uses sparse iterators for {{this}} only.

Another option would be to through an exception if {{isNaN()}} is {{true}}, in which case caching could be used for both {{isNaN()}} and {{isInfinite()}}.",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-07-07 12:30:01.698,,,false,,,,,,,,,,,,,,,,,292295,,,Thu Jul 12 13:38:05 UTC 2012,,,,,,,"0|i0rt3r:",160359,,,,,,,,,,,,,,,,"07/Jul/12 12:30;erans;bq. [...] robustness should probably be preferred over speed in default implementations.

+1
","12/Jul/12 13:19;celestin;In {{r1360662}}, {{RealVector.dotProduct(RealVector)}} now loops through all entries of the vectors.","12/Jul/12 13:33;celestin;In {{r1360668}}, {{RealVector.outerProduct(RealVector)}} now loops through all entries of the vectors.","12/Jul/12 13:38;celestin;Unit tests now pass, at the price of a performance loss.",,,,,,,,,,,,,,,,,,,,,,,,,,,
inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.,MATH-718,12533757,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,uchiyama,uchiyama,04/Dec/11 00:40,04/Mar/13 18:53,20/Mar/20 20:33,21/May/12 19:56,2.2,3.0,,,,,,3.1,,,1,,,,,,"The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem.

{{System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5));}}

This returns 499525, though it should be 499999.

I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.",,,,,,,,,,,,MATH-785,,"20/May/12 21:59;tn;MATH-718.diff;https://issues.apache.org/jira/secure/attachment/12528359/MATH-718.diff",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-12-04 09:22:55.786,,,false,,,,,,,,,,,,,,,,,219484,,,Mon May 21 19:56:36 UTC 2012,,,,,,,"0|i0rtf3:",160410,,,,,,,,,,,,,,,,"04/Dec/11 09:22;celestin;Hi Yuji,
thanks for reporting this. For version 3.0, we are currently reshaping the package distribution, and this will probably get resolved once we are over with MATH-692.
Best regards,
Sébastien","15/Dec/11 23:55;cwinter;There seem to be stability problems with Beta.regularizedBeta(...) when using extreme parameters. {{PascalDistribution.cumulativeProbability(Integer.MAX_VALUE)}} returns {{NaN}} instead of 1. We should look for a way to avoid both infinite values and NaNs in the implementation of the regularized beta function.","04/Feb/12 06:33;celestin;As rightly pointed out by Christian, this issue is strongly related to MATH-738.","04/Feb/12 06:35;celestin;Sorry, I only meant to postpone this issue.","12/Apr/12 17:09;jhkauf;I just wanted to let you know that our open source project (http://www.eclipse.org/stem/) needs this function and we are eagerly awaiting the update. We are experiencing the issue of wrong values for large trials.","12/Apr/12 18:48;celestin;Hi James,
thanks for your interest. STEM is a very interesting project!
I will try and find a fix for this issue as soon as possible. Any ideas are welcome!
Sébastien","13/Apr/12 13:20;tn;The problem Christian described wrt the PascalDistribution is a simple integer overflow in the class itself:

{noformat}
    public double cumulativeProbability(int x) {
        double ret;
        if (x < 0) {
            ret = 0.0;
        } else {
            ret = Beta.regularizedBeta(probabilityOfSuccess,
                    numberOfSuccesses, x + 1);
        }
        return ret;
    }
{noformat}

when x = Integer.MAX_VALUE, adding 1 to it will result in an overflow. As the parameter of regularizedBeta is anyway a double, so it should be changed to something like ""1L + x"" to enforce a long addition.

Edit: Similar things happen btw also in other Distribution implementations, so it should be fixed also there, e.g. BinomialDistribution","13/Apr/12 13:57;tn;The problem is not only related to the Beta function, also the ContinuedFraction.evaluate is numerically unstable.

The reason the cumulativeProbability returns infinity instead of NaN is because the evaluate return 0.0 when called from Beta.regularizedBeta, which leads to a division by zero. The used default epsilon of 10e-15 seems also quite restrictive, when relaxing the epsilon I got much better results (e.g. with 10e-5 I got a result of 499997).","20/May/12 21:42;tn;I further looked into this with relation to MATH-785. First of all, in the original bug report, the reporter mentions that the expected result should be 499999 which is wrong, imho it should be 500000.

After implementing the modified Lentz-Thompson algorithm, the results for the BinomialDistribution of large trials show correct results.","20/May/12 21:55;tn;The attached diff file shows the (preliminary) implementation of the modified Lentz-Thompson algorithm.

Edit: re-uploaded the diff file as it was broken.

Edit2: the failing unit tests I mentioned before were due to a wrong loop, the latest diff shows no unit test errors.","21/May/12 19:56;tn;Fixed in r1341171.",,,,,,,,,,,,,,,,,,,,
"FastMath.pow deviates from Math.pow for negative, finite base values with an exponent 2^52 < y < 2^53 ",MATH-904,12617007,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tn,tn,20/Nov/12 20:51,04/Mar/13 18:53,20/Mar/20 20:33,26/Nov/12 23:03,3.0,,,,,,,3.1,,,0,,,,,,"As reported by Jeff Hain:

pow(double,double):
Math.pow(-1.0,5.000000000000001E15) = -1.0
FastMath.pow(-1.0,5.000000000000001E15) = 1.0
===> This is due to considering that power is an even
integer if it is >= 2^52, while you need to test
that it is >= 2^53 for it.
===> replace
""if (y >= TWO_POWER_52 || y <= -TWO_POWER_52)""
with
""if (y >= 2*TWO_POWER_52 || y <= -2*TWO_POWER_52)""
and that solves it.",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-11-21 21:48:21.357,,,false,,,,,,,,,,,,,,,,,258924,,,Mon Nov 26 23:03:35 UTC 2012,,,,,,,"0|i0l5sn:",121589,,,,,,,,,,,,,,,,"21/Nov/12 18:42;tn;Does anyone know the reason why numbers > 2^53 are assumed to be even?

Looking at the javadoc of Math.pow, this is not mentioned, only the general case:

{noformat}
If the first argument is finite and less than zero
    if the second argument is a finite even integer, the result is equal to the result of raising the absolute value of the first argument to the power of the second argument
    if the second argument is a finite odd integer, the result is equal to the negative of the result of raising the absolute value of the first argument to the power of the second argument
    if the second argument is finite and not an integer, then the result is NaN. 
{noformat}","21/Nov/12 21:48;luc;bq. Does anyone know the reason why numbers > 2^53 are assumed to be even?

Because the mantissa of a double number encoding using IEEE-754 cannot handle a sufficient number of digits. If the Most Significant Bit is large enough, the Least Significant Bit becomes equal to 2.0 (and later when you still increase the MSB, then the LSB will become 4.0, and after that 8.0...

This is the essence of ""floating"" in floating point numbers. The decimal separator ""floats"", up to the end of the number than it slips out of the number.","22/Nov/12 09:44;tn;ah ok, thanks a lot, I found also an explanation here [http://en.wikipedia.org/wiki/Double-precision_floating-point_format]","26/Nov/12 23:03;erans;Fixed in revision 1413916.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Numerical Underflow in ContinuedFraction,MATH-785,12553447,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,cjfuller,cjfuller,01/May/12 02:54,04/Mar/13 18:53,20/Mar/20 20:33,21/May/12 20:01,3.0,,,,,,,3.1,,,0,,,,,,"The ContinuedFraction calculation can underflow in the evaluate method, similar to the overflow case already dealt with.  I encountered this problem while trying to evaluate the inverse cumulative probability of an F distribution with a large number of degrees of freedom.

I would guess this has the same cause as MATH-718 and MATH-738, though I am not experiencing inaccurate results but rather an exception.

For instance, the following test case fails:

double prob = 0.01;
FDistribution f = new FDistribution(200000, 200000);
double fails = f.inverseCumulativeProbability(prob);

This produces a NoBracketingException with the following stack trace:

org.apache.commons.math3.exception.NoBracketingException: function values at endpoints do not have different signs, endpoints: [0, 1], values: [-0.01, -∞]
	at org.apache.commons.math3.analysis.solvers.BrentSolver.doSolve(BrentSolver.java:118)
	at org.apache.commons.math3.analysis.solvers.BaseAbstractUnivariateSolver.solve(BaseAbstractUnivariateSolver.java:190)
	at org.apache.commons.math3.analysis.solvers.BaseAbstractUnivariateSolver.solve(BaseAbstractUnivariateSolver.java:195)
	at org.apache.commons.math3.analysis.solvers.UnivariateSolverUtils.solve(UnivariateSolverUtils.java:77)
	at org.apache.commons.math3.distribution.AbstractRealDistribution.inverseCumulativeProbability(AbstractRealDistribution.java:156)

I could avoid the issue as in the comment to MATH-718 by relaxing the default value of epsilon in ContinuedFraction, although in my test case I can't see any reason the current default precision shouldn't be attainable.

I fixed the issue by implementing underflow detection in ContinuedFraction and rescaling to larger values similarly to how the overflow detection that is already there works.  I will attach a patch shortly.

One possible issue with this fix is that if there exists a case where there is a legitimate reason for p2 or q2 to be zero (I cannot think of one), it might break that case.","Issue seen in both 3.0 release binary version as well as a fresh checkout of the subversion trunk.

java -version output:

java version ""1.6.0_26""
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)

(On Ubuntu 12.04)




",,,,,,,,,,,,,"01/May/12 02:55;cjfuller;patch.txt;https://issues.apache.org/jira/secure/attachment/12525151/patch.txt",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-05-10 21:47:41.889,,,false,,,,,,,,,,,,,,,,,237609,,,Mon May 21 20:01:03 UTC 2012,,,,,,,"0|i0rt87:",160379,,,,,,,,,,,,,,,,"01/May/12 02:55;cjfuller;Patch to fix the numerical underflow problem in ContinuedFraction.","10/May/12 21:47;tn;I have looked into this patch, and it looks very reasonable.

My original experiments with the epsilon were just scraping on the symptom but this seems to deal with the actual cause of the numerical instability problems.

Results of distributions using this fix also greatly improved to the situation before.","20/May/12 21:38;tn;I looked further into it and am not convinced anymore that this really to solve the numerical stability problems. In fact the results are pretty much random depending on the choice of the scaling factor.

In fact I implemented the modified Lentz-Thompson algorithm to do the continued fraction evaluation and the results are much much better. All the unit tests run through and the probability evaluations for the different distributions for large trials are stable and return correct values.","21/May/12 20:01;tn;The problem has fixed together with MATH-718. Instead of applying the attached patch, the evaluation of the continued fraction has been changed to the modified Lentz-Thompson algorithm which does not suffer from underflow/overflow problems as the original implementation. A test case for the described problem has been added too.

Thanks for the report.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Polygon difference produces erronious results in some cases,MATH-880,12611561,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,curtis,curtis,12/Oct/12 17:31,04/Mar/13 18:53,20/Mar/20 20:33,22/Oct/12 19:28,3.0,,,,,,,3.1,,,0,,,,,,"The 2D polygon difference method is returning incorrect
results.  Below is a test case of subtracting two polygons (Sorry,
this is the simplest case that I could find that duplicates the
problem).  

There are three problems with the result. The first is that the first
point of the first set of vertices is null (and the first point of the
second set is also null).  The second is that, even if the first null
points are ignored,  the returned polygon is not the correct result.
The first and last points are way off, and the remaining points do not
match the original polygon boundaries.  Additionally, there are two
holes that are returned in the results.  This subtraction case should
not have holes.

{code:title=""Complex Polygon Difference Test""}
public void testComplexDifference() {
        Vector2D[][] vertices1 = new Vector2D[][] {
            new Vector2D[] {
                    new Vector2D( 90.08714908223715,  38.370299337260235),
                    new Vector2D( 90.08709517675004,  38.3702895991413),
                    new Vector2D( 90.08401538704919,  38.368849330127944),
                    new Vector2D( 90.08258210430711,  38.367634558585564),
                    new Vector2D( 90.08251455106665,  38.36763409247078),
                    new Vector2D( 90.08106599752608,  38.36761621664249),
                    new Vector2D( 90.08249585300035,  38.36753627557965),
                    new Vector2D( 90.09075743352184,  38.35914647644972),
                    new Vector2D( 90.09099945896571,  38.35896264724079),
                    new Vector2D( 90.09269383800086,  38.34595756121246),
                    new Vector2D( 90.09638631543191,  38.3457988093121),
                    new Vector2D( 90.09666417351019,  38.34523360999418),
                    new Vector2D( 90.1297082145872,  38.337670454923625),
                    new Vector2D( 90.12971687748956,  38.337669827794684),
                    new Vector2D( 90.1240820219179,  38.34328502001131),
                    new Vector2D( 90.13084259656404,  38.34017811765017),
                    new Vector2D( 90.13378567942857,  38.33860579180606),
                    new Vector2D( 90.13519557833206,  38.33621054663689),
                    new Vector2D( 90.13545616732307,  38.33614965452864),
                    new Vector2D( 90.13553111202748,  38.33613962818305),
                    new Vector2D( 90.1356903436448,  38.33610227127048),
                    new Vector2D( 90.13576283227428,  38.33609255422783),
                    new Vector2D( 90.13595870833188,  38.33604606376991),
                    new Vector2D( 90.1361556630693,  38.3360024198866),
                    new Vector2D( 90.13622408795709,  38.335987048115726),
                    new Vector2D( 90.13696189099994,  38.33581914328681),
                    new Vector2D( 90.13746655304897,  38.33616706665265),
                    new Vector2D( 90.13845973716064,  38.33650776167099),
                    new Vector2D( 90.13950901827667,  38.3368469456463),
                    new Vector2D( 90.14393814424852,  38.337591835857495),
                    new Vector2D( 90.14483839716831,  38.337076122362475),
                    new Vector2D( 90.14565474433601,  38.33769000964429),
                    new Vector2D( 90.14569421179482,  38.3377117256905),
                    new Vector2D( 90.14577067124333,  38.33770883625908),
                    new Vector2D( 90.14600350631684,  38.337714326520995),
                    new Vector2D( 90.14600355139731,  38.33771435193319),
                    new Vector2D( 90.14600369112401,  38.33771443882085),
                    new Vector2D( 90.14600382486884,  38.33771453466096),
                    new Vector2D( 90.14600395205912,  38.33771463904344),
                    new Vector2D( 90.14600407214999,  38.337714751520764),
                    new Vector2D( 90.14600418462749,  38.337714871611695),
                    new Vector2D( 90.14600422249327,  38.337714915811034),
                    new Vector2D( 90.14867838361471,  38.34113888210675),
                    new Vector2D( 90.14923750157374,  38.341582537502575),
                    new Vector2D( 90.14877083250991,  38.34160685841391),
                    new Vector2D( 90.14816667319519,  38.34244232585684),
                    new Vector2D( 90.14797696744586,  38.34248455284745),
                    new Vector2D( 90.14484318014337,  38.34385573215269),
                    new Vector2D( 90.14477919958296,  38.3453797747614),
                    new Vector2D( 90.14202393306448,  38.34464324839456),
                    new Vector2D( 90.14198920640195,  38.344651155237216),
                    new Vector2D( 90.14155207025175,  38.34486424263724),
                    new Vector2D( 90.1415196143314,  38.344871730519),
                    new Vector2D( 90.14128611910814,  38.34500196593859),
                    new Vector2D( 90.14047850603913,  38.34600084496253),
                    new Vector2D( 90.14045907000337,  38.34601860032171),
                    new Vector2D( 90.14039496493928,  38.346223030432384),
                    new Vector2D( 90.14037626063737,  38.346240203360026),
                    new Vector2D( 90.14030005823724,  38.34646920000705),
                    new Vector2D( 90.13799164754806,  38.34903093011013),
                    new Vector2D( 90.11045289492762,  38.36801537312368),
                    new Vector2D( 90.10871471476526,  38.36878044144294),
                    new Vector2D( 90.10424901707671,  38.374300101757),
                    new Vector2D( 90.10263482039932,  38.37310041316073),
                    new Vector2D( 90.09834601753448,  38.373615053823414),
                    new Vector2D( 90.0979455456843,  38.373578376172475),
                    new Vector2D( 90.09086514328669,  38.37527884194668),
                    new Vector2D( 90.09084931407364,  38.37590801712463),
                    new Vector2D( 90.09081227075944,  38.37526295920463),
                    new Vector2D( 90.09081378927135,  38.375193883266434)
            }
        };
        PolygonsSet set1 = buildSet(vertices1);

        Vector2D[][] vertices2 = new Vector2D[][] {
            new Vector2D[] {
                    new Vector2D( 90.13067558880044,  38.36977255037573),
                    new Vector2D( 90.12907570488,  38.36817308242706),
                    new Vector2D( 90.1342774136516,  38.356886880294724),
                    new Vector2D( 90.13090330629757,  38.34664392676211),
                    new Vector2D( 90.13078571364593,  38.344904617518466),
                    new Vector2D( 90.1315602208914,  38.3447185040846),
                    new Vector2D( 90.1316336226821,  38.34470643148342),
                    new Vector2D( 90.134020944832,  38.340936644972885),
                    new Vector2D( 90.13912536387306,  38.335497255122334),
                    new Vector2D( 90.1396178806582,  38.334878075552126),
                    new Vector2D( 90.14083049696671,  38.33316530644106),
                    new Vector2D( 90.14145252901329,  38.33152722916191),
                    new Vector2D( 90.1404779335565,  38.32863516047786),
                    new Vector2D( 90.14282712131586,  38.327504432532066),
                    new Vector2D( 90.14616669875488,  38.3237354115015),
                    new Vector2D( 90.14860976050608,  38.315714862457924),
                    new Vector2D( 90.14999277782437,  38.3164932507504),
                    new Vector2D( 90.15005207194997,  38.316534677663356),
                    new Vector2D( 90.15508513859612,  38.31878731691609),
                    new Vector2D( 90.15919938519221,  38.31852743183782),
                    new Vector2D( 90.16093758658837,  38.31880662005153),
                    new Vector2D( 90.16099420184912,  38.318825953291594),
                    new Vector2D( 90.1665411125756,  38.31859497874757),
                    new Vector2D( 90.16999653861313,  38.32505772048029),
                    new Vector2D( 90.17475243391698,  38.32594398441148),
                    new Vector2D( 90.17940844844992,  38.327427213761325),
                    new Vector2D( 90.20951909541378,  38.330616833491774),
                    new Vector2D( 90.2155400467941,  38.331746223670336),
                    new Vector2D( 90.21559881391778,  38.33175551425302),
                    new Vector2D( 90.21916646426041,  38.332584299620805),
                    new Vector2D( 90.23863749852285,  38.34778978875795),
                    new Vector2D( 90.25459855175802,  38.357790570608984),
                    new Vector2D( 90.25964298227257,  38.356918010203174),
                    new Vector2D( 90.26024593994703,  38.361692743151366),
                    new Vector2D( 90.26146187570015,  38.36311080550837),
                    new Vector2D( 90.26614159359622,  38.36510808579902),
                    new Vector2D( 90.26621342936448,  38.36507942500333),
                    new Vector2D( 90.26652190211962,  38.36494042196722),
                    new Vector2D( 90.26621240678867,  38.365113172030874),
                    new Vector2D( 90.26614057102057,  38.365141832826794),
                    new Vector2D( 90.26380080055299,  38.3660381760273),
                    new Vector2D( 90.26315345241,  38.36670658276421),
                    new Vector2D( 90.26251574942881,  38.367490323488084),
                    new Vector2D( 90.26247873448426,  38.36755266444749),
                    new Vector2D( 90.26234628016698,  38.36787989125406),
                    new Vector2D( 90.26214559424784,  38.36945909356126),
                    new Vector2D( 90.25861728442555,  38.37200753430875),
                    new Vector2D( 90.23905557537864,  38.375405314295904),
                    new Vector2D( 90.22517251874075,  38.38984691662256),
                    new Vector2D( 90.22549955153215,  38.3911564273979),
                    new Vector2D( 90.22434386063355,  38.391476432092134),
                    new Vector2D( 90.22147729457276,  38.39134652252034),
                    new Vector2D( 90.22142070120117,  38.391349167741964),
                    new Vector2D( 90.20665060751588,  38.39475580900313),
                    new Vector2D( 90.20042268367109,  38.39842558622888),
                    new Vector2D( 90.17423771242085,  38.402727751805344),
                    new Vector2D( 90.16756796257476,  38.40913898597597),
                    new Vector2D( 90.16728283954308,  38.411255399912875),
                    new Vector2D( 90.16703538220418,  38.41136059866693),
                    new Vector2D( 90.16725865657685,  38.41013618805954),
                    new Vector2D( 90.16746107640665,  38.40902614307544),
                    new Vector2D( 90.16122795307462,  38.39773101873203)
            }
        };
        PolygonsSet set2 = buildSet(vertices2);
        PolygonsSet set  = (PolygonsSet) new
RegionFactory<Euclidean2D>().difference(set1.copySelf(),

              set2.copySelf());

        Vector2D[][] verticies = set.getVertices();
        Assert.assertTrue(verticies[0][0] != null);
        Assert.assertEquals(1, verticies.length);
    }
{code}",,,,,,,,,,,,,,"12/Oct/12 17:37;curtis;PolygonDiffAll.png;https://issues.apache.org/jira/secure/attachment/12548927/PolygonDiffAll.png","12/Oct/12 17:37;curtis;PolygonDiffResults.png;https://issues.apache.org/jira/secure/attachment/12548928/PolygonDiffResults.png","12/Oct/12 17:35;curtis;PolygonInputs.png;https://issues.apache.org/jira/secure/attachment/12548926/PolygonInputs.png",,,,3.0,,,,,,,,,,,,,,,,,,,,2012-10-17 14:43:46.056,,,false,,,,,,,,,,,,,,,,,248021,,,Tue Oct 30 19:54:41 UTC 2012,,,,,,,"0|i09ahz:",52134,,,,,,,,,,,,,,,,"12/Oct/12 17:35;curtis;Plot of the input polygons.","12/Oct/12 17:37;curtis;Attached images of difference results (by themselves and overlayed on the inputs)","12/Oct/12 17:37;curtis;FYI:  I have discovered, that if I change the decimal precision of the
input polygons, I get a much better result (no nulls, and it looks
almost correct).  I changed the inputs to have only 7 digits after the
decimal place.  Though, it is still the incorrect results (just much
closer).","12/Oct/12 17:37;curtis;I've also discovered  that if I translate the points to left by 89
units (I changed 90 to 1), that the results are even better.","17/Oct/12 14:43;luc;I confirm the issue.

The null points probably come from the algorithm thinking some boundaries are not closed, i.e. they extend to infinity. This is the way they are represented when transforming from BSP to boundary representation. So the problem is not the nul points by themselves, but trather why the algorithm fails to see the boundary should remain closed here.

There is probably a numerical problem here. It will require some investigations.","17/Oct/12 15:37;luc;I have reduced the problem to the following case with fewer points so it is easier to debug:

{code}
    @Test
    public void testIssue880() {

        Vector2D[][] vertices1 = new Vector2D[][] {
            new Vector2D[] {
                    new Vector2D( 90.13595870833188,  38.33604606376991),
                    new Vector2D( 90.14047850603913,  38.34600084496253),
                    new Vector2D( 90.11045289492762,  38.36801537312368),
                    new Vector2D( 90.10871471476526,  38.36878044144294),
                    new Vector2D( 90.10424901707671,  38.374300101757),
                    new Vector2D( 90.0979455456843,   38.373578376172475),
                    new Vector2D( 90.09081227075944,  38.37526295920463),
                    new Vector2D( 90.09081378927135,  38.375193883266434)
            }
        };

        Vector2D[][] vertices2 = new Vector2D[][] {
            new Vector2D[] {
                    new Vector2D( 90.13067558880044,  38.36977255037573),
                    new Vector2D( 90.1342774136516,   38.356886880294724),
                    new Vector2D( 90.13090330629757,  38.34664392676211),
                    new Vector2D( 90.15508513859612,  38.31878731691609),
                    new Vector2D( 90.15919938519221,  38.31852743183782),
            }
        };
        PolygonsSet set  =
                (PolygonsSet) new RegionFactory<Euclidean2D>().difference(buildSet(vertices1),
                                                                          buildSet(vertices2));

        Vector2D[][] verticies = set.getVertices();
        Assert.assertTrue(verticies[0][0] != null);
        Assert.assertEquals(1, verticies.length);

    }
{code}","17/Oct/12 16:49;luc;The problem does not lie in the polygon difference, but in the creation of the first polygon, the one created using buildset(vertices1).
Instead of being a simple closed shape, it extends to infinity in two branches. This is easily seen by using simple points checks to see which points are inside as follows:

{code}
        for (double x = 90.0; x < 90.15; x += 0.0005) {
            for (double y = 38.33; y < 38.4; y += 0.0005) {
                Vector2D p = new Vector2D(x, y);
                if (set1.checkPoint(p) == Location.INSIDE) {
                    System.out.println(x + "" "" + y);
                }
            }
        }
{code}

So we have to fix the polygon building from boundary representation.","18/Oct/12 13:41;luc;Here are some news about the investigations done on this issue.

The problem is indeed due to some numerical issues. During the conversion from boundary representation to BSP tree, we build edges from successive pair of points {p0, p1}, {p1, p2}, {p2, p3} ... These edges belong to hyperplanes (i.e. lines) h01, h12, h23 ... These hyperplanes are then inserted into a BSP tree which is built top down. What happens here is that as h12 is inserted in a tree which already contains h01, we compute the intersection of the two hyperplanes and find a point p1' slightly different from p1 due to numerical considerations. This points extends the edge very slightly, and it is considered afterwards to extend on both sides of h01 instead of one side only. So both sides of hyperplane h01 are split by hyperplane h12. The side that should not have been split now contains an additional artificial cut.

I am still looking for a solution to this. It seems doubtful to be able to completely avoid numerical problems and avoid this extra splitting. However, BSP trees allow extra cut hyperplanes to appear in the tree without belonging to the boundary (this happens for example with non-convex shapes). These extra cut sub-hyperplane simply define two neighboring cells which are both inside or outside the polygon. I am looking at a way to detect this case more efficiently and avoid creating an artificial ""inside"" cell.

This is a difficult problem.","18/Oct/12 15:14;curtis;Thanks for looking into.  We were also seeing problems with the inside detection and figured that there may be some connection between the contains and the difference problems.  We looked through the code to attempt to find the problem.  We soon realized that we would need to appeal to outside help.  So, again, thanks for looking at the problem.","18/Oct/12 15:57;luc;Curtis, I was wondering if the API we propose for building the polygons is a good one.
It was created this way because it is dimension-independent and is in fact implemented at a rather high level (in the AbstractRegion class).

Perhaps we should provide you an additional way to build polygons, which would be 2D-specific and would not lose topologic information like the general case obviously does now. When you build your polygons, do you have some additional knowledge about it? It seems you already have continuous loops and have organized your points in a logical way. Is this always the case? Could we assume that you know how the various segments are connected together, could we assume you always have closed shapes, could we assume you loops around the shapes with the inside on your left? If so, do you think having a dedicated API for 2D-polygons would be useful to you?","21/Oct/12 10:04;luc;Here is an update about this issue.

I have created a new constructor using only a simple boundary loop as a sequence of vertices:
{code}
    /** Build a polygon from a simple list of vertices.
     * <p>The boundary is provided as a list of points considering to
     * represent the vertices of a simple loop. The interior part of the
     * region is on the left side of this path and the exterior is on its
     * right side.</p>
     * <p>This constructor does not handle polygons with a boundary
     * forming several disconnected paths (such as polygons with holes).</p>
     * <p>For cases where this simple constructor applies, it is expected to
     * be numerically more robust than the {@link #PolygonsSet(Collection) general
     * constructor} using {@link SubHyperplane subhyperplanes}.</p>
     * <p>If the list is empty, the region will represent the whole
     * space.</p>
     * @param hyperplaneThickness tolerance below which points are consider to
     * belong to the hyperplane (which is therefore more a slab)
     * @param vertices vertices of the simple loop boundary
     */
    public PolygonsSet(final double hyperplaneThickness, final Vector2D ... vertices) {
      ...
    }
{code}

This constructor avoid the two way conversion: vertices -> hyperplanes -> rebuilt vertices. This two way conversion induces the problem as the vertices rebuilt from hyperplanes intersections are numerically slightly different from the original vertices. It does so by preserving the initial vertices all the way through, so it uses only the the first part of the conversion: vertices -> hyperplanes and never comes backs from hyperplanes to vertices.

As explained in the javadoc, this constructor does not handle boundaries in several paths. So if for example a polygon with a hole is to be built, then one polygon without the hole and hole itself can be built separately from this constructor, and the final polygon with a hole would be created by subtracting the hole from the polygon. I don't think the limitation is too cumbersome for users, so I think I will let the API as is.

This works well on the reduced case I presented in earlier comments above, but currently fails miserably on the initial bug report.
I am working on it.","21/Oct/12 20:11;luc;A partial fix for the problem has been committed in subversion as of r1400717.

There are still some problems with the test case, as shown by two TODO markers in the unit tests.

Could you check if this fix at least partially works for you?","22/Oct/12 14:38;luc;Well, the remaining problem is worse than expected. The current version even fails on the simplistic following case:

{code}
Vector2D[] vertices = new Vector2D[] {
    new Vector2D(-6, -4), new Vector2D(-8, -8), new Vector2D(  8, -8),
    new Vector2D( 6, -4), new Vector2D(10,  4), new Vector2D(-10,  4)
};
PolygonsSet set = new PolygonsSet(1.0e-10, vertices);
{code}

Still working on it :-(","22/Oct/12 19:28;luc;Fixed in subversion repository as of r1401022.

Note that the unit test corresponding to the issue uses a value equal to 1.0e-8 for the hyperplaneThickness parameter. Smaller values do not work due to numerical issues. This new parameter is therefore an important one for such problems and too small values should be avoided.

There are no definitive value that can be suggested and would work in every case. The appropriate value depends on the polygon (does it have thin pikes or dents, does it have vertices with almost flat angles, does it have independent edges that happen to be almost aligned despite they are not directly connected ..).

Thanks for reporting the problem.","30/Oct/12 19:54;curtis;The new PolygonsSet constructor fixes the problems we have been seeing.  So far 1.0e-8 works for all of our cases.  If it ever doesn't work, we'll come up with a heuristic for setting the thickness.

To answer the earlier questions about how we organize our data, we have created a layer on top of Apache Math.  We have developed our own internal API for different general euclidean representations and operations that we use on different projects.   We work both in 3D and 2D coordinate spaces.  Our polygon classes contain a clockwise ordered array of points.  In all cases, our polygons are sequentially connected, fully continuous, and closed (nothing fancy).  We keep track of holes as polygons defined in the same way.  We have methods to convert and order points in many different fashions.  Our 3D space is almost exclusively topographical in nature, so we generally reduce to 2D for most geometric calculations.  In addition to a topographical third dimension we do sometimes keep track of other information on those points (such as a altitude or measurement error value).  The PolygonsSet constructor you have created should work nicely for us.  If there was a way to build 2D polygons but keep the topological information (or other information), that would be nice; though, we are able to work with whatever you provide.

Because we have an abstraction layer on top of Apache Math, we should be able to adapt to any API that is provided.  Before Apache Math 3, we had a combination of our home grown code and Sun's Vecmath (Thanks for Apache Math 3; it fixes some limitations we had previously).  In that regard, the API for building polygons is ""a good one.""  It does what we need.  As long as there are examples, test cases, and documentation we'll figure it out.  The Apache Math API has many capabilities we don't need and so the building of Polygons is probably not as simple as it could be for what we are doing.  Now that you know roughly how we are using it, I'll leave it up to as to how ""good"" the API is for building polygons. One limitation I would point out is the absence of non-polygonal shapes like circles/spheres, ellipses/ellipsoids, cylinders, etc.  All in all, we are happy with it and very appreciative.

Thanks again, and Happy Halloween
",,,,,,,,,,,,,,,,
Correlated random vector generator fails (silently) when faced with zero rows in covariance matrix,MATH-789,12554463,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,gertvv,gertvv,09/May/12 10:47,04/Mar/13 18:53,20/Mar/20 20:33,13/Sep/12 15:19,3.0,,,,,,,3.1,,,0,,,,,,"The following three matrices (which are basically permutations of each other) produce different results when sampling a multi-variate Gaussian with the help of CorrelatedRandomVectorGenerator (sample covariances calculated in R, based on 10,000 samples):

Array2DRowRealMatrix{
{0.0,0.0,0.0,0.0,0.0},
{0.0,0.013445532,0.01039469,0.009881156,0.010499559},
{0.0,0.01039469,0.023006616,0.008196856,0.010732709},
{0.0,0.009881156,0.008196856,0.019023866,0.009210099},
{0.0,0.010499559,0.010732709,0.009210099,0.019107243}}

> cov(data1)
   V1 V2 V3 V4 V5
V1 0 0.000000000 0.00000000 0.000000000 0.000000000
V2 0 0.013383931 0.01034401 0.009913271 0.010506733
V3 0 0.010344006 0.02309479 0.008374730 0.010759306
V4 0 0.009913271 0.00837473 0.019005488 0.009187287
V5 0 0.010506733 0.01075931 0.009187287 0.019021483

Array2DRowRealMatrix{
{0.013445532,0.01039469,0.0,0.009881156,0.010499559},
{0.01039469,0.023006616,0.0,0.008196856,0.010732709},
{0.0,0.0,0.0,0.0,0.0},
{0.009881156,0.008196856,0.0,0.019023866,0.009210099},
{0.010499559,0.010732709,0.0,0.009210099,0.019107243}}

> cov(data2)
            V1 V2 V3 V4 V5
V1 0.006922905 0.010507692 0 0.005817399 0.010330529
V2 0.010507692 0.023428918 0 0.008273152 0.010735568
V3 0.000000000 0.000000000 0 0.000000000 0.000000000
V4 0.005817399 0.008273152 0 0.004929843 0.009048759
V5 0.010330529 0.010735568 0 0.009048759 0.018683544 

Array2DRowRealMatrix{
{0.013445532,0.01039469,0.009881156,0.010499559},
{0.01039469,0.023006616,0.008196856,0.010732709},
{0.009881156,0.008196856,0.019023866,0.009210099},
{0.010499559,0.010732709,0.009210099,0.019107243}}

> cov(data3)
            V1          V2          V3          V4
V1 0.013445047 0.010478862 0.009955904 0.010529542
V2 0.010478862 0.022910522 0.008610113 0.011046353
V3 0.009955904 0.008610113 0.019250975 0.009464442
V4 0.010529542 0.011046353 0.009464442 0.019260317


I've traced this back to the RectangularCholeskyDecomposition, which does not seem to handle the second matrix very well (decompositions in the same order as the matrices above):

CorrelatedRandomVectorGenerator.getRootMatrix() = 
Array2DRowRealMatrix{{0.0,0.0,0.0,0.0,0.0},{0.0759577418122063,0.0876125188474239,0.0,0.0,0.0},{0.07764443622513505,0.05132821221460752,0.11976381821791235,0.0,0.0},{0.06662930527909404,0.05501661744114585,0.0016662506519307997,0.10749324207653632,0.0},{0.13822895138139477,0.0,0.0,0.0,0.0}}
CorrelatedRandomVectorGenerator.getRank() = 5

CorrelatedRandomVectorGenerator.getRootMatrix() = 
Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.0},{0.07764443622513505,0.13029949164628746,0.0},{0.0,0.0,0.0},{0.06662930527909404,0.023203936694855674,0.0},{0.13822895138139477,0.0,0.0}}
CorrelatedRandomVectorGenerator.getRank() = 3

CorrelatedRandomVectorGenerator.getRootMatrix() = 
Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.033913748226348225,0.07303890149947785},{0.07764443622513505,0.13029949164628746,0.0,0.0},{0.06662930527909404,0.023203936694855674,0.11851573313229945,0.0},{0.13822895138139477,0.0,0.0,0.0}}
CorrelatedRandomVectorGenerator.getRank() = 4

Clearly, the rank of each of these matrices should be 4. The first matrix does not lead to incorrect results, but the second one does. Unfortunately, I don't know enough about the Cholesky decomposition to find the flaw in the implementation, and I could not find documentation for the ""rectangular"" variant (also not at the links provided in the javadoc).",JDK 1.6 / Eclipse Indigo on Ubuntu 10.04,,,,,,,,,,,,,"11/May/12 13:54;gertvv;MultivariateGaussianGeneratorTest.java;https://issues.apache.org/jira/secure/attachment/12526521/MultivariateGaussianGeneratorTest.java",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-05-10 22:02:51.197,,,false,,,,,,,,,,,,,,,,,238705,,,Thu Sep 13 15:19:07 UTC 2012,,,,,,,"0|i0rt7j:",160376,,,,,,,,,,,,,,,,"10/May/12 22:02;tn;Hi Gert,

thanks for the report. Could you please attach a test case for the described problem. This would really help investigating the problem.

Thanks,

Thomas","11/May/12 13:54;gertvv;Failing test case to reproduce bug (fails for covMatrix3).","11/May/12 21:11;tn;Thanks for the test.

My first investigation is as follows:

in the RectangularCholeskyDecomposition class, the following code does not actually produce the maximal diagonal element:

{noformat}
   // find maximal diagonal element
   swap[r] = r;
   for (int i = r + 1; i < order; ++i) {
       int ii = index[i];
       int isi = index[swap[i]];
       if (c[ii][ii] > c[isi][isi]) {
         swap[r] = i;
       }
   }
{noformat}

thus the rank of the matrix is computed wrongly as the ordering of the columns is wrong and as a consequence the loop finishes too early. This can be fixed quite easily by changing index[swap[i]] to index[swap[r]].

The increment of r seems also to be wrong in the case the diagonal element is smaller than the user-defined limit.

When making the changes, the rank is correct, but the resulting root matrix is not very good (root * root.transpose() != covariance), thus the transformation of the matrix has to be further reviewed (I did not figure it out yet).

Unfortunately there is no unit test for the RectangularCholeskyDecomposition yet, so this should be added in the process of fixing this issue. ","13/Sep/12 15:19;luc;Fixed in subversion repository as of r1384363.

The problem was due to some missing permutations in the root matrix. It seems the old fix for MATH-226 was not good.

Thanks for the report, and thanks to Thomas for the identification of the wrong maximum element detection.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Rotation constructor does wrong calculation,MATH-846,12603667,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,fhess,fhess,15/Aug/12 23:07,04/Mar/13 18:53,20/Mar/20 20:33,16/Aug/12 16:11,3.0,,,,,,,3.1,,,0,,,,,,"The following code produces the wrong result.  The resulting Rotation is does not even hold a normalized quaternion:

final Vector3D u1 = new Vector3D(1.0, 0.0, 0.0);
final Vector3D u2 = new Vector3D(1.0, -1.0, 0.0);
final Vector3D v1 = new Vector3D(0.9999999, 0., 0.0);
final Vector3D v2 = new Vector3D(0., 1., 0.0);
final Rotation rot = new Rotation(u1, u2, v1, v2);
System.err.println(""rot quaternion: "" + rot.getQ0() + "" "" + rot.getQ1() + "" "" + rot.getQ2() + "" "" + rot.getQ3());


For me it outputs:

rot quaternion: 0.0 0.0 0.0 -7.450580596923828E-9

The correct output should have been:

rot quaternion: 0.0 1.0 0.0 0.0

The constructor seems to be hitting some kind of numerical instability.",Windows java 1.7r5,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-08-16 11:58:19.667,,,false,,,,,,,,,,,,,,,,,292277,,,Thu Aug 16 16:11:39 UTC 2012,,,,,,,"0|i0rszr:",160341,,,,,,,,,,,,,,,,"16/Aug/12 11:58;luc;I am not able to reproduce this behavior with the current development version.

I think the problem has already been solved on June 6th as part of MATH-801.

Could you check with the development version from our subversion repository?","16/Aug/12 13:43;fhess;I think you are right.  I downloaded svn HEAD (r1373782) and the problem seems to be gone.  Thanks for your help.","16/Aug/12 16:11;luc;This issue was fixed as part of fixing MATH-801",,,,,,,,,,,,,,,,,,,,,,,,,,,,
RealVector.subtract(RealVector) returns wrong answer.,MATH-802,12560043,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,celestin,celestin,celestin,09/Jun/12 13:05,04/Mar/13 18:53,20/Mar/20 20:33,09/Jun/12 13:14,3.0,,,,,,,3.1,,,0,,,,,,"The following piece of code
{code:java}
import org.apache.commons.math3.linear.ArrayRealVector;
import org.apache.commons.math3.linear.OpenMapRealVector;
import org.apache.commons.math3.linear.RealVectorFormat;

public class DemoMath {

    public static void main(String[] args) {
        final double[] data1 = {
            0d, 1d, 0d, 0d, 2d
        };
        final double[] data2 = {
            3d, 0d, 4d, 0d, 5d
        };
        final RealVectorFormat format = new RealVectorFormat();
        System.out.println(format.format(new ArrayRealVector(data1)
            .subtract(new ArrayRealVector(data2))));
        System.out.println(format.format(new OpenMapRealVector(data1)
            .subtract(new ArrayRealVector(data2))));
    }
}
{code}

prints
{noformat}
{-3; 1; -4; 0; -3}
{3; 1; 4; 0; -3}
{noformat}

the second line being wrong. In fact, when subtracting mixed types, {{OpenMapRealVector}} delegates to the default implementation in {{RealVector}} which is buggy.",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,292300,,,Sat Jun 09 13:14:22 UTC 2012,,,,,,,"0|i0rt4v:",160364,,,,,,,,,,,,,,,,"09/Jun/12 13:14;celestin;This bug was revealed while working on MATH-795. Previous unit tests didn't show that bug, because in {{RealVectorTest}}, {{TestVectorImpl}} used to override the default implementation of {{RealVector.subtract(RealVector)}}.

This is now corrected in {{r1348396}} (tests to be updated following work on MATH-795).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need range checks for elitismRate in ElitisticListPopulation constructors.,MATH-776,12549191,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,reidhoch,reidhoch,02/Apr/12 17:15,04/Mar/13 18:53,20/Mar/20 20:33,02/Apr/12 18:47,3.0,,,,,,,3.1,,,0,genetics,,,,,"There is a range check for setting the elitismRate via ElitisticListPopulation's setElitismRate method, but not via the constructors.",,3600,3600,,0%,3600,3600,,,,,,,"02/Apr/12 17:16;reidhoch;MATH-776.txt;https://issues.apache.org/jira/secure/attachment/12520993/MATH-776.txt",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-04-02 18:47:29.342,,,false,,,,,,,,,,,,,,,,,234182,,,Mon Apr 02 18:47:29 UTC 2012,,,,,,,"0|i0rt9r:",160386,,,,,,,,,,,,,,,,"02/Apr/12 17:16;reidhoch;Proposed patch for MATH-776.","02/Apr/12 18:47;tn;Patch applied with minor modification in r1308454.

Thanks for the contribution!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CMAESOptimizer does not enforce bounds,MATH-864,12608315,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,fhess,fhess,19/Sep/12 19:32,04/Mar/13 18:53,20/Mar/20 20:33,22/Sep/12 10:05,3.0,,,,,,,3.1,,,0,,,,,,"The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.",,,,,,,,,,,,,,"19/Sep/12 22:07;fhess;Math864Test.java;https://issues.apache.org/jira/secure/attachment/12545812/Math864Test.java",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-09-19 21:14:22.681,,,false,,,,,,,,,,,,,,,,,292269,,,Fri Sep 21 22:41:05 UTC 2012,,,,,,,"0|i0rsxz:",160333,,,,,,,,,,,,,,,,"19/Sep/12 20:08;fhess;Some more info: I see now that FitnessFunction.value will ""repair"" the parameters to be in-bounds before passing them to computeObjectiveValue.  However, the generationLoop does not ""repair"" the parameters when storing the optimum parameter values, it saves the unrepaired values (which may be outside of bounds).","19/Sep/12 21:14;erans;bq. I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.

It would be very helpful if you could set up a unit test (i.e. a minimal example) showing the failure.
Thanks.

bq. the generationLoop does not ""repair"" the parameters when storing the optimum parameter values

That looks suspicious indeed.
","19/Sep/12 22:07;fhess;Test program that shows bug.","21/Sep/12 14:21;erans;Thanks for the report, the show-case and the hint toward solving this problem.
Fix committed in revision 1388517. Please test.
","21/Sep/12 22:41;fhess;Thanks, seems to be fixed testing with revision 1388555.",,,,,,,,,,,,,,,,,,,,,,,,,,
Using Rotation to convert Euler angles to Quaternions produces wrong results,MATH-847,12603769,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Not A Problem,,dunmatt,dunmatt,16/Aug/12 18:57,04/Mar/13 18:53,20/Mar/20 20:33,17/Aug/12 06:17,3.0,,,,,,,3.1,,,0,,,,,,"import org.apache.commons.math3.geometry.euclidean.threed.Rotation;
import org.apache.commons.math3.geometry.euclidean.threed.RotationOrder;

public class temp {
  public static void main(String args[]) {
    Rotation r = new Rotation(RotationOrder.XYZ, -Math.PI / 2d, 0, 0);
    System.out.println(""("" + r.getQ0() + "" "" + r.getQ1() + "" "" + r.getQ2() + "" "" + r.getQ3() + "")"");
  }
}

Prints (0.707 0.707 0.0 0.0) (in-sig-figs elided), but when I type the same thing into Wolfram Alpha I get (.707 -.707 0 0) (note the negative)  see: http://www.wolframalpha.com/input/?i=euler+angles&a=*C.euler+angles-_*Formula.dflt-&a=*FP.EulerRotation.EAS-_e123&f3=-pi%2F2+rad&f=EulerRotation.th1_-pi%2F2+rad&f4=0&f=EulerRotation.th2_0&f5=0&f=EulerRotation.th3_0

One of the guys in the lab suggested that if Rotation is assuming the Euler angle is in a left-handed coordinate space this is an expected result, but if that's the case the question is, why is the less popular coordinate system the only option?",1.6,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-08-16 19:33:35.816,,,false,,,,,,,,,,,,,,,,,292276,,,Fri Aug 17 06:17:54 UTC 2012,,,,,,,"0|i0rszj:",160340,,,,,,,,,,,,,,,,"16/Aug/12 19:33;luc;This is a question rather than a bug report, so it should be on the users mailing list, not on Jira.

Nevertheless, the answer to this question is not that we use a left handed coordinate system, it is that the angles are counted from a vectorial operator point of view, not from a frame conversion point of view. Please read the javadoc for all methods and for the class for an explanation.","16/Aug/12 21:48;erans;Can we close this report?","16/Aug/12 21:58;dunmatt;Sure.  Can we ammend it to ask that the part of the javadoc that describes this be moved from the one constructor to the whole class since it applies no matter how the rotation is constructed?","17/Aug/12 06:17;luc;Sure, we will try to improve the javadoc.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Eliminate meaningless properties in multivariate distribution classes,MATH-881,12612484,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,18/Oct/12 19:57,04/Mar/13 18:53,20/Mar/20 20:33,19/Oct/12 11:09,3.0,,,,,,,3.1,,,0,,,,,,"The MultivariateRealDistribution interface includes the following properties which make no sense for multivariate distributions:
getSupportLowerBound, getSupporUpperBound, isSupportLowerBoundInclusive, isSupportUpperBoundInclusive
In addition, the following property makes sense, but is unlikely to be useful:
isSuportConnected

All of these properties should be deprecated in 3.1 and dropped in 4.0",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-10-18 21:05:01.197,,,false,,,,,,,,,,,,,,,,,249662,,,Fri Oct 19 09:33:12 UTC 2012,,,,,,,"0|i0ag8f:",58903,,,,,,,,,,,,,,,,"18/Oct/12 21:05;erans;bq. All of these properties should be deprecated in 3.1 and dropped in 4.0

This interface and implementing classes are post 3.0.
No need to wait for 4.0 to clean up. :)
","18/Oct/12 21:27;erans;All above methods removed in revision 1399864.
","18/Oct/12 21:33;erans;Shall I also remove this method:
{code}
/**
 * For a random variable {@code X} whose values are distributed according to
 * this distribution, this method returns {@code P(X = x)}. In other words,
 * this method represents the probability mass function (PMF) for the
 * distribution.
 *
 * @param x Point at which the PMF is evaluated.
 * @return the value of the probability mass function at point {@code x}.
 */
double probability(double[] x);
{code}
?

We already discussed that all the (currently implemented) {{RealDistribution}} classes return zero. I guess that is is even less likely that an exotic {{MultivariateRealDistribution}} will soon be implemented...
","18/Oct/12 23:17;psteitz;Sweet!  I did not realize this was post-3.0 stuff.  Yes, I would also nuke the pmf method.","19/Oct/12 09:33;erans;bq. [...] nuke the pmf method.

Removed in revision 1400010.",,,,,,,,,,,,,,,,,,,,,,,,,,
"In the ListPopulation constructor, the check for a negative populationLimit should occur first.",MATH-775,12549190,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,reidhoch,reidhoch,02/Apr/12 17:09,04/Mar/13 18:53,20/Mar/20 20:33,12/Apr/12 18:19,3.0,,,,,,,3.1,,,0,genetics,,,,,"In the ListPopulation constructor, the check to see whether the populationLimit is positive should occur before the check to see if the number of chromosomes is greater than the populationLimit.",,3600,3600,,0%,3600,3600,,,,,,,"02/Apr/12 17:11;reidhoch;MATH-775.txt;https://issues.apache.org/jira/secure/attachment/12520992/MATH-775.txt",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-04-04 18:31:36.962,,,false,,,,,,,,,,,,,,,,,234181,,,Thu Apr 12 18:19:13 UTC 2012,,,,,,,"0|i0rt9z:",160387,,,,,,,,,,,,,,,,"02/Apr/12 17:11;reidhoch;Proposed patch for MATH-775.","04/Apr/12 18:31;tn;Good catch.
After looking at the class, I see even more problems:

 * setPopulationSize allows to change the population size, but does not check the chromosome list for compatible size
 * addChromosome does not check the population size at all
 * the chromosome list given as parameter to the ctor and setChromosomes is assigned directly, allowing the list to be changed from the outside later on 

From my understanding the following should happen:

 * setPopulationSize should shrink the existing chromosome list if it is larger than the new size (optionally we could also remove this method)
 * addChromosome should throw an exception if adding a chromosome would exceed the population size
 * when setting an externally provided chromosome list, the entries should be copied to a new internal list ","05/Apr/12 22:15;tn;The patch has been committed in r1310103, together with other changes that have been outlined before.

Additionally, I have added two new methods:

 * public void addChromosomes(Collection<Chromosome> c)
 * protected List<Chromosome> getChromosomeList()

and made setChromosomes deprecated.

Rationale:

The internal state of ListPopulation shall be protected, and shall not be changeable from the outside as it was possible before. When adding chromosomes, the entries are added to the internal list, instead of setting the internal list reference to the provided list.

Derived classes can get access to the internal list via getChromosomeList (we could also make the internal list protected, is there a policy in CM?).

The setters throw appropriate exceptions to keep the internal state consistent, and addChromosome also throws an exception if the population would exceed the population limit.","09/Apr/12 16:55;tn;I changed the internal list in ListPopulation to a protected member and removed the getChromosomeList again. This way it is cleaner imho.","10/Apr/12 01:33;sebb;Using a getter is better.
Exposing a field means that it is harder to make changes later, e.g. should there be a need to synch. the field or make it read-only.

As a general rule, mutable fields should never be exposed.","10/Apr/12 07:38;tn;hmm, I see your point. My concern was that there is already a public getChromosomes method that I have now changed to return an unmodifiable version of the internal list. As derived classes need access to the internal list (to alter it), I added a protected getChromosomeList method, but somehow this did not feel right.

Maybe a different name for the method would suffice?","12/Apr/12 18:19;tn;Fixed in r1325422.

I kept the name getChromosomeList for the getter.",,,,,,,,,,,,,,,,,,,,,,,,
SimplexSolver returns values out of constraints bounds,MATH-808,12596501,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,tn,alexeyslepov,alexeyslepov,30/Jun/12 11:41,04/Mar/13 18:53,20/Mar/20 20:33,18/Jul/12 21:05,3.0,,,,,,,3.1,,,0,constraints,math,SimplexSolver,unexpected,,SimplexSolver gives back result that doesn't match constraints,"win7 64, eclipse 3.7, Apache Math 3.0",,,,,,,,,,,,,"30/Jun/12 11:44;alexeyslepov;InputOrientedDEAAlgorithm.java;https://issues.apache.org/jira/secure/attachment/12534102/InputOrientedDEAAlgorithm.java","30/Jun/12 11:45;alexeyslepov;InputOrientedDEAAlgorithmTest.java;https://issues.apache.org/jira/secure/attachment/12534103/InputOrientedDEAAlgorithmTest.java","30/Jun/12 11:42;alexeyslepov;InputOrientedDEAAlgorithm_12-6-30_15-32-52.test.txt;https://issues.apache.org/jira/secure/attachment/12534101/InputOrientedDEAAlgorithm_12-6-30_15-32-52.test.txt",,,,3.0,,,,,,,,,,,,,,,,,,,,2012-06-30 14:02:40.626,,,false,,,,,,,,,,,,,,,,,292297,,,Thu Jul 19 15:27:42 UTC 2012,,,,,,,"0|i0rt47:",160361,,,,,,,,,,,,,,,,"30/Jun/12 11:42;alexeyslepov;Test report file. Keep attention to 'EXCEPTION: Wrong equation' lines","30/Jun/12 11:44;alexeyslepov;Source code file. Take a look into 
public PointValuePair findEntityOptimum(EntityArgument entity, GoalType goalType) throws Exception
method","30/Jun/12 11:45;alexeyslepov;Unit test to run to repeat the unexpected SimplexSolver behavior","30/Jun/12 14:02;erans;Could you please simplify the test case? It should contain the bare minimum of code necessary to show the buggy behaviour. The unit test should also contain a method from the {{org.junit.Assert}} class in order to indicate which result is expected.
Thanks.
","01/Jul/12 13:41;tn;Hi,

I could not run your test code as some classes are missing.
Maybe the described behavior is related to an accuracy flaw that has been fixed in MATH-781 (see https://issues.apache.org/jira/browse/MATH-781). Could you please check if the same problem occurs with the latest trunk.

For the test case that Gilles mentioned you could take a look at the attached ones from MATH-781.

Thanks,

Thomas","16/Jul/12 23:26;erans;Has this been confirmed to still occur with the development version?","17/Jul/12 08:14;tn;No, it has not been confirmed, but I will try to extract relevant information from the attached test case and run it on trunk.","17/Jul/12 18:42;tn;I further looked into the test case and finally was able to run it (create Algorithm interface myself and use MathArrays.distance to compute the euclidean distance).

Imho, the bug report is wrong, as the failing tests are due to an invalid result validation:

{noformat}
1. First equation set -q(r) + QL >= 0 <---> QL >= q(r) verifications: 
	1: 0.0000*t  + 1.0940*0.0592  + 1.8101*0.0000  + 2.7621*0.0000  + 2.1166*0.0674  + 3.5084*0.3067  + 2.1122*0.0919  + 1.2818*0.2595  >= 1.8101   <--------> 1.81011188 >= 1.81011200
EXCEPTION: Wrong equation
java.lang.Exception: Wrong equation
	at ru.hse.cst.algorithm.InputOrientedDEAAlgorithm.findEntityOptimum(InputOrientedDEAAlgorithm.java:378)
{noformat}

Actually this is based on the following check:

{noformat}
if(sum < 0) {
   throw new Exception(""Wrong equation"");
}
{noformat}

Now, the SimplexSolver is not a perfect algorithm and has to cope with the usual limitations of floating-point arithmetic. So the solution will only be accurate within a given epsilon. The default one is 1e-6, so the result has to be evaluated taking this epsilon into account:

{noformat}
if(Precision.compareTo(sum, 0, 1e-6) < 0){
   throw new Exception(""Wrong equation"");
}
{noformat}

With such a check the test cases run through successfully. I still encountered some ""No feasible solution found"" and ""Max count exceeded"" situations, which I have not further debugged and are most likely happening because of test setup (the coefficients are created randomly).","18/Jul/12 21:05;tn;Following my observations, I close this issue as invalid. If you encounter other problems with the simplex solver, feel free to create a new issue with test cases attached.

Thanks, Thomas","19/Jul/12 15:27;alexeyslepov;Thanks Thomas for you involvement in this particular case. Now I see that the problem was out of unproper using of Math3. But! Always there is but :) the problem with this LP problem moved to MATH-828 issue. I'll be appreciated to you if you look in there again (now the code is all-sufficient and easier to read)",,,,,,,,,,,,,,,,,,,,,
FDistribution NoBracketingException in BrentSolver,MATH-909,12617545,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Cannot Reproduce,,meyerjp,meyerjp,25/Nov/12 23:38,26/Nov/12 19:31,20/Mar/20 20:33,26/Nov/12 19:31,3.0,,,,,,,,,,0,,,,,,"I get an exception when running the code below. the exception is 

{code}
function values at endpoints do not have different signs, endpoints: [0, 1.002], values: [-0.025, -∞]
{code}

The problematic code:

{code}
double df1 = 10675;
double df2 = 501725;
FDistribution fDist = new FDistribution(df1, df2);
System.out.println(fDist.inverseCumulativeProbability(0.025));//NoBracketingException
{code}

However, R returns the value 0.9733505. The R code is:
{code}
qf(p=.025, df1=10675, df2=501725)
{code}

I don't know enough about the FDistribution class to know the solution to the exception, but I thought I would report it.
",,86400,86400,,0%,86400,86400,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-11-26 09:10:49.939,,,false,,,,,,,,,,,,,,,,,292045,,,Mon Nov 26 19:31:16 UTC 2012,,,,,,,"0|i0rorz:",159658,,,,,,,,,,,,,,,,"26/Nov/12 09:10;tn;I tested it with the latest trunk version, and the result is: 0.9730779455126357

The FDistribution relies on the Beta function, which got several improvements in terms of accuracy since the 3.0 release.
Could you please test yourself with a more recent version of CM?

Thanks,

Thomas","26/Nov/12 09:57;celestin;bq. The FDistribution relies on the Beta function, which got several improvements in terms of accuracy since the 3.0 release.

Actually, in the lastest trunk {{r1413531}}, {{Beta}} hasn't been changed yet (ongoing work in MATH-738). Accuracy of {{Gamma.logGamma}} has improved a lot (see MATH-849) which might explain why this bug no longer shows up.

Even if this should not be considered as a bug, I propose we include this simple test in our unit tests, as {{Beta}} will undergo some changes in the coming days, hopefully for the best, but it never hurts to check non-regression, does it?","26/Nov/12 10:02;tn;Ah ok my fault, but adding more regression tests is definitely useful.
Will add the test case later on.","26/Nov/12 13:17;meyerjp;I tested it with the most recent version and I also got 0.9730779455126357. The exception no longer occurs, but the result still seems to be too different from the value reported by R.","26/Nov/12 14:17;celestin;Patrick, the arguments of the incomplete beta function are large, so a loss of accuracy is to be expected. We are working on it (see MATH-738), but there is still a lot to do.

Meanwhile, I've used MAXIMA to compute in multiple precision the result you are asking for. Here is the script I wrote

{code}
kill(all);
load(newton1);
fpprec : 128;
d1 : 10675;
d2 : 501725;

p : 0.025b0;

F(x) := block(
  y : d1 * x / (d1 * x + d2),
  beta_incomplete(0.5b0 * d1, 0.5b0 * d2, y) / beta(0.5b0 * d1, 0.5b0 * d2)
  );

x0 : 0.9733505b0;
x1 : newton(F(x) - p, x, x0, 10**(-fpprec+1));
print(x1);
{code}

Basically, {{F(X)}} is the cumulative distribution function, and {{p}} is the target. I then look for an approximate solution of {{F(X) == p}}, starting from {{x0 = 0.9733505b0}} which is the value returned by R.

Here is the result I get
{code}
9.730779455235159798387713198169726833852358770410804399004797817813867760559669310857118225960097152689836562586854107391295331b-1
{code}

It seems to me that the value returned by CM is more accurate than the one returned by R. Could you carry out an independent check on that?

NOTA: for some reason, I had to compute the regularized incomplete beta function as the ratio of the incomplete beta function and the beta function, as the function {{beta_incomplete_regularized (a, b, z)}} led to errors.","26/Nov/12 14:56;meyerjp;Ha, you're right! R is less accurate. I checked the value with Stata (code listed below) and the result was 0.97307795. I'm satisfied. CM returns a more accurate value.

{code}
display invF(10675, 501725, 0.025)
{code}","26/Nov/12 15:09;celestin;Excellent! I'm glad CM turns out to be more accurate. Be cautious, though, since we are still working on the incomplete beta function (MATH-738).

Just out of curiosity: I Haven't R installed on my computer. Could you check the doc for the incomplete beta function, and let us know on which implementation it is based?
If I remember correctly, it's slatec. I've noticed that slatec is in fact *not very accurate* for the gamma and beta functions, so I ruled it out for MATH-738.

Patrick: are you satisfied? Can we consider this issue as solved? If that's OK with you, I suggest we keep it open until the unit test has been implemented. Then, we will close it. What do you think?","26/Nov/12 15:52;erans;bq. I suggest we keep it open until the unit test has been implemented.

As this test would not demonstrate a current buggy behaviour of CM, it should not be linked to this issue (i.e. not need to create a ""testMath909()"" method).
I think that the issue can be resolved now as ""Cannot reproduce"".

Of course, unit tests are always welcome. But in this case, your approach of fairly exhaustively checking the result of the underlying functions by comparison with ""maxima"" makes it superfluous to separately test the ""client"" (i.e. ""FDistribution"").
","26/Nov/12 17:21;meyerjp;According to the R documentation, the gamma and beta functions are C translations of the SLATEC Fortran subroutines, as you suspected. The incomplete gamma appears to have a different origin. According to the R documentation, the pbeta function is related to the incomplete beta function of Abramowitz and Stegun. They cite two different sources for the function depending on whether it is a central or non-central pbeta.

Central pbeta:

Didonato, A. and Morris, A., Jr, (1992) Algorithm 708: Significant digit computation of the incomplete beta function ratios, ACM Transactions on Mathematical Software, 18, 360–373. (See also
Brown, B. and Lawrence Levy, L. (1994) Certification of algorithm 708: Significant digit computation of the incomplete beta, ACM Transactions on Mathematical Software, 20, 393–397.)

Non-central pbeta:

Lenth, R. V. (1987) Algorithm AS226: Computing noncentral beta probabilities. Appl. Statist, 36, 241–244, incorporating
Frick, H. (1990)'s AS R84, Appl. Statist, 39, 311–2, and
Lam, M.L. (1995)'s AS R95, Appl. Statist, 44, 551–2.

As far as test cases go, I think we should include a test case, given the proposed work on the underlying incomplete beta function. The test case does not have to be specific to this issue, but it would be safe to include a test.




","26/Nov/12 18:12;celestin;bq. Of course, unit tests are always welcome. But in this case, your approach of fairly exhaustively checking the result of the underlying functions by comparison with ""maxima"" makes it superfluous to separately test the ""client"" (i.e. ""FDistribution"").

I wouldn't say superfluous. Maybe (maybe) redundant. But remember that the incomplete beta function has three arguments. Exhaustive checking of this function is going to be difficult...

On the whole, I agree with you. This test would seem a bit weird. I'll make sure that comparison with Maxima are ""exhaustive enough"" to include a test equivalent to the values proposed by Patrick in this issue.

I agree this issue should be tagged as ""cannot reproduce"".","26/Nov/12 19:27;celestin;bq. According to the R documentation, the gamma and beta functions are C translations of the SLATEC Fortran subroutines, as you suspected. The incomplete gamma appears to have a different origin. According to the R documentation, the pbeta function is related to the incomplete beta function of Abramowitz and Stegun. They cite two different sources for the function depending on whether it is a central or non-central pbeta.

Thank you Patrick for checking the references. Didonato and Morris (1992) is actually the reference I used for our new implementation of {{Gamma.logGamma}} and {{Gamma.gamma}}, as well as {{Beta.logBeta}} (not yet committed).

I'm worried about the incomplete beta function, as R uses this paper, and the accuracy seems to be not so good. I will look into it.","26/Nov/12 19:31;celestin;New implementation of {{Gamma.logGamma}} seems to solve this issue.
A test equivalent to the extreme test proposed by Patrick will be included in the unit tests of the incomplete beta function.",,,,,,,,,,,,,,,,,,,
BlockRealMatrix java.beans.IntrospectionException,MATH-858,12606711,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,mdongi,mdongi,08/Sep/12 09:56,10/Sep/12 12:26,20/Mar/20 20:33,10/Sep/12 12:26,3.0,,,,,,,,,,0,BlockRealMatrix,,,,,"When I try to read properties (like getColumnDimension) of an instance of class BlockRealMatrix (org.apache.commons.math3.linear.BlockRealMatrix), system raise this exception:

Class - java.beans.IntrospectionException
Message - type mismatch between indexed read and indexed write methods: columnMatrix

=================
Code

import org.apache.commons.math3.stat.correlation.PearsonsCorrelation
import strategoianalysis.util.math.matrix.MatrixStatutils
import org.apache.commons.math3.linear.*

class JohnsonRWAController {

    private double[] computeRWA(double[][] data) {

BlockRealMatrix correlationMatrix = new PearsonsCorrelation().computeCorrelationMatrix(data)

        int rowDim = correlationMatrix.getRowDimension() 
        
      //EXCEPTION HERE


.... etc.
",MacOSX 10.8.1 (Macbook Air 4Gb RAM)- IntelliJ IDEA 11 - Grails 2.10 ,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-09-08 12:06:42.719,,,false,,,,,,,,,,,,,,,,,292272,,,Mon Sep 10 12:26:20 UTC 2012,,,,,,,"0|i0rsyn:",160336,,,,,,,,,,,,,,,,"08/Sep/12 12:06;erans;Which version of Commons Math are you using?

In the current development version, this code:
{noformat}
BlockRealMatrix correlationMatrix = new PearsonsCorrelation().computeCorrelationMatrix(data);
{noformat}
does not even compile.

When you report a bug, please provide a fully working minimal code example.

Could you also provide the stack trace of the exception?

Moreover I cannot find any Commons Math error message that contains the string ""mismatch between indexed read""...
","08/Sep/12 16:45;mdongi;Yes sorry, the right line of code is this:
{noformat}RealMatrix correlationMatrix = (new PearsonsCorrelation()).computeCorrelationMatrix(data);{noformat}

When I try to access to getRowDimension() method, the exception occurs.

I'm working on Grails 2.10 platform with apache commons math3 3.0 libraries.

After some other tests, it seems to be a spring/groovy trouble. Error messages comes from Groovy compiler, which raise an org.codehaus.groovy.runtime.InvokerInvocationException.

So it's not a commons math bug. Really thanks for quick response Gilles, and sorry for this _off topic_ issue.
I'm going to close this issue.","10/Sep/12 12:26;mdongi;The problem is not related to apache commons math library. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incomplete reinitialization with some events handling,MATH-695,12528715,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,pparraud,pparraud,25/Oct/11 16:52,24/Mar/12 16:16,20/Mar/20 20:33,26/Oct/11 09:09,3.0,,,,,,,3.0,,,0,,,,,,"I get a bug with event handling: I track 2 events that occur in the same step, when the first one is accepted, it resets the state but the reinitialization is not complete and the second one becomes unable to find its way.
I can't give my context, which is rather large, but I tried a patch that works for me, unfortunately it breaks the unit tests.",,,,,,,,,,,,,,"25/Oct/11 16:57;pparraud;events.patch;https://issues.apache.org/jira/secure/attachment/12500719/events.patch","26/Oct/11 08:24;luc;test-case-MATH-695.patch;https://issues.apache.org/jira/secure/attachment/12500839/test-case-MATH-695.patch",,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-10-25 19:26:20.212,,,false,,,,,,,,,,,,,,,,,214575,,,Wed Oct 26 09:09:54 UTC 2011,,,,,,,"0|i0aoun:",60299,,,,,,,,,,,,,,,,"25/Oct/11 16:57;pparraud;this patch resolves my problem but breaks some unit tests.","25/Oct/11 19:26;luc;As I work with Pascal (we share the same office at work), I have seen how the bug occurs and am trying to set up a simplified test case that reproduces it. It seems that there is a combination of conditions that is not handled properly. We have seen the bug occurring when both following conditions are true:

* there are several events occurring in the same integration step
* when one of the earliest events occurrence is triggered, it returns RESET_DERIVATIVES

In this case, the acceptStep method in AbstractIntegrator returns early from inside the while loop and the remaining events that where expected to occur later on are left in an inconsistent state with respect to the integrator. The t0 and g0 fields in the corresponding EventState instance still contain values from the beginning of the step, they do not reflect the fact the event has been triggered. This implies that when next step is started with the updated derivatives, evaluateStep tries to catch up from t0 to current t and calls the g function at times that do not belong to the current step.

Up to now, I have not been able to set up a simplified test case that exhibits this, but I'm working on it.","26/Oct/11 08:24;luc;The attached test case reproduces the error.","26/Oct/11 09:09;luc;Fixed in subversion repository as of r1189086.

The fix is different from the proposed patch, it directly updates the events when the step truncation occurs, thus preventing even transient inconsistency.

Thanks for the report and for the patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,
DormandPrince853 integrator leads to revisiting of state events,MATH-705,12530878,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,dhendriks,dhendriks,09/Nov/11 13:29,24/Mar/12 16:16,20/Mar/20 20:33,27/Nov/11 14:34,3.0,,,,,,,3.0,,,0,,,,,,"See the attached ReappearingEventTest.java. It has two unit tests, which use either the DormandPrince853 or the GraggBulirschStoer integrator, on the same ODE problem. It is a problem starting at time 6.0, with 7 variables, and 1 state event. The state event was previously detected at time 6.0, which is why I start there now. I provide and end time of 10.0. Since I start at the state event, I expect to integrate all the way to the end (10.0). For the GraggBulirschStoer this is what happens (see attached ReappearingEventTest.out). For the DormandPrince853Integerator, it detects a state event and stops integration at 6.000000000000002.

I think the problem becomes clear by looking at the output in ReappearingEventTest.out, in particular these lines:

{noformat}
computeDerivatives: t=6.0                  y=[2.0                 , 2.0                 , 2.0                 , 4.0                 , 2.0                 , 7.0                 , 15.0                ]
(...)
g                 : t=6.0                  y=[1.9999999999999996  , 1.9999999999999996  , 1.9999999999999996  , 4.0                 , 1.9999999999999996  , 7.0                 , 14.999999999999998  ]
(...)
final result      : t=6.000000000000002    y=[2.0000000000000013  , 2.0000000000000013  , 2.0000000000000013  , 4.000000000000002   , 2.0000000000000013  , 7.000000000000002   , 15.0                ]
{noformat}

The initial value of the last variable in y, the one that the state event refers to, is 15.0. However, the first time it is given to the g function, the value is 14.999999999999998. This value is less than 15, and more importantly, it is a value from the past (as all functions are increasing), *before* the state event. This makes that the state event re-appears immediately, and integration stops at 6.000000000000002 because of the detected state event.

I find it puzzling that for the DormandPrince853Integerator the y array that is given to the first evaluation of the g function, has different values than the y array that is the input to the problem. For GraggBulirschStoer is can be seen that the y arrays have identical values.","Commons Math trunk, Java 6, Linux",,,,,,,,,,,,,"09/Nov/11 13:31;dhendriks;ReappearingEventTest.java;https://issues.apache.org/jira/secure/attachment/12503077/ReappearingEventTest.java","09/Nov/11 13:31;dhendriks;ReappearingEventTest.out;https://issues.apache.org/jira/secure/attachment/12503078/ReappearingEventTest.out",,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-11-09 19:57:08.802,,,false,,,,,,,,,,,,,,,,,216616,,,Sun Nov 27 14:34:04 UTC 2011,,,,,,,"0|i0aosn:",60290,,,,,,,,,,,,,,,,"09/Nov/11 13:31;dhendriks;The Java unit tests that show the problem, and the console output they give, as described in the issue description.","09/Nov/11 19:57;luc;The reason for this strange behavior is that g function evaluations are based on the integrator-specific interpolator.

Each integration method has its specific algorithm and preserve a rich internal data set. From this data set, it is possible to build an interpolator which is specific to the integrator and in fact share part of the data set (they reference the same arrays). So integrator and interpolator are tightly bound together.

For embedded Runge-Kutta methods like Dormand-Prince 8(5,3), this data set corresponds to one state vector value and several state vector derivatives sampled throughout the step. When the step is accepted after error estimation, the state value is set to the value at end of step and the interpolator is called. So the equations of the interpolator are written in such a way interpolation is backward: we start from the end state and roll back to beginning of step. This explains why when we roll all the way back to step start, we may find a state that is not exactly the one we started from, due to both the integration order and interpolation order.

For Gragg-Bulirsch-Stoer, the data set corresponds to one state vector value and derivatives at several orders, all taken at step middle point. When the step is accepted after error estimation, the interpolator is called before the state value is set to the value at end of step and. So the equations of the interpolator are written in such a way interpolation is forward: we start from the start state and go on towards end of step. This explains why when we go all the way to step end, we may find a state that is not exactly the one that will be used for next step, due to both the integration order and interpolation order.

So one integrator type is more consistent at step start and has more error at step end, while the other integrator has a reversed behavior.

In any case, the interpolation that is used (and in fact the integration data set it is based upon) are not error free. The error is related to step size.

We could perhaps rewrite some interpolators by preserving both start state s(t[k]) and end state s(t[k+1]) and switching between two hal model as follows:
  i(t) = s(t[k])   + forwardModel(t[k], t)    if t <= (t[k] + t[k+1])/2
and
  i(t) = s(t[k+1]) + backwardModel(t, t[k+1]) if t > (t[k] + t[k+1])/2

This would make interpolator more consistent with integrator at both step start and step end and perhaps reduce this problem. This would however not be perfect, as it will introduce a small error at junction point. I'm not sure if it would be easy or not, we would have to review all interpolators and all integrators for that. All models are polynomial ones.

Note that the problem should not appear for Adams methods (when they will be considered validated ...), because in this case, it is the interpolator that is built first and the integrator is in fact an application of the interpolator at step end! So interpolator and integrator are by definition always perfectly consistent with each other.

What do you think ?

Should we let this problem alone and consider we are in the grey zone of expected numerical inaccuracy due to integration/interpolation orders or should we attempt the two half-models trick ?
","10/Nov/11 14:31;dhendriks;bq. Should we let this problem alone and consider we are in the grey zone of expected numerical inaccuracy due to integration/interpolation orders or should we attempt the two half-models trick?

I consider it important that events are properly detected, and are detected exactly once (if they occur exactly once). Therefore, in general, I think the half-models trick would be a good idea, as it is more important (to me) to have higher accuracy at the end points than in the middle. For me, the DormandPrince853Integrator is now practically useless.
","20/Nov/11 21:44;luc;This issue should be fixed in subversion repository as of r1204270.

Could you check it works for you ? If so, I will use the same trick for other Runge-Kutta type step interpolators.","21/Nov/11 07:46;dhendriks;bq. This issue should be fixed in subversion repository as of r1204270. Could you check it works for you ? If so, I will use the same trick for other Runge-Kutta type step interpolators.

It seems to work perfectly. Thanks!","27/Nov/11 14:34;luc;Fixed in subversion repository as of r1206723.

Applied same method to all Runge-Kutta based integrators.

Thanks for the report.",,,,,,,,,,,,,,,,,,,,,,,,,
FastMath.pow much slower than Math.pow,MATH-579,12507487,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,aploese,aploese,17/May/11 13:14,24/Mar/12 16:16,20/Mar/20 20:33,18/May/11 09:50,3.0,,,,,,,3.0,,,0,,,,,,"calculating FastMath.pow(10, 0.1 / 20) is approximately 65 times slower as the Math.pow() function.
Ether this is a bug or a javadoc comment is missing.","java version ""1.6.0_22""
OpenJDK Runtime Environment (IcedTea6 1.10.1) (6b22-1.10.1-0ubuntu1)
OpenJDK 64-Bit Server VM (build 20.0-b11, mixed mode)


java version ""1.6.0_24""
Java(TM) SE Runtime Environment (build 1.6.0_24-b07)
Java HotSpot(TM) 64-Bit Server VM (build 19.1-b02, mixed mode)
",,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-05-17 18:33:05.774,,,false,,,,,,,,,,,,,,,,,150674,,,Thu May 19 06:05:19 UTC 2011,,,,,,,"0|i0rtnb:",160447,,,,,,,,,,,,,,,,"17/May/11 18:33;luc;This is strange. Could you run the FastMathTestPerformance junit test (in the utils package).
On my personal computer (which is also a 64 bits machine running Ubuntu), FastMath.pow is about 28% faster than either StrictMath.pow or Math.pow on Java 5 and 26% faster on Java 6.","17/May/11 18:36;aploese;This happens only on the first! run.
So take a separate project and run this test case:


    @Test
    public void testPowDuration() {
        int n = 10;
        long t1 = System.currentTimeMillis();
        for (int i = 0; i < n; i++) {
            double x1 = FastMath.pow(10, 0.1 / 20);
        }
        long t2 = System.currentTimeMillis();
        for (int i = 0; i < n; i++) {
            double x1 = Math.pow(10, 0.1 / 20);
        }
        long t3 = System.currentTimeMillis();
        double d1 = t2 - t1;
        double d2 = t3 - t2;
          if (d2 == 0) {
            d2 = 1;
        }
        if (d1 / d2 > 2.0)
            throw new RuntimeException(""pow(10, 0.1 / 20) ratio"" + (d1 / d2));
    }
Looks like some initialization takes quite long ...","17/May/11 19:34;luc;This is normal and is probably the same for all FastMath methods (I just checked for sin, abs and sqrt, knowing that FastMath.sqrt simply calls Math.sqrt). The initialization occurs at class loading as many tables are computed, so this overhead occurs only one per program run and does not change with the number of calls.

The number of calls is also important as the native code optimizing compilers kicks of only after the same part of code has been used many times. FastMath relies heavily on this and attempts to be fast for large scale computation. The side effect you see is that it is much slower for very short programs like the benchmark above.

Also note that 10 runs is far too low with regard to both the resolution of currentTimeMillis (one can use System.nanoTime() instead) and for results significance.

So I would like to close this as WONTFIX. Perhaps we should add in the javadoc that FastMath targets large scale computation.","17/May/11 20:01;sebb;AIUI FastMath is also targetted at faster calculation over the full range of the operands.

The performance test does not (currently) use suitable ranges for all the functions.","17/May/11 20:30;aploese;If you add a hint in the javadocs of class FastMath -> usage for large scale (and initilaization time up to 100 ms) this issue is FIXED

So having small numbers of computation, stick with Math.* , except you need asinh, ... ;-)  ","17/May/11 22:40;erans;I don't think that a remark like ""targets large scale computation"" adds any more information here (i.e. for ""FastMath"") than it would on any other class (slow initialization and JIT compiler behaviour is the same).
Also, I guess that if one fires up a Java application that only does a few math functions calls, there wouldn't be any noticeable time differences; they will be dwarfed by the JVM startup time.

OK for ""won't fix"".
","18/May/11 06:04;aploese;How it I stumbled over it:

I did debug my app, and came across the call to FastMath.pow() (first call) it took some time (100 ms) to complete the call.
Then I searched the javadoc for pow - nothing the same for class FastMath - nothing. So I decided must be a bug.

As you wrote FastMath is no general replacement of Math as the name Fast* would suggest, but it is intended for large scale operation.
If you want prevent confusion by end users (developers), a hint would be fine. ","18/May/11 09:50;luc;So I have added a thorough explanation in the class javadoc without any code change as of r1124151.

Thanks for reporting the issue and discussing about it.
","18/May/11 09:53;erans;In the end, it is the _absolute_ running time that counts. If you call ""pow"" _once_ in an application, why would it matter that it takes 100 ms or 1 ms?

Could it be that the ""Math"" class is already loaded (as part of the JVM initialization), so that ""FastMath"" is at a disadvantage in your benchmark (because it still needs to be loaded at the first call to ""pow"")?

I maintain that such a disclaimer (""large scale usage"") provides more confusion that it clears: What is large-scale? People who are worried will profile their (complete) application and can decide which implementation (of any interface) to use, based on realistic timings, not on a micro-benchmark (which can provide contradictory results).

What we could do is add a link to the performance test class.
","18/May/11 15:05;psteitz;I think Luc did a great job providing the right information to users.  In answer to your question, Gilles, there are some applications where 100ms is a big deal and if they just make one or two calls, the associated latency will be surprising and could cause problems for them.  If we get a lot of feedback from users indicating that this latency is a material practical problem for them, or prevents them using the library, we may want to consider making the embedded use configurable.  I can think of only a few cases in my experience where this might be an issue; but I am thankful to Arne for having pointed it out and Luc for improving the documentation.","18/May/11 16:02;erans;{quote}
In answer to your question, Gilles, there are some applications where 100ms is a big deal and if they just make one or two calls [...]
{quote}

This does not answer my question, which could be restated as: How can 100 ms matter when the JVM can take several seconds to start up?
I'm curious of what real applications (that use CM) would run for less than a few seconds...

{quote}
If we get a lot of feedback from users indicating that this latency is a material practical problem for them, or prevents them using the library, we may want to consider making the embedded use configurable.
{quote}

I surely hope that micro-benchmarks are not going to be taken into consideration...

{quote}
I am thankful to Arne for having pointed it out [...]
{quote}

At the time ""FastMath"" was introduced, I had already pointed out the relative slowness of some functions, to which it had been answered that the ""fast"" in ""FastMath"" would kick in only when doing several millions calls (i.e. after the JIT compiler would compile the methods to native code).

{quote}
I am thankful to [...] Luc for improving the documentation.
{quote}

+1
","18/May/11 17:21;luc;{quote}
This does not answer my question, which could be restated as: How can 100 ms matter when the JVM can take several seconds to start up?
I'm curious of what real applications (that use CM) would run for less than a few seconds...
{quote}

I think mainly about hosted application, in environments like Eclipse, web servers, service oriented architectures, perhaps even Android devices. The JVM is already started but I'm not sure the class are reused between requests, I think a new fresh context is set up with a new classloader, which involves reloading the class.

For sure, micro-benchmark should be avoided. Despite it is quite old, the paper about flawed micro benchmark by Brian Goetz [http://www.ibm.com/developerworks/java/library/j-jtp02225/index.html] is really enlightening.

If we get further reports about this latency, we may look at a way to pre-compute the tables at compile time rather than at runtime to see if we can save some milliseconds.","18/May/11 20:33;erans;{quote}
hosted application, in environments like [...] web servers, [...]
{quote}

From what I've just been reading, servlets are only reloaded when their "".class"" file has changed. And they refer to this as a feature (to allow code to be modified without needing a server restart) but also as a hack (because the usual class loader of the JVM does not do that)...

I don't know how Eclipse or Android works but I don't see why a class would be reloaded inside a given application. For Android, the recompilation argument doesn't even apply.
","19/May/11 06:05;aploese;{quote}This does not answer my question, which could be restated as: How can 100 ms matter when the JVM can take several seconds to start up?
I'm curious of what real applications (that use CM) would run for less than a few seconds...{quote}

I could imagin of an GUI app where the users put some values (maybe a 3x3 matix) ant hit the calc button and wait. In a GUI 100 ms are a long time.

So the developer can load FastMath (if she really want to use FastMath) in a separate thread at startup ...

Thist hwole issue is more about usabillity and expected behavior and not the fact that it takes up to 100ms to initialize FastMath.

If I use a lib where I am not really satified with, I will try to replace it. If I know there is a startup penalty, I know it, and I can put the startup time in a place where it does not hurt - its fine.   ",,,,,,,,,,,,,,,,,
GraggBulirschStoerIntegrator output too low,MATH-596,12510666,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,dhendriks,dhendriks,17/Jun/11 14:44,24/Mar/12 16:16,20/Mar/20 20:33,26/Jun/11 16:28,3.0,,,,,,,,,,0,,,,,,"I have the following problem:
x(3.0) = 4.0
x' = 3.0
t0 = 3.0
tend = 10.0

ODE solving using the GraggBulirschStoerIntegrator(1e-10, 100.0, 1e-7, 1e-7) integrator, gives me:

t, x, x'
3.0, 4.0, 3.0
3.105840007284127, 4.0, 3.0
3.829973288493221, 4.31752002185238, 3.0
8.784328663271161, 6.489919865479664, 3.0
10.0, 21.35298598981348, 3.0

Clearly, the value of x at time 3.10... should be something like 4.30... and not 4.0. Also, the value of x at time 10.0 should be around 25.0 and not be 21.35...

If we switch to the DormandPrince853Integrator(1e-10, 100.0, 1e-7, 1e-7), it gives me:

3.0, 4.0, 3.0
3.079933916721644, 4.239801750164932, 3.0
3.8792730839380845, 6.637819251814253, 3.0
10.0, 24.999999999999996, 3.0

as expected.

This seems to me like the GraggBulirschStoerIntegrator has a bug...",,,,,,,,,,,,,,"17/Jun/11 14:46;dhendriks;MyTest.java;https://issues.apache.org/jira/secure/attachment/12482939/MyTest.java","23/Jun/11 09:08;dhendriks;MyTest2.java;https://issues.apache.org/jira/secure/attachment/12483564/MyTest2.java",,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-06-22 14:12:57.981,,,false,,,,,,,,,,,,,,,,,69988,,,Sun Jun 26 16:28:07 UTC 2011,,,,,,,"0|i0rtkn:",160435,,,,,,,,,,,,,,,,"17/Jun/11 14:46;dhendriks;This is the file I used to test this. Running it gives the output as given in de issue description.","22/Jun/11 14:12;luc;It seems to me the problem is related to the interpolator the integrator sends to the step handler.
If I change the implementation of requiresDenseOutput to return true instead of false in the step handle, the error becomes about 9e-16.

Could you check this change completely solves the problem for you.

This requiresDenseOutput was added long ago (even before this code was included in Apache Commons Math) as an optimization to avoid calling computeDerivatives too many times for some integrators. In fact, only Dormand-Prince 8 (5,3) needs it because if the interpolator is not used, we can save 3 calls per step.

Now I think this feature brings more problems than it solves:
 - it forces users to implement this method despite its purpose is not clear,
 - obviously it creates problems with at least Gragg-Bulirsch-Stoer since this
   integrator really needs interpolation
 - it will create the same problems for Adams integrators (they also need interpolation)
 - this ""optimization"" is useful only for one integrator
 - in many cases, even for this integrator it does not optimize anything since
   people will need interpolation

So I would like to completely remove this.
I'm switching to the developers mailing list to discuss about it. It is a better place for discussion than
this JIRA issue. Please join the discussion here, and we will post the conclusion to complete this report.","23/Jun/11 09:08;dhendriks;New test, with output for both dense and non-dense step handler.

Note how for the non-dense step handler, the derivatives are computed for 10.0, and the result is 24.999999999999996. It then continues to call the step handler for time 10.0 with value 21.35298598981348, which is the value calculated for time 8.784328663271161, from before the previous time the step handler was called. It seems the compute derivative method is called for the appropriate times, only the output is not correctly used to set the interpolator values.

If you think this is indeed *only* caused by the optionality of dense output, or it is no longer relevant if non-dense output is removed, then removing the non-dense option would indeed fix this issue.

It would probably be better to create a separate issue for the removal of non-dense output. I have no objections to the removal of non-dense output.
","26/Jun/11 14:57;luc;Yes, I think the optional dense output feature is the only problem here.
When the step handler doesn't requirs dense output and there are no events, a dummy step interpolator is used. This is due to the following statements in the integrator:
{code}
if (denseOutput) {
  interpolator = new GraggBulirschStoerStepInterpolator(y, yDot0,
                                                        y1, yDot1,
                                                        yMidDots, forward);
} else {
  interpolator = new DummyStepInterpolator(y, yDot1, forward);
}
{code}

So in your case, you get a DummyStepInterpolator which simply copies some intermediate states computed earlier.

I will open a separate issue for removing optional dense output, solve the new issue and solve this one afterwards.
","26/Jun/11 16:28;luc;Fixed in subversion repository as of r1139831.

Thanks for the report",,,,,,,,,,,,,,,,,,,,,,,,,,
ADJUSTED R SQUARED INCORRECT IN REGRESSION RESULTS,MATH-619,12514014,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,gsteri1,gsteri1,14/Jul/11 05:33,24/Mar/12 16:16,20/Mar/20 20:33,14/Jul/11 06:15,3.0,,,,,,,3.0,,,0,,,,,,"I forgot to cast to double when dividing two integers:

            this.globalFitInfo[ADJRSQ_IDX] = 1.0 - 
                    (1.0 - this.globalFitInfo[RSQ_IDX]) *
                    (  nobs / ( (nobs - rank)));
Should be
            this.globalFitInfo[ADJRSQ_IDX] = 1.0 - 
                    (1.0 - this.globalFitInfo[RSQ_IDX]) *
                    ( (double) nobs / ( (double) (nobs - rank)));

Patch attached.",Java,,,,,,,,,,,,,"14/Jul/11 05:33;gsteri1;regres;https://issues.apache.org/jira/secure/attachment/12486407/regres",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-07-14 06:15:50.147,,,false,,,,,,,,,,,,,,,,,68104,,,Thu Jul 14 06:15:50 UTC 2011,,,,,,,"0|i0ap4v:",60345,,,,,,,,,,,,,,,,"14/Jul/11 06:15;psteitz;Fixed in r1146575.  Lets keep the updates (including test cases :) for RegressionResults on (still open) MATH-607.  We can use separate issues for implementations; but we should keep the updates to RegressionResults attached to that issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)",MATH-727,12535580,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,17/Dec/11 16:36,24/Mar/12 16:16,20/Mar/20 20:33,17/Dec/11 16:56,3.0,,,,,,,3.0,,,0,,,,,,"Adaptive step size integrators compute the first step size by themselves if it is not provided.
For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,221264,,,Sat Dec 17 16:56:04 UTC 2011,,,,,,,"0|i0aopb:",60275,,,,,,,,,,,,,,,,"17/Dec/11 16:56;luc;Fixed in subversion repository as of r1215524.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MaxIterationsExceededException in SVD (EigenDecompositionImpl.findEigenVectors ) caused by NaN,MATH-383,12468767,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,dimpbx,kushkuley,kushkuley,07/Jul/10 22:18,24/Mar/12 16:16,20/Mar/20 20:33,20/Jul/11 12:19,3.0,,,,,,,3.0,,,0,,,,,,"In the following code fragment from EigenDecompositionImpl.findEigenVectors 

....................................................................
    for (int j = 0; j < n; j++) {
            int its = 0;
            int m;
            do {
                for (m = j; m < n - 1; m++) {
                    double delta = Math.abs(realEigenvalues[m]) + Math.abs(realEigenvalues[m + 1]);
                    if (Math.abs(e[m]) + delta == delta) {
                        break;
                    }
                }
          

                if (m != j) {
                    .........................................

the test for ""(Math.abs(e[m]) + delta == delta)"" is not executed  when m is equal to n -1.
As a result  e[m]  == 0 (does happen!) causes variables q and realEigenvalues[m] to become NaN that in turn causes   ""Math.abs(e[m]) + delta == delta)"" to become always false.

My guess (seems to work) is that another test for e[m] == 0 is needed, so that the code becomes

   for (int j = 0; j < n; j++) {
            int its = 0;
            int m;
            do {
                for (m = j; m < n - 1; m++) {
                    double delta = Math.abs(realEigenvalues[m]) + Math.abs(realEigenvalues[m + 1]);
                    if (Math.abs(e[m]) + delta == delta) {
                        break;
                    }
                }
          
               // begin patch 
               if ( m == n - 1 && e[m-1] == 0 )
                       break;
               // end patch


                
               if (m != j) {
                    ......................................... 

or something like that

",Java 6,86400,86400,,0%,86400,86400,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-07-26 10:12:23.786,,,false,,,,,,,,,,,,,,,,,67398,,,Wed Jul 20 12:19:35 UTC 2011,,,,,,,"0|i0rut3:",160635,,,,,,,,,,,,,,,,"26/Jul/10 10:12;psteitz;Thanks for reporting this.","26/Jul/10 14:52;dimpbx;Could you supply with a matrix which causes the problem?  The present unit tests are not helpful enough.","26/Dec/10 20:03;psteitz;Leaving open, but pushing out to 3.0.  Needs more info and possibly retest when 3.0 changes to SVD are implemented.","20/Jul/11 12:19;luc;Fixed in subversion repository as of r1148714.

This issue was fixed by changing SVD implementation according to issue MATH-611.",,,,,,,,,,,,,,,,,,,,,,,,,,,
SymmLQ not tested in SymmLQTest,MATH-770,12547758,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,celestin,celestin,celestin,23/Mar/12 07:24,24/Mar/12 16:16,20/Mar/20 20:33,23/Mar/12 08:14,3.0,3.1,,,,,,3.1,,,0,linear,solver,,,,"In {{SymmLQTest}}, two test actually create instances of {{ConjugateGradient}} instead of {{SymmLQ}}. These tests are
* {{testUnpreconditionedNormOfResidual()}}
* {{testPreconditionedNormOfResidual()}}.",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,232855,,,Fri Mar 23 08:14:24 UTC 2012,,,,,,,"0|i0rtav:",160391,,,,,,,,,,,,,,,,"23/Mar/12 07:26;celestin;Typos corrected in {{r1304215}}; {{testPreconditionedNormOfResidual()}} now fails.","23/Mar/12 07:53;celestin;Test corrected in {{r1304216}}. Failure was due to the fact that in {{SymmLQ}}, the residual for the preconditioned system is really {{P * r}}, where {{r = b - A * x}}, and {{P}} is the square root of the preconditioner.","23/Mar/12 08:14;celestin;There is a follow-up to this ticket in MATH-771.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
MathRuntimeException with simple ebeMultiply on OpenMapRealVector,MATH-645,12518787,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,celestin,celestin,13/Aug/11 16:18,24/Mar/12 16:16,20/Mar/20 20:33,02/Sep/11 20:54,3.0,,,,,,,3.0,,,0,linear,sparse,vector,,,"The following piece of code
{code:java}
import org.apache.commons.math.linear.OpenMapRealVector;
import org.apache.commons.math.linear.RealVector;

public class DemoBugOpenMapRealVector {
    public static void main(String[] args) {
        final RealVector u = new OpenMapRealVector(3, 1E-6);
        u.setEntry(0, 1.);
        u.setEntry(1, 0.);
        u.setEntry(2, 2.);
        final RealVector v = new OpenMapRealVector(3, 1E-6);
        v.setEntry(0, 0.);
        v.setEntry(1, 3.);
        v.setEntry(2, 0.);
        System.out.println(u);
        System.out.println(v);
        System.out.println(u.ebeMultiply(v));
    }
}
{code}
raises an exception
{noformat}
org.apache.commons.math.linear.OpenMapRealVector@7170a9b6
Exception in thread ""main"" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating
	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373)
	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564)
	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372)
	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1)
	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)
{noformat}
",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-08-13 18:12:31.058,,,false,,,,,,,,,,,,,,,,,62820,,,Sat Aug 13 18:12:31 UTC 2011,,,,,,,"0|i0ap1r:",60331,,,,,,,,,,,,,,,,"13/Aug/11 18:12;erans;Probably fixed in revision 1157403.
Added a unit test for ""ebeMultiply"" and ""ebeDivide"".
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BaseMultiStartMultivariateRealOptimizer.optimize() can generate NPE if starts < 1,MATH-466,12494847,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,sebb,sebb,06/Jan/11 14:33,24/Mar/12 16:16,20/Mar/20 20:33,07/Jan/11 17:01,3.0,,,,,,,3.0,,,0,,,,,,"The Javadoc for BaseMultiStartMultivariateRealOptimizer says that starts can be <= 1; however if it is set to 0, then the optimize() method will try to throw a null exception.

Perhaps starts should be constrained to be at least 1?",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-01-06 19:50:20.572,,,false,,,,,,,,,,,,,,,,,150595,,,Fri Jan 07 17:01:48 UTC 2011,,,,,,,"0|i0rubj:",160556,,,,,,,,,,,,,,,,"06/Jan/11 14:34;sebb;Same issue applies to BaseMultiStartMultivariateVectorialOptimizer and MultiStartUnivariateRealOptimizer","06/Jan/11 19:50;luc;Yes, the javadoc should be changed and the number of starts should be checked with an error triggered if it is not at least 1.","07/Jan/11 17:01;erans;Added preconditions checks in revision 1056391.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad interaction between step handlers and events detectors that reset state in ODE integrators,MATH-706,12531198,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,11/Nov/11 23:33,24/Mar/12 16:16,20/Mar/20 20:33,11/Nov/11 23:40,3.0,,,,,,,3.0,,,0,,,,,,"During ODE integration, when en event detector attempts to reset a state, if there are also step handlers
associated with the integrator, the reset state is not called with the state at event time, but may be called with some other state. The time provided always correspond to the real event time (and hence is inconsistent with the state).",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,216935,,,Fri Nov 11 23:40:33 UTC 2011,,,,,,,"0|i0aosf:",60289,,,,,,,,,,,,,,,,"11/Nov/11 23:40;luc;Fixed in subversion repository as of r1201105.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Errors in BOBYQAOptimizer when numberOfInterpolationPoints is greater than 2*dim+1,MATH-728,12535776,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,bjohnson,bjohnson,20/Dec/11 02:05,24/Mar/12 16:16,20/Mar/20 20:33,11/Feb/12 23:15,3.0,,,,,,,3.0,,,0,,,,,,"I've been having trouble getting BOBYQA to minimize a function (actually a non-linear least squares fit) so as one change I increased the number of interpolation points.  It seems that anything larger than 2*dim+1 causes an error (typically at

line 1662
                   interpolationPoints.setEntry(nfm, ipt, interpolationPoints.getEntry(ipt, ipt));

I'm guessing there is an off by one error in the translation from FORTRAN.  Changing the BOBYQAOptimizerTest as follows (increasing number of interpolation points by one) will cause failures.

Bruce



Index: src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java
===================================================================
--- src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java	(revision 1221065)
+++ src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java	(working copy)
@@ -258,7 +258,7 @@
 //        RealPointValuePair result = optim.optimize(100000, func, goal, startPoint);
         final double[] lB = boundaries == null ? null : boundaries[0];
         final double[] uB = boundaries == null ? null : boundaries[1];
-        BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 1);
+        BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 2);
         RealPointValuePair result = optim.optimize(maxEvaluations, func, goal, startPoint, lB, uB);
 //        System.out.println(func.getClass().getName() + "" = "" 
 //              + optim.getEvaluations() + "" f("");
","Mac Java(TM) SE Runtime Environment (build 1.6.0_29-b11-402-11M3527)
",,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-12-20 13:47:18.654,,,false,,,,,,,,,,,,,,,,,221460,,,Sat Feb 11 23:15:55 UTC 2012,,,,,,,"0|i0aop3:",60274,,,,,,,,,,,,,,,,"20/Dec/11 03:22;bjohnson;Making this change fixes the problem and seems reasonable, but one of the original translators (from FORTRAN to Java) should look this section of code over.


@@ -1657,10 +1657,10 @@
                     final int tmp2 = jpt;
                     jpt = ipt - n;
                     ipt = tmp2;
-                    throw new PathIsExploredException(); // XXX
+                    //throw new PathIsExploredException(); // XXX
                 }
-                interpolationPoints.setEntry(nfm, ipt, interpolationPoints.getEntry(ipt, ipt));
-                interpolationPoints.setEntry(nfm, jpt, interpolationPoints.getEntry(jpt, jpt));
+                interpolationPoints.setEntry(nfm, ipt-1, interpolationPoints.getEntry(ipt, ipt-1));
+                interpolationPoints.setEntry(nfm, jpt-1, interpolationPoints.getEntry(jpt, jpt-1));

","20/Dec/11 13:47;erans;Thanks for spotting this. Revision 1221253 contains your fix, together with a unit test that exercises the setting of more interpolation points.
","20/Dec/11 15:15;bjohnson;Thanks for checking this in, and it's worth noting that my function now minimizes properly.  It seems that with a large number of interpolation points there was an out of bounds error, but even where that out of bounds error wasn't thrown, the interpolation was set up wrong for the last few points, causing a failure to converge.  Others who have had trouble with convergence would do well to recheck with this fix.","20/Dec/11 15:43;erans;Anyone who wishes to use the ""BOBYQAOptimizer"" class should have a look at the MATH-621 issue.
As you have figured out, the code is really not ready yet. Unfortunately, the implementation being not ""natural"" in Java, it is not easy to separate algorithm complexity from Fortran-driven optimizations (which should be removed).
The problem is all compounded by the fall-through switch-cases which should also be recoded properly.

We are still in the middle of the river: Many things have been improved structure-wise but bugs could have crept in while doing so. Bugs like the one you discovered.
And we don't have a thorough test suite to ensure that every code path works as in the original Fortran.

The code was checked in under the assumption that it would be converted into ""natural"" Java, so that people can maintain it.
I wonder whether it should not be removed for the upcoming release...
","20/Dec/11 15:55;bjohnson;I, for one, am very happy to see BOBYQA in CM and will continue with ""real world"" testing.  If I find more issues I'll certainly report them, but so far it's looking very promising with my applications.","23/Dec/11 13:40;erans;It would be very useful if you could provide unit tests that cover the still unexplored code paths (cf. lines containing ""throw new PathIsExploredException();"").
Thanks in advance for your contributions.
","08/Feb/12 12:14;erans;Hi Bruce.

Would you be interested in testing your code with a large number of ""additional"" interpolation points?
I'm referring to the unit test ""testConstrainedRosenWithMoreInterpolationPoints"" in ""BOBYQAOptimizerTest"", at lines 236-256. It would be nice to know whether the failures, for some values of the number of points, reveal yet other bugs. (Or whether they are expected; in which case, the reason would be a welcome addition to the documentation...)
","08/Feb/12 12:46;bjohnson;Hi Giles,

I'll try to do so over the next couple days.

cheers,

Bruce","11/Feb/12 22:11;bjohnson;I've been playing with BOBYQA (downloaded from svn repository today, and commenting out the PathNotExplored exceptions).  A couple observations.
1) I can't make it fail with large number of interpolation points (as long as you stay under the (2n+1)*(2n+2)/2 recommended max. So this issue is resolved.
2) With large number of interpolation points the algorithm is significantly slower.  I'm minimizing a function  with a 169 parameters.  The function evaluation takes ~50 msec.  With n+2 interpolation points, the additional time for each step is about 10 msec.  With 2*n+1 points, the additional time is about 50 msec, and with about 6*n, the additional time is 250 msec.  So with larger nInterpolation points a lot of time is spent in the algorithm, besides evaluating the function.   At some point I'll try to do some profiling of the code.
3) It makes a big difference to normalize the parameters as the initial search region is dependent on the point with the smallest boundary difference.  So it seems one shouldn't directly fit the ""natural"" parameters but normalized values.","11/Feb/12 23:15;erans;Although the bug that triggered this issue is fixed, failures of the unit test still miss an explanation...

The poor performance is to be expected given the current state of the code (e.g. many matrix calculations are done explicitly, with getters and setters, instead of calling methods of the matrix objects).
",,,,,,,,,,,,,,,,,,,,,
GaussianFitter Unexpectedly Throws NotStrictlyPositiveException,MATH-519,12499105,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ole,ole,19/Feb/11 04:37,24/Mar/12 16:16,20/Mar/20 20:33,22/Feb/11 23:51,3.0,,,,,,,3.0,,,0,,,,,,"Running the following:

    	double[] observations = 
    	{ 
    			1.1143831578403364E-29, 
    			 4.95281403484594E-28, 
    			 1.1171347211930288E-26, 
    			 1.7044813962636277E-25, 
    			 1.9784716574832164E-24, 
    			 1.8630236407866774E-23, 
    			 1.4820532905097742E-22, 
    			 1.0241963854632831E-21, 
    			 6.275077366673128E-21, 
    			 3.461808994532493E-20, 
    			 1.7407124684715706E-19, 
    			 8.056687953553974E-19, 
    			 3.460193945992071E-18, 
    			 1.3883326374011525E-17, 
    			 5.233894983671116E-17, 
    			 1.8630791465263745E-16, 
    			 6.288759227922111E-16, 
    			 2.0204433920597856E-15, 
    			 6.198768938576155E-15, 
    			 1.821419346860626E-14, 
    			 5.139176445538471E-14, 
    			 1.3956427429045787E-13, 
    			 3.655705706448139E-13, 
    			 9.253753324779779E-13, 
    			 2.267636001476696E-12, 
    			 5.3880460095836855E-12, 
    			 1.2431632654852931E-11 
    	};
  
    	GaussianFitter g = 
    		new GaussianFitter(new LevenbergMarquardtOptimizer());
    	
    	for (int index = 0; index < 27; index++)
    	{
    		g.addObservedPoint(index, observations[index]);
    	}
       	g.fit();

Results in:

org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0)
	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184)
	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129)


I'm guessing the initial guess for sigma is off.  ",,14400,14400,,0%,14400,14400,,,,,,,"19/Feb/11 22:12;erans;GaussianFitter.java;https://issues.apache.org/jira/secure/attachment/12471493/GaussianFitter.java","19/Feb/11 18:03;ole;GaussianFitter2Test.java;https://issues.apache.org/jira/secure/attachment/12471478/GaussianFitter2Test.java",,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-02-19 12:28:41.507,,,false,,,,,,,,,,,,,,,,,150635,,,Tue Feb 22 23:51:58 UTC 2011,,,,,,,"0|i0ru0f:",160506,,,,,,,,,,,,,,,,"19/Feb/11 12:28;erans;Will you investigate?
A good starting point would be to prepare a unit test.

If it's indeed the guesser, we should add a check that the guessed sigma is positive (and, if not, return the opposite value).
","19/Feb/11 13:48;ole;Yes I'm going to start looking at it this weekend.","19/Feb/11 17:38;ole;Gilles - BTW - Did you ever figure out what was happening with this test in the ParametricGaussianFunction test:

    /**
     * The parameters d is 0.
     *
     * @throws MathUserException in the event of a test case error
     */
    @Test(expected=ZeroException.class)
    public void testValue03() throws MathUserException {
        Gaussian.Parametric f = new Gaussian.Parametric();
        f.value(0.0, new double[] {1.0, 1.0, 0.0});
    }

","19/Feb/11 18:13;ole;It seems the optimizer is producing a negative value for sigma...","19/Feb/11 18:38;ole;I tried commenting out the validateParameters(param) line in Gaussian.value() and now it converges:

Mean: 53.15727927329495
Sigma: 5.752146229144571","19/Feb/11 22:12;erans;I guess that the clean solution would be to be able to specify constraints such that the optimizer won't try invalid parameters.

An ugly workaround would be that the ""CurveFitter"" internally catches all ""RuntimeException""s generated by the function, and consider the residuals to be infinite. But this would violate the stated policy that CM does not catch ""MathUserException""s (and could have nasty side-effects when the exception is really unexpected).

I've tried another workaround in ""GaussianFitter"" (see attached file) whereby an invalid parameter is turned into the function returning ""NaN"" (""POSITIVE_INFINITY"" also works).
Let me know if you see any issues with this solution or if would be an adequate solution for this problem.
","19/Feb/11 23:52;ole;I like the last option the best.  I'm wondering though whether we are placing constraints on variables that are not supposed to be constrained?

Perhaps the optimizer should be allowed to call a method on Gaussian that does not effectively constrain the valid values of sigma?

","20/Feb/11 18:42;erans;In the case of the Gaussian, that parameter (""sigma"") is used squared, so negative values don't really matter; however, the semantics of ""sigma"" calls for it to be non-negative.
But it certainly cannot be zero, so that's a constraint that cannot be removed.
","21/Feb/11 01:53;ole;I think that while the optimizer is searching for a solution, it should be able to pass parameters that are not necessarily valid, but enable it to proceed.  If it finds a solution that is not valid, then an exception should be thrown. 

","21/Feb/11 02:25;erans;{quote}
[...] pass parameters that are not necessarily valid [...]
{quote}

What would that mean?
To direct its search, the optimizer needs feedback from the objective function; in case of invalid parameters, the objective function cannot provide feedback (i.e. compute the theoretical curve)!

The only issue is how to ""tell"" the optimizer that it should discard invalid parameters (i.e. make it consider that they are far away from a solution).
","21/Feb/11 04:01;ole;I'm assuming that the value that results from passing in a negative sigma is closer to the optimal than POSITIVE infinity or NaN, and that this will result in faster convergence.  How about only returning Nan or POSITIVE_INFINITY if the optimizer passes in zero for sigma, but letting it proceed otherwise?


","21/Feb/11 08:27;luc;Maybe this discussion should be held on the dev list since it becomes long.
Anyway, returning NaN or POSITIVE_INFINITY would work only with some optimizers.
I guess a proper solution would be to have constrained optimization available (see MATH-196).
For simple bounds on estimated parameters, this can be done using intermediate variables and mapping functions, but for general non-linear constraints, we need Lagrangian multipliers and all this stuff.","21/Feb/11 12:18;erans;{quote}
I'm assuming that the value that results from passing in a negative sigma is closer to the optimal than POSITIVE infinity or NaN [...]
{quote}

POSITIVE_INFINITY or NaN are the returned values of the objective function and its gradient, specially chosen because they are quite likely to be different from the actual values of the function and its gradient; it's not ""sigma"" that is assumed to infinity or NaN.

If you think is that we can accept a negative sigma as the result of the fitting, I don't agree. In the case of the Gaussian, it's by ""chance"" that a semantically invalid parameter (negative sigma) would still be usable (as it is being squared before use).
In most case you cannot expect such a forgiving situation. For example, if you want to fit ""a"" in the following function:
{noformat}
  log(a * x)
{noformat}
no invalid values for ""a"" are usable.
The ""Gaussian"" class should not be unsafe (no validation of sigma) just because of its particular use here.
[Moreover the workaround is useful in showing users how to setup a fitting of a function that can raise an exception.]

{quote}
[...] and that this will result in faster convergence.
{quote}

Did you try?
","21/Feb/11 20:21;ole;Yes!  I figured out how to quote!

First of all I hope we are talking about this function:
http://en.wikipedia.org/wiki/Gaussian_function

As the objective function right?  If I got that wrong then ignore the below.

{quote}
Anyway, returning NaN or POSITIVE_INFINITY would work only with some optimizers.
{quote}

Seems to me that if the optimizer does not understand POSITIVE_INFINITY then that's a bug.  

{quote}
If you think is that we can accept a negative sigma as the result of the fitting...
{quote}

No no no - Not at all.  I'm saying that we should let the optimizer try negative values for sigma if it wants to while it's in the middle of trying to find the optimal sigma.  If it returns a negative sigma as a result, then we need to throw a NotStrictlyPositiveException.

{quote}
Did you try?
{quote}

I could give it a whirl, but it does not necessarily prove anything.  Even it it converges quicker, does that mean it will do so in all cases?  It just seems to me like POSITIVE_INFINITE is as far from the optimal as you can get, and therefore it will take longer to get to the optimal.

Also, I changed my mind about an earlier comment.  If sigma is zero then the gaussian function is zero, so we should probably just return zero.
","21/Feb/11 22:14;erans;{quote}
I'm saying that we should let the optimizer try negative values for sigma if it wants to while it's in the middle of trying to find the optimal sigma.
{quote}

The point is that we cannot allow invalid parameters because, for those values (of the parameters), the objective function is, by definition of ""invalid"", undefined.

{quote}
[...] Even it it converges quicker [...]
{quote}

Well, actually it doesn't (cf. my mail on the ""dev"" ML).

{quote}
It just seems to me like POSITIVE_INFINITE is as far from the optimal as you can get, [...]
{quote}

Indeed, that's exactly the intention: it tells the optimizer to step back from this wrong value for sigma.
[Note that POSITIVE_INFINITY is not a value of sigma, it is the value of the objective function for any negative sigma.]

{quote}
If sigma is zero then the gaussian function is zero, [...]
{quote}

No, when sigma is strictly zero, there is no Gaussian anymore: the value at ""b"" (mean) is undefined.
Also, cf. http://en.wikipedia.org/wiki/Dirac_delta

","21/Feb/11 23:04;ole;OK - I see what you mean - I also get 17009 iterations with just the validate line commented out.  I'm on board with POSITIVE_INFINITY.    ","22/Feb/11 08:56;luc;If this works, go for it.","22/Feb/11 23:51;erans;Workaround in revision 1073554.",,,,,,,,,,,,,
FieldLUDecomposition.Solver is missing appropriate testing,MATH-673,12523516,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,celestin,celestin,celestin,19/Sep/11 06:22,18/Mar/12 09:27,20/Mar/20 20:33,20/Sep/11 06:14,3.0,,,,,,,3.0,,,0,,,,,,I could not find any unit test for this class.,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,60136,,,Sun Mar 18 09:27:11 UTC 2012,,,,,,,"0|i0aoyf:",60316,,,,,,,,,,,,,,,,"20/Sep/11 06:14;celestin;Revision r1172988 proposes {{FieldLUSolverTest}}, a unit test based on what was done with {{RealMatrix}} (see {{LUSolverTest}}). The entries are {{Fraction}}. The tests did not reveal any bug.","18/Mar/12 09:27;celestin;Fixed in 3.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath scalb() does not handle large magnitude exponents correctly,MATH-502,12496590,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb,sebb,24/Jan/11 19:15,23/Mar/11 20:39,20/Mar/20 20:33,24/Jan/11 19:24,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,"scalb does not handle MAX_VALUE exponents properly:

double scalb(-1.7976931348623157E308, 2147483647) expected -Infinity actual -8.988465674311579E307 entries [6, 5]
double scalb(1.7976931348623157E308, 2147483647) expected Infinity actual 8.988465674311579E307 entries [7, 5]
double scalb(-1.1102230246251565E-16, 2147483647) expected -Infinity actual -5.551115123125783E-17 entries [8, 5]
double scalb(1.1102230246251565E-16, 2147483647) expected Infinity actual 5.551115123125783E-17 entries [9, 5]
double scalb(-2.2250738585072014E-308, 2147483647) expected -Infinity actual -0.0 entries [10, 5]
double scalb(2.2250738585072014E-308, 2147483647) expected Infinity actual 0.0 entries [11, 5]

float scalb(3.4028235E38, 2147483647) expected Infinity actual 1.7014117E38 entries [7, 5]
float scalb(-3.4028235E38, 2147483647) expected -Infinity actual -1.7014117E38 entries [9, 5]

It looks as though the problem is with the calculation of the scaledExponent - for large values, this can wrap round, so some of the checks against its value may give misleading results.",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-01-24 19:24:13.447,,,false,,,,,,,,,,,,,,,,,150621,,,Wed Mar 23 20:39:04 UTC 2011,,,,,,,"0|i0ru47:",160523,,,,,,,,,,,,,,,,"24/Jan/11 19:24;luc;fixed in subversion repository as of r1062928 for trunk and r1062929 for branch 2.X","23/Mar/11 20:39;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FastMath nextAfter(double,double) bugs with special doubles",MATH-499,12496428,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,sebb,sebb,sebb,23/Jan/11 11:02,23/Mar/11 20:38,20/Mar/20 20:33,23/Jan/11 12:24,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,"nextAfter(double, double) is added in StrictMath 1.6, so one needs to test with Java 1.6 to see thi.

There are various boundary errors with nextAfter(double, double) - see below.

I think these are partially due to missing checks for special cases (e.g. the NaNs), and partially due to the following code:

{code}
if (d * (direction - d) >= 0) {
// we should increase the mantissa
{code}

This appears to be a shorthand for something like:

{code}
if (((d >=0) && (direction >= d)) || ((d<0) && (direction <0))) {
// we should increase the mantissa
{code}

however the expression (direction - d) overlows for some double values, thus causing the wrong branch to be taken.

double nextAfter(-0.0, -0.0) expected -0.0 actual 4.9E-324 entries [1, 1]
double nextAfter(-0.0, 0.0) expected 0.0 actual 4.9E-324 entries [1, 2]
double nextAfter(-0.0, NaN) expected NaN actual 4.9E-324 entries [1, 3]
double nextAfter(0.0, -0.0) expected -0.0 actual 4.9E-324 entries [2, 1]
double nextAfter(0.0, 0.0) expected 0.0 actual 4.9E-324 entries [2, 2]
double nextAfter(0.0, NaN) expected NaN actual 4.9E-324 entries [2, 3]
double nextAfter(-Infinity, NaN) expected NaN actual -Infinity entries [4, 3]
double nextAfter(Infinity, NaN) expected NaN actual Infinity entries [5, 3]
double nextAfter(-1.7976931348623157E308, NaN) expected NaN actual -1.7976931348623155E308 entries [6, 3]
double nextAfter(1.7976931348623157E308, NaN) expected NaN actual 1.7976931348623155E308 entries [7, 3]
double nextAfter(-1.1102230246251565E-16, NaN) expected NaN actual -1.1102230246251564E-16 entries [8, 3]
double nextAfter(1.1102230246251565E-16, NaN) expected NaN actual 1.1102230246251564E-16 entries [9, 3]
double nextAfter(-2.2250738585072014E-308, -0.0) expected -2.225073858507201E-308 actual -2.225073858507202E-308 entries [10, 1]
double nextAfter(-2.2250738585072014E-308, 0.0) expected -2.225073858507201E-308 actual -2.225073858507202E-308 entries [10, 2]
double nextAfter(-2.2250738585072014E-308, NaN) expected NaN actual -2.225073858507201E-308 entries [10, 3]
double nextAfter(-2.2250738585072014E-308, 1.1102230246251565E-16) expected -2.225073858507201E-308 actual -2.225073858507202E-308 entries [10, 9]
double nextAfter(-2.2250738585072014E-308, 2.2250738585072014E-308) expected -2.225073858507201E-308 actual -2.225073858507202E-308 entries [10, 11]
double nextAfter(-2.2250738585072014E-308, -4.9E-324) expected -2.225073858507201E-308 actual -2.225073858507202E-308 entries [10, 12]
double nextAfter(-2.2250738585072014E-308, 4.9E-324) expected -2.225073858507201E-308 actual -2.225073858507202E-308 entries [10, 13]
double nextAfter(2.2250738585072014E-308, -0.0) expected 2.225073858507201E-308 actual 2.225073858507202E-308 entries [11, 1]
double nextAfter(2.2250738585072014E-308, 0.0) expected 2.225073858507201E-308 actual 2.225073858507202E-308 entries [11, 2]
double nextAfter(2.2250738585072014E-308, NaN) expected NaN actual 2.225073858507201E-308 entries [11, 3]
double nextAfter(2.2250738585072014E-308, -1.1102230246251565E-16) expected 2.225073858507201E-308 actual 2.225073858507202E-308 entries [11, 8]
double nextAfter(2.2250738585072014E-308, -2.2250738585072014E-308) expected 2.225073858507201E-308 actual 2.225073858507202E-308 entries [11, 10]
double nextAfter(2.2250738585072014E-308, -4.9E-324) expected 2.225073858507201E-308 actual 2.225073858507202E-308 entries [11, 12]
double nextAfter(2.2250738585072014E-308, 4.9E-324) expected 2.225073858507201E-308 actual 2.225073858507202E-308 entries [11, 13]
double nextAfter(-4.9E-324, -0.0) expected -0.0 actual -1.0E-323 entries [12, 1]
double nextAfter(-4.9E-324, 0.0) expected -0.0 actual -1.0E-323 entries [12, 2]
double nextAfter(-4.9E-324, NaN) expected NaN actual -0.0 entries [12, 3]
double nextAfter(-4.9E-324, 1.1102230246251565E-16) expected -0.0 actual -1.0E-323 entries [12, 9]
double nextAfter(-4.9E-324, 2.2250738585072014E-308) expected -0.0 actual -1.0E-323 entries [12, 11]
double nextAfter(-4.9E-324, 4.9E-324) expected -0.0 actual -1.0E-323 entries [12, 13]
double nextAfter(4.9E-324, -0.0) expected 0.0 actual 1.0E-323 entries [13, 1]
double nextAfter(4.9E-324, 0.0) expected 0.0 actual 1.0E-323 entries [13, 2]
double nextAfter(4.9E-324, NaN) expected NaN actual 0.0 entries [13, 3]
double nextAfter(4.9E-324, -1.1102230246251565E-16) expected 0.0 actual 1.0E-323 entries [13, 8]
double nextAfter(4.9E-324, -2.2250738585072014E-308) expected 0.0 actual 1.0E-323 entries [13, 10]
double nextAfter(4.9E-324, -4.9E-324) expected 0.0 actual 1.0E-323 entries [13, 12]",Java 1.6,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-01-23 12:14:55.293,,,false,,,,,,,,,,,,,,,,,150619,,,Wed Mar 23 20:38:42 UTC 2011,,,,,,,"0|i0ru4v:",160526,,,,,,,,,,,,,,,,"23/Jan/11 12:14;luc;This should have been fixed in repository this morning.
Do you still see these problems ?","23/Jan/11 12:24;sebb;Fixed in:

URL: http://svn.apache.org/viewvc?rev=1062387&view=rev
Log:
fixed nextAfter implementations for handling of some special values
fixed the signature of the float version, as the spec is to have a double second argument
moved the existing tests that were used in the former implementation in MathUtils,
fixing them also as two of them were not compliant with the spec for equal numbers
Jira: MATH-478

and

URL: http://svn.apache.org/viewvc?rev=1062385&view=rev
Log:
fixed nextAfter implementations for handling of some special values
fixed the signature of the float version, as the spec is to have a double second argument
moved the existing tests that were used in the former implementation in MathUtils,
fixing them also as two of them were not compliant with the spec for equal numbers
Jira: MATH-478","23/Mar/11 20:38;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath atan2 does not agree with StrictMath for special cases,MATH-494,12496341,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb,sebb,21/Jan/11 21:56,23/Mar/11 20:38,20/Mar/20 20:33,22/Jan/11 20:21,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,"FastMath atan2 does not agree with StrictMath for special cases.

There are two sign problems:
atan2(double -0.0, double -Infinity) expected -3.141592653589793 actual 3.141592653589793 entries [1, 4]
atan2(double -0.0, double Infinity) expected -0.0 actual 0.0 entries [1, 5]

A lot of NaNs where there should be a valid return:
atan2(double -1.7976931348623157E308, double -1.7976931348623157E308) expected -2.356194490192345 actual NaN entries [6, 6]
atan2(double -1.7976931348623157E308, double 1.7976931348623157E308) expected -0.7853981633974483 actual NaN entries [6, 7]
atan2(double -1.7976931348623157E308, double -1.1102230246251565E-16) expected -1.5707963267948968 actual NaN entries [6, 8]
atan2(double -1.7976931348623157E308, double 1.1102230246251565E-16) expected -1.5707963267948966 actual NaN entries [6, 9]
atan2(double -1.7976931348623157E308, double -2.2250738585072014E-308) expected -1.5707963267948968 actual NaN entries [6, 10]
atan2(double -1.7976931348623157E308, double 2.2250738585072014E-308) expected -1.5707963267948966 actual NaN entries [6, 11]
atan2(double -1.7976931348623157E308, double -4.9E-324) expected -1.5707963267948968 actual NaN entries [6, 12]
atan2(double -1.7976931348623157E308, double 4.9E-324) expected -1.5707963267948966 actual NaN entries [6, 13]
atan2(double 1.7976931348623157E308, double -1.7976931348623157E308) expected 2.356194490192345 actual NaN entries [7, 6]
atan2(double 1.7976931348623157E308, double 1.7976931348623157E308) expected 0.7853981633974483 actual NaN entries [7, 7]
atan2(double 1.7976931348623157E308, double -1.1102230246251565E-16) expected 1.5707963267948968 actual NaN entries [7, 8]
atan2(double 1.7976931348623157E308, double 1.1102230246251565E-16) expected 1.5707963267948966 actual NaN entries [7, 9]
atan2(double 1.7976931348623157E308, double -2.2250738585072014E-308) expected 1.5707963267948968 actual NaN entries [7, 10]
atan2(double 1.7976931348623157E308, double 2.2250738585072014E-308) expected 1.5707963267948966 actual NaN entries [7, 11]
atan2(double 1.7976931348623157E308, double -4.9E-324) expected 1.5707963267948968 actual NaN entries [7, 12]
atan2(double 1.7976931348623157E308, double 4.9E-324) expected 1.5707963267948966 actual NaN entries [7, 13]
atan2(double -1.1102230246251565E-16, double -1.7976931348623157E308) expected -3.141592653589793 actual NaN entries [8, 6]
atan2(double -1.1102230246251565E-16, double 1.7976931348623157E308) expected -0.0 actual NaN entries [8, 7]
atan2(double -1.1102230246251565E-16, double -4.9E-324) expected -1.5707963267948968 actual NaN entries [8, 12]
atan2(double -1.1102230246251565E-16, double 4.9E-324) expected -1.5707963267948966 actual NaN entries [8, 13]
atan2(double 1.1102230246251565E-16, double -1.7976931348623157E308) expected 3.141592653589793 actual NaN entries [9, 6]
atan2(double 1.1102230246251565E-16, double 1.7976931348623157E308) expected 0.0 actual NaN entries [9, 7]
atan2(double 1.1102230246251565E-16, double -4.9E-324) expected 1.5707963267948968 actual NaN entries [9, 12]
atan2(double 1.1102230246251565E-16, double 4.9E-324) expected 1.5707963267948966 actual NaN entries [9, 13]
atan2(double -2.2250738585072014E-308, double -1.7976931348623157E308) expected -3.141592653589793 actual NaN entries [10, 6]
atan2(double -2.2250738585072014E-308, double 1.7976931348623157E308) expected -0.0 actual NaN entries [10, 7]
atan2(double 2.2250738585072014E-308, double -1.7976931348623157E308) expected 3.141592653589793 actual NaN entries [11, 6]
atan2(double 2.2250738585072014E-308, double 1.7976931348623157E308) expected 0.0 actual NaN entries [11, 7]
atan2(double -4.9E-324, double -1.7976931348623157E308) expected -3.141592653589793 actual NaN entries [12, 6]
atan2(double -4.9E-324, double 1.7976931348623157E308) expected -0.0 actual NaN entries [12, 7]
atan2(double 4.9E-324, double -1.7976931348623157E308) expected 3.141592653589793 actual NaN entries [13, 6]
atan2(double 4.9E-324, double 1.7976931348623157E308) expected 0.0 actual NaN entries [13, 7]

There are also some spurious errors, which are due to a bug in the test case - expecting the values to be exactly the same as StrictMath
atan2(double 2.2250738585072014E-308, double -4.9E-324) expected 1.570796326794897 actual 1.5707963267948968 entries [11, 12]
atan2(double -2.2250738585072014E-308, double -4.9E-324) expected -1.570796326794897 actual -1.5707963267948968 entries [10, 12]
atan2(double 1.1102230246251565E-16, double -2.2250738585072014E-308) expected 1.5707963267948968 actual 1.5707963267948966 entries [9, 10]
atan2(double -1.1102230246251565E-16, double -2.2250738585072014E-308) expected -1.5707963267948968 actual -1.5707963267948966 entries [8, 10]
",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-03-23 20:38:08.963,,,false,,,,,,,,,,,,,,,,,150614,,,Wed Mar 23 20:38:08 UTC 2011,,,,,,,"0|i0ru5z:",160531,,,,,,,,,,,,,,,,"22/Jan/11 01:22;sebb;Note: using MathUtils.equals(double, double, 1) instead of Double.equals() allows the last few comparisons to succeed; however this also ignores the differences between +/- 0.0, so a more sensitive test is needed.","23/Mar/11 20:38;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FastMath min and max fail with (Infinity,-Infinity) and (0,0, -0.0)",MATH-493,12496333,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb,sebb,21/Jan/11 21:12,23/Mar/11 20:36,20/Mar/20 20:33,24/Jan/11 12:49,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,"FastMath min and max fail with (Infinity,-Infinity) and (0,0, -0.0):

min(float 0.0, float -0.0) expected -0.0 actual 0.0
min(float Infinity, float -Infinity) expected -Infinity actual NaN
max(float 0.0, float -0.0) expected 0.0 actual -0.0
max(float Infinity, float -Infinity) expected Infinity actual NaN

Similarly for the double versions.

The Infinity failures are because the code uses Float.isNaN(a + b) which gives NaN when +/1- Infinity are added together.",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-03-23 20:36:27.272,,,false,,,,,,,,,,,,,,,,,150613,,,Wed Mar 23 20:36:27 UTC 2011,,,,,,,"0|i0ru67:",160532,,,,,,,,,,,,,,,,"24/Jan/11 12:49;sebb;Fixed by using the Harmony code.

Note: this appears to be at least as quick as StrictMath on Sun Java 1.6 in a crude test","23/Mar/11 20:36;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath acos fails when input abs value is less than about 5.7851920321187236E-300 - returns NaN,MATH-489,12496028,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb,sebb,19/Jan/11 17:11,23/Mar/11 20:35,20/Mar/20 20:33,21/Jan/11 03:33,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,"FastMath acos fails when input absolute value is less than about 5.7851920321187236E-300

It returns NaN instead of an expected value close to PI/2.0

This appears to be due to the following code:

{code}
// Compute ratio r = y/x
double r = y/x;
temp = r * 1073741824.0;
{code}

r and temp can become infinite or Nan.
",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-03-23 20:35:36.96,,,false,,,,,,,,,,,,,,,,,150610,,,Wed Mar 23 20:35:36 UTC 2011,,,,,,,"0|i0ru73:",160536,,,,,,,,,,,,,,,,"21/Jan/11 03:33;sebb;URL: http://svn.apache.org/viewvc?rev=1061608&view=rev
Log:
Fix overflows in acos calculation

URL: http://svn.apache.org/viewvc?rev=1061609&view=rev
Log:
Fix overflows in acos calculation","23/Mar/11 20:35;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath toRadian and toDegree don't handle large double numbers well,MATH-486,12495960,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb,sebb,19/Jan/11 04:46,23/Mar/11 20:35,20/Mar/20 20:33,19/Jan/11 20:28,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,"FastMath toRadian and toDegree don't handle very large double numbers well.

For example, toDegrees(Double.MAX_VALUE) => NaN, but it should be INFINITY
and toRadian(Double.MAX_VALUE) => NaN instead of the proper value

This is because of the lines:

{code}
double temp = x * 1073741824.0; // == 0x40 00 00 00
double xa = x + temp - temp; // => NaN for x large enough
{code}

This seems to be an attempt to split x into a large and a small part, but fails when x >= MAX_VALUE / 1073741824.0

Not sure how to fix this",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-01-19 07:32:32.379,,,false,,,,,,,,,,,,,,,,,150609,,,Wed Mar 23 20:35:20 UTC 2011,,,,,,,"0|i0ru7r:",160539,,,,,,,,,,,,,,,,"19/Jan/11 07:32;dimpbx;What about the reciprocity of these two methods?  toDegrees(toRadian(x))==x and toRadian(toDegrees(x))==x
If you use INFINITY, you run into troubles.  If you return Double.MAX_VALUE instead, you look safer.   That does not prevent you returning INFINITY if you start with INFINITY and NaN if it is the argument of the method.

Furthermore, some consistency would help.  Why plural for degrees and singular for radian?","19/Jan/11 12:34;sebb;I wrote that toDegrees(Double.MAX_VALUE) should be INFINITY because that is how the Sun/Oracle Math.toDegrees() method behaves.

[Sorry, my bad - the methods are both plural in the code - only the JIRA is inconsistent]
","19/Jan/11 20:28;sebb;Fixed by setting temp =0 for very large input.","23/Mar/11 20:35;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath does not handle all special cases correctly,MATH-483,12495841,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb,sebb,18/Jan/11 00:24,23/Mar/11 20:34,20/Mar/20 20:33,19/Jan/11 19:51,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,"FastMath has some issues with special cases such as +0.0 and -0.0.

Here are the double cases so far found:

abs(-0.0) expected:<0.0> but was:<-0.0>
signum(-0.0) expected:<-0.0> but was:<0.0>
asin(-0.0) expected:<-0.0> but was:<0.0>
atan(-0.0) expected:<-0.0> but was:<0.0>
log10(-0.0) expected:<-Infinity> but was:<NaN>
toDegrees(-0.0) expected:<-0.0> but was:<0.0>
toRadians(-0.0) expected:<-0.0> but was:<0.0>
ulp(-Infinity) expected:<Infinity> but was:<NaN>

And float cases:
abs(-0.0) expected:<0.0> but was:<-0.0>
",,,,,,,,,,,,,,"19/Jan/11 12:36;sebb;MATH-483-generic.patch;https://issues.apache.org/jira/secure/attachment/12468750/MATH-483-generic.patch","18/Jan/11 00:25;sebb;MATH-483.patch;https://issues.apache.org/jira/secure/attachment/12468612/MATH-483.patch",,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-03-23 20:34:46.943,,,false,,,,,,,,,,,,,,,,,150608,,,Wed Mar 23 20:34:46 UTC 2011,,,,,,,"0|i0ru8f:",160542,,,,,,,,,,,,,,,,"18/Jan/11 00:25;sebb;Fix + test cases","18/Jan/11 01:27;sebb;Generic test methods using reflection to compare Math methods with FastMath methods for float and double special cases.

Replaces testHyperbolicSpecialCases() and testTrigSpecialCases() in the previous patch","19/Jan/11 12:36;sebb;Fixed bug - duplicate MAX_VALUE should have MIN_VALUE","19/Jan/11 19:51;sebb;There are still some issues with toDegrees() and toRadians() but these are covered by MATH-486","23/Mar/11 20:34;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,
"FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f",MATH-482,12495818,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,sebb,sebb,sebb,17/Jan/11 19:52,23/Mar/11 20:34,20/Mar/20 20:33,17/Jan/11 20:01,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,"FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f.

This is because the wrong variable is returned.

The bug was not detected by the test case ""testMinMaxFloat()"" because that has a bug too - it tests doubles, not floats.",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-03-23 20:34:29.616,,,false,,,,,,,,,,,,,,,,,150607,,,Wed Mar 23 20:34:29 UTC 2011,,,,,,,"0|i0ru8n:",160543,,,,,,,,,,,,,,,,"23/Mar/11 20:34;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""ulp"" in ""FastMath""",MATH-480,12495639,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,erans,erans,14/Jan/11 23:51,23/Mar/11 20:33,20/Mar/20 20:33,19/Jan/11 19:18,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,"When the argument is infinite, method ""ulp"" in ""FastMath"" produces ""NaN"" (whereas ""Math"" gives ""Infinity"").
",,,,,,,,,,,,,,"17/Jan/11 19:38;sebb;MATH-480.patch;https://issues.apache.org/jira/secure/attachment/12468591/MATH-480.patch",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-01-17 19:38:10.902,,,false,,,,,,,,,,,,,,,,,150605,,,Wed Mar 23 20:33:23 UTC 2011,,,,,,,"0|i0ru93:",160545,,,,,,,,,,,,,,,,"17/Jan/11 19:38;sebb;Fix for ulp(double) bug.
Also adds ulp(float) method and test cases for both","23/Mar/11 20:33;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FastMath.signum(-0.0) does not agree with Math.signum(-0.0) ; no tests for signum",MATH-479,12495631,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb,sebb,14/Jan/11 22:46,23/Mar/11 20:33,20/Mar/20 20:33,19/Jan/11 19:06,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,"There are no unit tests for FastMath.signum(double) as yet.

Here is one that should work, but fails:

{code}
@Test
public void testSignum() {
    Assert.assertTrue(Double.valueOf(FastMath.signum(+0.0)).equals(Double.valueOf(Math.signum(+0.0)))); // OK
    Assert.assertTrue(Double.valueOf(FastMath.signum(-0.0)).equals(Double.valueOf(Math.signum(-0.0)))); // FAILS
}
{code}

",,,,,,,,,,,,,,"15/Jan/11 01:32;sebb;Math479.patch;https://issues.apache.org/jira/secure/attachment/12468435/Math479.patch",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-03-23 20:33:07.718,,,false,,,,,,,,,,,,,,,,,150604,,,Wed Mar 23 20:33:07 UTC 2011,,,,,,,"0|i0ru9b:",160546,,,,,,,,,,,,,,,,"15/Jan/11 01:32;sebb;Patch to add test case against Math.signum(double) and fix for bug (also simplifies the code)","23/Mar/11 20:33;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath needs ulp(float),MATH-472,12495439,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb,sebb,13/Jan/11 00:03,23/Mar/11 20:31,20/Mar/20 20:33,19/Jan/11 19:20,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,"FastMath needs ulp(float), because ulp((double)float) does not generate the correct results.

Test case and patch to follow.",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-03-23 20:31:44.374,,,false,,,,,,,,,,,,,,,,,150600,,,Wed Mar 23 20:31:44 UTC 2011,,,,,,,"0|i0rua7:",160550,,,,,,,,,,,,,,,,"13/Jan/11 00:05;sebb;Similarly, there needs to be a nextAfter(float, float)","17/Jan/11 19:39;sebb;See MATH-480 for patch to add ulp(float)","19/Jan/11 19:20;sebb;Fixed as part of MATH-478","23/Mar/11 20:31;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,
HypergeometricDistribution logProbability() returns NaN for edge cases,MATH-1384,13002817,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Duplicate,,armanbilge,armanbilge,06/Sep/16 02:26,06/Sep/16 06:48,20/Mar/20 20:33,06/Sep/16 06:48,3.0,4.0,,,,,,4.0,,,0,,,,,,"For certain edge cases, HypergeometricDistribution.logProbability() will return NaN.

To compute the hypergeometric log probability, three binomial log probabilities are computed and then combined accordingly. The implementation is essentially the same as in BinomialDistribution.logProbability() and uses the SaddlePointExpansion. However, the Binomial implementation includes an extra check for the edge case of 0 trials which the HyperGeometric lacks.

An example call which fails is:
new HypergeometricDistribution(null, 11, 0, 1).logProbability(0)
which returns NaN instead of 0.0.
Note that
new HypergeometricDistribution(null, 10, 0, 1).logProbability(0)
returns 0 as expected.

Possible fixes:
1. Check for the edge cases and return appropriate values. This would make the code somewhat more complex.
2. Instead of duplicating the implementation use BinomialDistribution.logProbability(). This is much simpler/more readable but will reduce performance as each call to BinomialDistribution.logProbability() makes redundant checks of validity of input parameters etc.

I am happy to submit a PR at the GitHub repo implementing either 1 or 2 with the necessary tests.",,,,,,,,,,,MATH-1356,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-06 06:24:20.259,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 06 06:47:40 UTC 2016,,,,,,,"0|i339jr:",9223372036854775807,,,,,,,,,,,,,,,,"06/Sep/16 06:24;erans;This seems to be the same issue as MATH-1356.
","06/Sep/16 06:31;armanbilge;True; sorry for the duplicate. Where is the commit with the fix?","06/Sep/16 06:47;armanbilge;Found it; thanks.
0880a21c56cec1a2442b5123c3845bfc99e83a7f",,,,,,,,,,,,,,,,,,,,,,,,,,,,
NonLinear Optimizers seem to have a hard time hitting NIST standards,MATH-678,12525387,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Incomplete,,gsteri1,gsteri1,01/Oct/11 01:59,16/Feb/15 23:18,20/Mar/20 20:33,16/Feb/15 23:18,3.0,,,,,,,,,,0,NIST,NonLinear,Optimization,,,"As per a discussion on the mailing list, I am opening this ticket. In applying the nonlinear optimizers in commons, I noticed what I believe to be instability in the techniques. Further investigation investigation (both of my tests) and the code in prod is warranted. 

I will be pushing a first set of tests which should illustrate what I am seeing. 

 ",Java ,,,,,,,,,MATH-763,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-07-17 15:00:13.495,,,false,,,,,,,,,,,,,,,,,41752,,,Mon Feb 16 23:18:01 UTC 2015,,,,,,,"0|i0290n:",11062,,,,,,,,,,,,,,,,"17/Jul/12 15:00;erans;IMHO, this issue is too vague to be useful.
The unit tests referred to have been removed from the CM test suite (cf. MATH-763) because they were all failing (although such big problems were not revealed by the other unit tests). Nobody seems eager to determine whether there could be a problem in the test methodology (e.g. too stringent tolerances) or in some way clean them up so that the potential problems can be sorted out.

I would thus suggest to resolve this issue as ""Won't fix"".
","22/Sep/12 18:34;erans;Greg Sterijevski does not seem to be around this forum anymore.
","16/Feb/15 23:18;tn;Closing as the mentioned instabilities have never been documented.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent multi-start randomization (optimizers),MATH-914,12623747,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,erans,erans,13/Dec/12 13:28,07/Apr/13 09:17,20/Mar/20 20:33,09/Mar/13 17:38,3.0,3.1,,,,,,3.2,,,0,,,,,,"In class ""o.a.c.m.optim.BaseMultiStartMultivariateOptimizer"", the ""starting points"" generator is passed at construction. But random initial guesses must fulfill the bound constraint and be somehow related to the user-supplied initial guess; and those are passed to the ""optimize"" method and thus can change from one call to the other, leading to inconsistent (and probably useless) multi-starts.
",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-03-09 17:38:35.575,,,false,,,,,,,,,,,,,,,,,297446,,,Sun Apr 07 09:17:22 UTC 2013,,,,,,,"0|i14oqn:",235473,,,,,,,,,,,,,,,,"13/Dec/12 13:33;erans;In revision 1421287, I've marked (with ""XXX"") the code that should be revised.
I don't think that it is a blocking issue (it is also present in the ""optimization"" package), but anyone is welcome to provide a patch if a fix is quickly found.
","09/Mar/13 17:38;luc;Fixed in subversion repository as of r1454746.","07/Apr/13 09:17;luc;Closing issue as version 3.2 has been released on 2013-04-06.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
BrentSolver : function value accuracy convergence criterion,MATH-896,12615727,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Not A Problem,,vruch,vruch,12/Nov/12 13:48,04/Mar/13 18:58,20/Mar/20 20:33,12/Nov/12 15:24,3.0,,,,,,,,,,0,,,,,,"Hi everybody,

In the current implementation of the Brent solver, it seems that attribute ""functionValueAccuracy"" is only used in the ""doSolve"" method, before entering the iteration loop in ""brent"" method. 

Shouldn't it be a convergence test on the current function value to ensure that function value accuracy is reached? 

This is the first time I'm posting here, so maybe I have missed something. Sorry if it's the case.

Thanks for your time,

Vincent",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-11-12 14:48:06.464,,,false,,,,,,,,,,,,,,,,,257035,,,Mon Nov 12 15:24:00 UTC 2012,,,,,,,"0|i0j1l3:",109187,,,,,,,,,,,,,,,,"12/Nov/12 14:48;luc;Yes, this value is used only for an initial check in this algorithm.

The implementation of the loop corresponds to the original Brent algorithm, not to something specific created for the Apache Commons Math library. Our implementation of a classical algorithm should be as close as possible to the reference. Any departure from it would prevent comparison with other implementations and would only confuse users.

The value is inherited from an higher level base class and other implementations with different algorithms exist, which do use this parameter in the convergence loop. Examples are the secant based solvers and also BracketingNthOrderBrentSolver.

For most purposes, BracketingNthOrderBrentSolver is the recommended solver, it is often the most efficient one (typically with a setting for order 5) and has interesting features (bracketing).

Also note that the JIRA issue tracker is not the place for questions, it is a place for bug reports and features requests. Usage questions should be asked on the user list (see [http://commons.apache.org/mail-lists.html]). So if you agree, I will close this report as NOTAPROBLEM.","12/Nov/12 15:13;vruch;Thanks for your answer. 
And this was obviously not the right place to post, so of course this report can be closed as NOTAPROBLEM.","12/Nov/12 15:24;luc;As explained in the first comments, this behaviour is the expected one for the Brent algorithm.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
multiplication of infinity,MATH-620,12514033,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Won't Fix,,aploese,aploese,14/Jul/11 08:58,04/Mar/13 18:57,20/Mar/20 20:33,02/Oct/11 21:16,3.0,,,,,,,,,,0,,,,,,"Take the following testcase 
{code}
Assert.assertEquals(neginf, inf* neginf, Double.MIN_VALUE);  // <--Passes ordinary double
Assert.assertEquals(new Complex(neginf, 0), new Complex(inf, 0).multiply(new Complex(neginf, 0)));// <-- Fail only real parts no imaginary parts 
{code}

The outcome of multiply is Complex.INF if one part is infinity.
why not simply compute the multiplication and thats is?
",,,,,,,,,,,,,,"06/Sep/11 14:06;aploese;ComplexOctaveTest.java;https://issues.apache.org/jira/secure/attachment/12493151/ComplexOctaveTest.java","21/Sep/11 19:08;aploese;arne_tests.zip;https://issues.apache.org/jira/secure/attachment/12495425/arne_tests.zip","10/Sep/11 00:30;erans;arne_tests.zip;https://issues.apache.org/jira/secure/attachment/12493876/arne_tests.zip",,,,3.0,,,,,,,,,,,,,,,,,,,,2011-08-10 06:42:14.014,,,false,,,,,,,,,,,,,,,,,2207,,,Sun Oct 02 21:16:38 UTC 2011,,,,,,,"0|i0rtjb:",160429,,,,,,,,,,,,,,,,"14/Jul/11 09:08;aploese;This goes also wrong:
{code}
        Assert.assertEquals(new Complex(neginf, nan), new Complex(0,
inf).multiply(new Complex(0, inf)));// <-- Fail
{code}
the result is the output of octave - so I would expect the same.


Am Donnerstag, den 14.07.2011, 08:58 +0000 schrieb Arne Plöse (JIRA): 


","10/Aug/11 06:42;psteitz;I recommend WONT_FIX.  Looks to me like the behavior matches the javadoc and the current contracts are reasonable, IMO.  ","10/Aug/11 08:07;aploese;evaluate this testcase:
{code}
    @Test
    public void testMultiplyInf() {
        Complex z = new Complex(1, neginf);
        Complex w = z.multiply(z);
        Assert.assertEquals(w.getReal(), neginf, 0);
        Assert.assertEquals(w.getImaginary(), neginf, 0);
        Assert.assertEquals(w, z.divide(Complex.ONE.divide(z)));
    }
{code}
the result should be -inf -infi but actually is inf + infi ...
The division looks also broken...","02/Sep/11 22:24;erans;These
{quote}
Assert.assertEquals(w.getReal(), neginf, 0);
Assert.assertEquals(w.getImaginary(), neginf, 0);
{quote}
behave as intended: all infinities are mapped to a single ""point at infinity"" (chosen to be ""INF"" a.k.a. (+inf, +inf)), which IIRC is a way to deal with the infinite number of infinities along all the directions in the complex plane.

This one
{quote}
Assert.assertEquals(w, z.divide(Complex.ONE.divide(z)));
{quote}
looks wrong indeed.
","02/Sep/11 22:39;erans;I've created a new ticket for the above: MATH-657","05/Sep/11 10:36;erans;It would be helpful that you construct a complete unit test that compares Commons Math with Octave.

Also, there is a discussion, on the ""dev"" ML (cf. thread with subject ""Complex division""), about how operations in ""Complex"" should behave. Mentioning your problems and requirements might contribute to deciding which way to go.
","05/Sep/11 14:21;erans;Another bug (IMO) in CM:
{code}
Complex infInf = new Complex(Double.POSITIVE_INFINITY, Double.POSITIVE_INFINITY);
Assert.assertTrue(infInf.multiply(infInf).isInfinite());
{code}
succeeds, but the formula would have produced ""NaN"" in the real part.
Octave computes:
{noformat}
NaN + Infi
{noformat}
Added unit test (with alternative expectation) in revision 1165296.
","06/Sep/11 14:06;aploese;Here is the comparision between commons math and octave.
I hope this what you want Gilles ;-).","09/Sep/11 23:27;erans;It's on the right track, but the test method stops as soon as it encounters a difference; thus, we cannot have a complete overview of all the differences with Octave.
","10/Sep/11 00:30;erans;See what I mean in the attached file. You could readily apply the same layout to new test classes for ""subtract"", ""multiply"" and ""divide"" (which were in ""test4"" in your file).
","21/Sep/11 19:08;aploese;added test cases for sud/mul/div
fixed sign detection (readable output)
replaced Complex.valueOf with new Double(r, i)","24/Sep/11 10:04;erans;Hi.

# I don't really understand the necessity of ""getSign"". Couldn't you use ""Math.signum"" for the same
purpose?
# It would better to merge the assertions on the signs within the main test because, having them separate forces the operation (add, multiply, ...) to be performed 3 times. Really it is the same test (two results must be equal, sign included).
# I don't understand the statement with ""Complex.valueOf"".

Did you notice the MATH-667 issue?
","26/Sep/11 08:35;aploese;1. Math.signum: octave makes a distinction between +0 and -0, from the javadocs Math.signum does not.
2. No real need (just more verbose if a case failed) you can collapse them in one test case.
3. valueOf return Complex.NAN id a part is NAN (same for INF) ","26/Sep/11 11:40;erans;bq. Math.signum: octave makes a distinction between +0 and -0, from the javadocs Math.signum does not.

This is actually from the {{Math.signum}} Javadoc:
{panel}
[...]
* If the argument is positive zero or negative zero, then the result is the same as the argument.
{panel}
","02/Oct/11 21:16;erans;See MATH-667 for an alternative solution.",,,,,,,,,,,,,,,,
"Complex bug with NaN: the operation new Complex(0.0).multiply(new Complex(Double.POSITIVE_INFINITY)) gives (Infinity, Infinity) instead of NaN",MATH-788,12554203,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Won't Fix,,poslavsky_sv,poslavsky_sv,07/May/12 18:32,04/Mar/13 18:57,20/Mar/20 20:33,22/Oct/12 14:18,3.0,,,,,,,,,,0,patch,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-05-07 20:23:22.389,,,false,,,,,,,,,,,,,,,,,238432,,,Mon Oct 22 14:18:05 UTC 2012,,,,,,,"0|i028vz:",11041,,,,,,,,,,,,,,,,"07/May/12 20:23;erans;Please have a look at the related issues.
We know about those behaviours. Some have called this ""bugs"", others ""inconsistencies"", yet others are satisfied with the current ""conventions"".
The accepted way to get out of this is to implement the proposal described in MATH-667.
Any volunteer?","22/Oct/12 14:18;erans;Older discussions concluded that this won't be fixed in the ""Complex"" class as it currently exists in Commons Math.
We are open to have other implementations (where the result of some operation may be more consistent with other conventions or standards), as outlined in MATH-667.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Precision.EPSILON: wrong documentation,MATH-843,12602075,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,dgruntz,dgruntz,07/Aug/12 19:13,04/Mar/13 18:53,20/Mar/20 20:33,20/Aug/12 07:56,3.0,,,,,,,3.1,,,0,documentation,,,,,"The documentation of the Field {{EPSILON}} in class {{org.apache.commons.math3.util.Precision}} states, that {{EPSILON}} is the smallest positive number such that {{1 - EPSILON}} is not numerically equal to 1, and its value is defined as 1.1102230246251565E-16.

However, this is NOT the smallest positive number with this property.

Consider the following program:
{code}
public class Eps {
  public static void main(String[] args) {
    double e = Double.longBitsToDouble(0x3c90000000000001L);
	double e1 = 1-e;
	System.out.println(e);
	System.out.println(1-e);
	System.out.println(1-e != 1);
  }
}
{code}
The output is:
{code}
% java Eps
5.551115123125784E-17
0.9999999999999999
true
{code}

This proves, that there are smaller positive numbers with the property that 1-eps != 1.

I propose not to change the constant value, but to update the documentation. The value {{Precision.EPSILON}} is 
an upper bound on the relative error which occurs when a real number is
rounded to its nearest Double floating-point number. I propose to update 
the api docs in this sense.",,1800,1800,,0%,1800,1800,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-08-07 21:42:09.3,,,false,,,,,,,,,,,,,,,,,292279,,,Mon Aug 20 07:56:07 UTC 2012,,,,,,,"0|i0rt07:",160343,,,,,,,,,,,,,,,,"07/Aug/12 21:42;erans;Thanks for the report.

I've updated the documentation in revision 1370547. Do you agree with the new version?
","07/Aug/12 22:50;dgruntz;yes, the new documentation in rev 1370547 is correct; 2^(-53) has that property (i.e. it is the largest double eps such that 1+eps = 1):
{code}
scala> math.pow(2, -53)
res1: Double = 1.1102230246251565E-16

scala> 1+res1 == 1
res2: Boolean = true

scala> java.lang.Double.longBitsToDouble(java.lang.Double.doubleToLongBits(res1)+1)
res3: Double = 1.1102230246251568E-16

scala> 1+res3
res4: Double = 1.0000000000000002
{code}

So I agree, but I do not like it. I would not define this constant over 1+eps == 1 (as if executed in extended precision the result may be different), but I would define it as upper bound on the relative error due to rounding real numbers to {{double}} floating-point numbers. ","08/Aug/12 09:49;erans;bq. [...] but I do not like it.

I like it because it is more straightforward to understand while I must admit that I don't fully understand your definition...
I've added a trivial unit test (""testMath841"") that verifies the claim made in the documentation.

Please provide a unit test that makes your point clear, and I'll see no problem adding your proposed comment in the Javadoc.

bq. [...] if executed in extended precision the result may be different [...]

I don't understand: The Javadoc now explicitly states ""double-precision"".
Let me know whether it can be made clearer.
","08/Aug/12 16:16;dgruntz;{quote}
I don't understand: The Javadoc now explicitly states ""double-precision"".
Let me know whether it can be made clearer.
{quote}
The computation may be done in the registers of your processor and these registers may provide more accuracy (typically 80bits). In Java this extended precision can be prevented by using the {{strictfp}} keyword.

The same holds for your test. It should be marked with {{strictfp}} as well. On some machines, the expression {{1 + Precision.EPSILON}} in the test {{1 + Precision.EPSILON == 1}} might be hold in a register with extended precision and thus may be greater than {{1}}. The expression then returns false.

Regarding the other definition: This is the one which can be found in [Wikipedia|http://en.wikipedia.org/wiki/Machine_epsilon] and in the [Computing Surveys article by Goldberg|http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html]. The machine epsilon is defined as upper bound of the relative error which may occur when a real number is rounded to its closest floating-point approximation. This upper bound is {{B^(-(p-1))/2}} where {{B}} is the base and {{p}} is the precision or length of the mantissa. For IEEE754 64-bit numbers (e.g. for {{double}}) this value is {{2^(-53)}}.","09/Aug/12 12:14;erans;bq. On some machines, the expression 1 + Precision.EPSILON in the test 1 + Precision.EPSILON == 1 might be hold in a register with extended precision and thus may be greater than 1. The expression then returns false.

Did you actually see that?
I would (maybe naively) think that ""1 + ESPILON"" would be cast to a 64-bit double before the comparison. If so, wouldn't the equality still hold?

bq. Regarding the other definition: [...]

No problem to add this comment if you provide it here in a ""noformat"" block with all the references as HTML links (""<a href=""..."">...</a>"") which I can copy/paste into the code.
Thanks.
","17/Aug/12 18:50;celestin;Dominik,
could you please review {{r1374395}} before we resolve this issue?
Thanks","20/Aug/12 07:51;dgruntz;looks good to me, thanks. Or is there another place for reviewing?
Dominik
","20/Aug/12 07:56;celestin;Thanks, Dominik, that will do!
I'm resolving this issue right now.",,,,,,,,,,,,,,,,,,,,,,,
A random crash of MersenneTwister random generator,MATH-899,12616365,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,darksnake,darksnake,16/Nov/12 08:08,04/Mar/13 18:53,20/Mar/20 20:33,18/Nov/12 21:40,3.0,,,,,,,3.1,,,0,,,,,,"There is a very small probability that MersenneTwister generator gives a following error: 
java.lang.ArrayIndexOutOfBoundsException: 624
in MersenneTwister.java line 253
The error is completely random and its probability is about 1e-8.

UPD: The problem most probably arises only in multy-thread mode.","Windows 7, JDK 1.7.05",,,,,,,,,,,,,"18/Nov/12 01:07;erans;SynchronizedRandomGenerator.java;https://issues.apache.org/jira/secure/attachment/12553933/SynchronizedRandomGenerator.java","18/Nov/12 01:07;erans;SynchronizedRandomGeneratorTest.java;https://issues.apache.org/jira/secure/attachment/12553934/SynchronizedRandomGeneratorTest.java",,,,,2.0,,,,,,,,,,,,,,,,,,,,2012-11-16 08:45:01.418,,,false,,,,,,,,,,,,,,,,,258143,,,Sun Nov 18 22:50:42 UTC 2012,,,,,,,"0|i0kn9r:",118587,,,,,,,,,,,,,,,,"16/Nov/12 08:45;tn;Hi,

Looking at the code, I do not understand how this can happen:

{noformat}
        if (mti >= N) { // generate N words at one time
            ...

            mti = 0;
        }

        y = mt[mti++];
{noformat}

So first we check if mti is gte to N, reset it in this case, and then use the mti value and increment afterwards.
Could it be that you use the MersenneTwister in a multi-threading environment, where the nextXXX() methods are called in a concurrent manner?","16/Nov/12 08:55;luc;Doing exactly the same analysis and doing somes tests (still running), I came to the same conclusion.

The mti index is not random at all (it is an index running through the pool and wrapping at the end). Only the content of the pool is random.

The MersenneTwister class is not thread safe, so I also guess there is a multi-threading issue there.","16/Nov/12 09:51;darksnake;Yes, i was running some tests in multi-thread mode. So I guess the problem is in concurrency. In this case it should be marked as non thread safe in the Docs.","16/Nov/12 12:07;erans;I propose to add the ""volatile"" keyword for variable ""mti"".
I also think that the ""synchronized"" keyword must be added to the ""setSeed"" methods.

Alexander,
Would you be willing to stress-test the codes before and after this modification to ensure to it makes the class thread-safe?
","16/Nov/12 12:19;luc;Adding synchronized to setSeed is not sufficient.
The part of the next(int bytes) method that uses mti (i.e. the part shown in Thomas comment above) should also be synchronized.
","16/Nov/12 12:23;tn;Indeed, I think it is a more general problem, as many of the RNGs are not thread-safe (if not all of them).
It would be nice to have annotations for thread-safe code, so a user would not have to dig into code to be sure.

For now we could also document in the class javadoc whether an implementation is thread-safe or not (like we did in commons-codec).","16/Nov/12 12:47;darksnake;I always can insert new version of the generator in the test program where I have found the problem in the first place. Still I don't know what conditions are required to error for occur. For now I'm using a JDKRandomGenerator and it works just fine so far.","16/Nov/12 13:24;erans;I'm no expert in thread-safety but ...

bq. The part of the next(int bytes) method that uses mti [...]

... I don't think so; or more exactly, it can be that synchronization of the whole method is more than strictly necessary, or that it should be refactored into blocks of statements that must be synchronized and blocks that don't need to.
I think that it is one of the important collateral issues about making a code thread-safe: not overdoing it to avoid too much efficiency loss.

bq. For now we could also document in the class javadoc whether an implementation is thread-safe or not

To do that is a huge effort. For now, users must assume that no class (except immutable ones) is thread-safe.
","16/Nov/12 13:35;tn;Regarding sync (see embedded code from next(int) method).

This part has to be sync'ed, as it not only relies on the internal variable mti, it also modifies the internal array mt:

{noformat}
        if (mti >= N) { // generate N words at one time
            int mtNext = mt[0];
            for (int k = 0; k < N - M; ++k) {
                int mtCurr = mtNext;
                mtNext = mt[k + 1];
                y = (mtCurr & 0x80000000) | (mtNext & 0x7fffffff);
                mt[k] = mt[k + M] ^ (y >>> 1) ^ MAG01[y & 0x1];
            }
            for (int k = N - M; k < N - 1; ++k) {
                int mtCurr = mtNext;
                mtNext = mt[k + 1];
                y = (mtCurr & 0x80000000) | (mtNext & 0x7fffffff);
                mt[k] = mt[k + (M - N)] ^ (y >>> 1) ^ MAG01[y & 0x1];
            }
            y = (mtNext & 0x80000000) | (mt[0] & 0x7fffffff);
            mt[N - 1] = mt[M - 1] ^ (y >>> 1) ^ MAG01[y & 0x1];

            mti = 0;
        }

        y = mt[mti++];
{noformat}

This part does not need to be sync'ed:

{noformat}
        // tempering
        y ^=  y >>> 11;
        y ^= (y <<   7) & 0x9d2c5680;
        y ^= (y <<  15) & 0xefc60000;
        y ^=  y >>> 18;

        return y >>> (32 - bits);
{noformat}

So, have a more fine-grained sync would not be of much benefit imho. Maybe adding a sync'ed wrapper of RandomGenerator makes sense, which takes another RandomGenerator as input?

{quote}
To do that is a huge effort. For now, users must assume that no class (except immutable ones) is thread-safe.
{quote}

for all of CM indeed, for just the RNGs this would be feasible. I think the problem here also comes from the fact that the JDK RNG is thread-safe, so people may tend to assume the same is true for CM RNGs.","16/Nov/12 15:11;erans;bq. a more fine-grained sync would not be of much benefit

Maybe, maybe not.
Another approach (and preferrable in my non-expert opinion) is to use the JDK's [atomic utilities|http://docs.oracle.com/javase/1.5.0/docs/api/java/util/concurrent/atomic/package-summary.html].

bq. for just the RNGs this would be feasible

I agree, modulo the above remark.
","16/Nov/12 15:40;erans;Referring to the quoted code block.

Only the block that will be executed if the condition is true must synchronized (assuming that ""mti"" is to be ""volatile"").
{code}
if (mti >= N) {
 updateMtArray();
 mti = 0;
}
{code}
where ""updateMtArray()"" would be ""synchronized"".
We thus have a finer-grained thread-safety (locking occurs every N calls).
","16/Nov/12 15:46;tn;The moment you did the check ""if (mti >= N)"" another thread could already have updated mti, invalidating your program flow.","16/Nov/12 16:33;erans;Oh, yes. :(

Another related issue is that this implementation is a translation from the original algorithm.
Can we make it thread-safe and still make that claim?

To make it thread-safe but not too inefficient, can we depart from the standard implementation?

How much inefficiency is acceptable from such a utility? If efficiency is not critical, your suggestion of a wrapper is possibly the right answer.
","16/Nov/12 18:00;luc;bq. Can we make it thread-safe and still make that claim?

Yes, just do what was said a few comments above: synchronize the block identified above.
","16/Nov/12 19:15;psteitz;I am -1 on attempting to make this class threadsafe.  Each generating thread should have its own generator.  If users really need a shared singleton generator, they should do as Gilles suggests, which is to wrap and then only they will pay the sync overhead cost.  ","17/Nov/12 00:46;erans;Actually, it was Thomas's suggestion that we could provide an implementation of ""RandomGenerator"" that, IIUC, would wrap another instance and override all the methods with the ""synchronized"" keyword.
{code}
/**
 * Any ""RandomGenerator"" can be thread-safe if it is used through
 * an instance of this class.
 */
public class SynchronizedRandomGenerator {
  private final RandomGenerator wrapped;

  /**
   * @param rng Generator whose methods will be called through
   * their corresponding overridden synchronized version.
   * To ensure thread-safety, the wrapped generator <em>must</em>
   * not be used directly.
   */
  public SynchronizedRandomGenerator(RandomGenerator rng) {
    wrapped = rng;
  }

  public synchronized void setSeed(int seed) {
    wrapped.setSeed(seed);
  }

  // [Similarly for all methods.]
}
{code}

Any caveat with that solution?
","17/Nov/12 01:13;psteitz;Sounds reasonable to me.","18/Nov/12 01:04;erans;I've implemented that class.
But I have a hard time implementing a unit test that would consistently reproduce the error reported here: It does not necessarily happen even with a fairly large number of threads.

The unit test with the synchronized wrapper passes, but it would have been more convincing to have both, showing that the wrapper indeed solves the problem.
","18/Nov/12 01:07;erans;Here are the Java files.
Ideas to improve the unit tests welcome.
","18/Nov/12 07:12;darksnake;The synchronized wrapper works fine. I can not guarantee that it does not produce error at all because I made only finite number of runs, but no errors so far.","18/Nov/12 10:35;tn;{quote}
But I have a hard time implementing a unit test that would consistently reproduce the error reported here: It does not necessarily happen even with a fairly large number of threads.
{quote}

hmm, I am not sure if this is necessary or achievable. When looking at comparable things (e.g. commons-collections, or openjdk), there also do not exist specific tests to prove that the synchronization prevents a race condition in not thread-safe code.

I think in this case it would be sufficient to do a proof by induction (sort of):

 * we know that e.g. MersenneTwister is not thread-safe
 * by providing a fully synchronized wrapper we make it thread-safe

btw. in the test you refer to MATH-900 while its actually MATH-899.","18/Nov/12 17:44;erans;bq. it would be sufficient to do a proof by induction (sort of): [...]

Well, this could be said for any piece of code: we must have property <x>, so we write code that implements <x>. ;)
We write unit tests that actually show that the code behaves as expected.

After spending quite some time on the attached code, I of course agree that it is complicated in some situations. In this case, the code is simple enough as to not necessitates a unit test.
But the issue will crop up again if we start introducing more ""complicated"" code (like the utilities in the ""java.util.concurrent"" package).

So do I commit just the class and no unit test, or just the unit test of the synchronizing wrapper?
","18/Nov/12 18:35;tn;You could and should provide a test that shows that the wrapper provides the same result as a wrapped RandomGenerator (using the same seed), imho.","18/Nov/12 19:38;psteitz;I think the reason that the unit test does not work is that Junit does not actually see the exception.  The Executor will not propagate it.  Changing the body of the core method (and other sigs, etc) to the following, I can get consistent failures when I execute with -Dtest=SynchronizedRandomGenratorTest

{code}
final RandomGenerator rng = new MersenneTwister();
final RandomGenerator wrapper = sync ? new SynchronizedRandomGenerator(rng) : rng;
final AtomicBoolean failed = new AtomicBoolean(false);
final ExecutorService exec = Executors.newFixedThreadPool(numThreads);
for (int i = 0; i < numGenerators; i++) {
    exec.execute(new Runnable() {
        public void run() {
            for (int j = 0; j < numSamples; j++) {
                try {
                    wrapper.nextGaussian();   
                } catch (ArrayIndexOutOfBoundsException ex) {
                    failed.getAndSet(true);
                    break;
                }
             }
         }
         });
        }
        exec.shutdown();
        exec.awaitTermination(100, TimeUnit.SECONDS);
        Assert.assertTrue(failed.get());
{code}

What I don't understand is why this consistently succeeds when executed as a single test, but usually fails when executed as part of the full test suite with
{code}
 mvn clean test
{code}

I would say in any case there is no need to add a unit test to show the non-thread-safety of MersenneTwister or to verify that adding synchronization does what it says it does, i.e., I would say go ahead and commit the wrapper with no test class.","18/Nov/12 20:54;erans;bq. I think the reason that the unit test does not work is that Junit does not actually see the exception.

That's not the problem. The test code works correctly and failures _are_ reported (and seen by Junit). Just they do not occur consistently, as must be expected, I assume, because the execution itself becomes random (subject to the threads scheduling by the system).

bq. commit the wrapper with no test class.

I'll do that then.

bq. You could and should provide a test that shows that the wrapper provides the same result as a wrapped RandomGenerator (using the same seed)

I don't understand.
","18/Nov/12 21:01;erans;Wrapper committed in revision 1410990.","18/Nov/12 21:40;erans;""MersenneTwister"" not being thread-safe, if an instance is to be accessed from multiple threads, it must be through the wrapper proposed in this discussion.
","18/Nov/12 22:07;tn;thanks!

{quote}
I don't understand.
{quote}

Nevermind, I just had something like this in mind:

{noformat}
  RandomGenerator orig = new MersenneTwister(1234l);
  RandomGenerator wrapper = new SynchronizedRandomGenerator(new MersenneTwister(1234l));

  for (i = 0; i < 100; i++) {
     assertEquals(orig.nextGaussian(), wrapper.nextGaussian());
  }
  ... // similar for the other methods
{noformat}","18/Nov/12 22:50;erans;bq. I just had something like this in mind [...]

I'm ashamed to admit that you are perfectly right... Cf. revision 1411009.
",,
Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets,MATH-790,12556531,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,mikl,james_pic,james_pic,19/May/12 17:01,04/Mar/13 18:53,20/Mar/20 20:33,12/Jun/12 14:28,3.0,,,,,,,3.1,,,1,newbie,patch,,,,"When performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations.

Attached is a patch, including a test, and a fix, which modifies the affected code to use doubles","Ubuntu Linux x64, Sun Java 6",3600,3600,,0%,3600,3600,,,,,,,"19/May/12 17:02;james_pic;MannWhitnetUOVerflowPatch.diff;https://issues.apache.org/jira/secure/attachment/12528263/MannWhitnetUOVerflowPatch.diff",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-05-19 19:49:05.513,,,false,,,,,,,,,,,,,,,,,292311,,,Tue Jun 12 14:28:08 UTC 2012,,,,,,,"0|i0rt7b:",160375,,,,,,,,,,,,,,,,"19/May/12 17:02;james_pic;A patch, which adds a unit test for the issue, and a fix in the affected code","19/May/12 19:49;mikl;Thank you very much for reporting this. I will have a look at this ASAP.","08/Jun/12 11:04;mikl;This is fixed in SVN revision 1348024. Thanks again for reporting this issue.","12/Jun/12 09:18;mikl;Thomas Neidhart suspect an overflow issues is still present.","12/Jun/12 11:50;tn;As discussed on the ML, there may be still a problem with integer overflow in the code fragment below:

{noformat}
final double n1n2prod = n1 * n2;

// http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U#Normal_approximation
final double EU = n1n2prod / 2.0;
final double VarU = n1n2prod * (n1 + n2 + 1) / 12.0;

final double z = (Umin - EU) / FastMath.sqrt(VarU);
{noformat}

The calculation of n1n2prod may still overflow if n1 and n2 are too big as it still does an int multiplication, so I would suggest to do it like that:

{noformat}
final long n1n2prod = (long) n1 * n2;

// http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U#Normal_approximation
final double EU = n1n2prod / 2.0;
final double VarU = n1n2prod * (n1 + n2 + 1) / 12.0;

final double z = (Umin - EU) / FastMath.sqrt(VarU);
{noformat}
","12/Jun/12 13:58;mikl;Thanks for the details. Why not use a double immediately as below? Is it to avoid precision loss?
{noformat}
final double n1n2prod = (double) n1 * n2;

// http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U#Normal_approximation
final double EU = n1n2prod / 2.0;
final double VarU = n1n2prod * (n1 + n2 + 1) / 12.0;

final double z = (Umin - EU) / FastMath.sqrt(VarU);
{noformat}","12/Jun/12 14:04;tn;Actually yes, this was my intention (and long multiplication should be faster too ;-), but in this case it may be negligible.","12/Jun/12 14:07;mikl;I agree - I just wanted to have it in writing :-). Thanks for this.","12/Jun/12 14:28;mikl;This second issue is fixed in SVN revision 1349372. Thanks again for reporting this issue, too.",,,,,,,,,,,,,,,,,,,,,,
"Fraction(double, int) constructor strange behaviour",MATH-836,12600886,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,snalin,snalin,31/Jul/12 17:04,04/Mar/13 18:53,20/Mar/20 20:33,04/Aug/12 16:27,3.0,,,,,,,3.1,,,0,Fraction,,,,,"The Fraction constructor Fraction(double, int) takes a double value and a int maximal denominator, and approximates a fraction. When the double value is a large, negative number with many digits in the fractional part, and the maximal denominator is a big, positive integer (in the 100'000s), two distinct bugs can manifest:

1: the constructor returns a positive Fraction. Calling Fraction(-33655.1677817278, 371880) returns the fraction 410517235/243036, which both has the wrong sign, and is far away from the absolute value of the given value

2: the constructor does not manage to reduce the Fraction properly. Calling Fraction(-43979.60679604749, 366081) returns the fraction -1651878166/256677, which should have* been reduced to -24654898/3831.

I have, as of yet, not found a solution. The constructor looks like this:

public Fraction(double value, int maxDenominator)
        throws FractionConversionException
    {
       this(value, 0, maxDenominator, 100);
    }

Increasing the 100 value (max iterations) does not fix the problem for all cases. Changing the 0-value (the epsilon, maximum allowed error) to something small does not work either, as this breaks the tests in FractionTest. 

The problem is not neccissarily that the algorithm is unable to approximate a fraction correctly. A solution where a FractionConversionException had been thrown in each of these examples would probably be the best solution if an improvement on the approximation algorithm turns out to be hard to find.

This bug has been found when trying to explore the idea of axiom-based testing (http://bldl.ii.uib.no/testing.html). Attached is a java test class FractionTestByAxiom (junit, goes into org.apache.commons.math3.fraction) which shows these bugs through a simplified approach to this kind of testing, and a text file describing some of the value/maxDenominator combinations which causes one of these failures.

* It is never specified in the documentation that the Fraction class guarantees that completely reduced rational numbers are constructed, but a comment inside the equals method claims that ""since fractions are always in lowest terms, numerators and can be compared directly for equality"", so it seems like this is the intention. ",,,,,,,,,,,,,,"31/Jul/12 17:05;snalin;FractionTestByAxiom.java;https://issues.apache.org/jira/secure/attachment/12538573/FractionTestByAxiom.java","31/Jul/12 17:06;snalin;value-maxDenominator_pairs_that_fails;https://issues.apache.org/jira/secure/attachment/12538574/value-maxDenominator_pairs_that_fails",,,,,2.0,,,,,,,,,,,,,,,,,,,,2012-08-01 21:14:45.678,,,false,,,,,,,,,,,,,,,,,292283,,,Sun Aug 05 20:14:29 UTC 2012,,,,,,,"0|i0rt13:",160347,,,,,,,,,,,,,,,,"31/Jul/12 17:05;snalin;FractionTestByAxiom.java added: Tests the Fraction class through axioms. Not intended to be a part of a finished product, but to explore the class, looking for bugs.","31/Jul/12 17:06;snalin;value-maxDenominator_pairs_that_fails added: pairs of doubles and ints that makes the constructor Fraction(double, int) return faulty Fractions.","01/Aug/12 21:14;tn;The overflow check in Fraction does not take negative values into account, it has to be changed to the following:

{noformat}
  if (FastMath.abs(a0) > overflow) {
      throw new FractionConversionException(value, a0, 1l);
  }

...

  if ((FastMath.abs(p2) > overflow) || (FastMath.abs(q2) > overflow)) {
      throw new FractionConversionException(value, p2, q2);
  }
{noformat}

In that case your examples fail correctly with a FractionConversionException and all unit tests run through successfully.

I would be interested how you generated the test cases. Are these auto-generated?

Thanks,

Thomas","01/Aug/12 21:25;tn;Committed the changes in r1368253.","01/Aug/12 21:52;tn;While looking into this issue, I realized that there is a very similar class in commons-lang, which also does a reduction when creating a Fraction object from a double value. We should consider also doing this for commons-math.","02/Aug/12 09:50;snalin;Thomas - the test cases are not auto-generated, they are hand-written. We're working on a tool - JaxT2 -  that interprets axioms for classes and auto-generates unit tests through reflection, to improve on the old JaxT tool (http://www.ii.uib.no/mouldable/testing/jaxt/index.html).

Good job with the overflow.","04/Aug/12 16:27;tn;Thanks for the info. This work looks really interested. If you need any support in further testing commons-math or any other commons component, do not hesitate to ask.

I resolve this issue now, as it seems to fix your problems.

Thomas","05/Aug/12 20:14;psteitz;JaxT definitely looks very interesting and broadly useful for us.  Looking forward to more applications to commons math and other commons components.  ",,,,,,,,,,,,,,,,,,,,,,,
"""HarmonicFitter.ParameterGuesser"" sometimes fails to return sensible values",MATH-844,12603145,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,erans,erans,12/Aug/12 21:03,04/Mar/13 18:53,20/Mar/20 20:33,16/Aug/12 20:45,3.0,,,,,,,3.1,,,0,,,,,,"The inner class ""ParameterGuesser"" in ""HarmonicFitter"" (package ""o.a.c.m.optimization.fitting"") fails to compute a usable guess for the ""amplitude"" parameter.
",,,,,,,,,,,,,,"12/Aug/12 21:09;erans;MATH-844.test.patch;https://issues.apache.org/jira/secure/attachment/12540577/MATH-844.test.patch",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-08-15 12:02:58.036,,,false,,,,,,,,,,,,,,,,,292278,,,Thu Aug 16 20:45:55 UTC 2012,,,,,,,"0|i0rszz:",160342,,,,,,,,,,,,,,,,"12/Aug/12 21:09;erans;I've attached a unit test demonstrating the problem reporting in [this thread|http://markmail.org/message/42yuec5lk5wr6gbo].
","14/Aug/12 23:03;erans;I propose to check whether the values are sensible, and when they are not, either
# throw an appropriate exception (i.e. signal that the guesser fails), or
# return arbitrary values that would allow the optimizer to proceed.

Any preference?
","15/Aug/12 12:02;luc;In this case, the guesser fails because the function in really far from an harmonic function. It is a triangular periodic function with amplitude +/-3 and period 12, and all sample points are taken as integer abscissa, so values all belong to the integer subset {-3, -2, -1, 0, 1, 2, 3}.

This is an (interesting) ill-conditioned case. As small integers are represented exactly even as primitive doubles, all computations are exact and one of the intermediate parameters (c2 in the source code) is a perfect 0 (there are no approximation here, the result is exact), and we divide by c2.

I would suggest to raise an exception here, and to store this as a junit test for failure mode.","16/Aug/12 20:45;erans;As of revision 1374046, the code will generate an exception.",,,,,,,,,,,,,,,,,,,,,,,,,,,
"""BrentOptimizer"" not always reporting the best point",MATH-855,12605932,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,erans,erans,02/Sep/12 23:52,04/Mar/13 18:53,20/Mar/20 20:33,05/Sep/12 14:23,3.0,,,,,,,3.1,,,0,,,,,,"{{BrentOptimizer}} (package ""o.a.c.m.optimization.univariate"") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.",,,,,,,,,,,,,,"03/Sep/12 00:00;erans;MATH-855.patch;https://issues.apache.org/jira/secure/attachment/12543502/MATH-855.patch",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,292274,,,Sun Sep 09 10:41:46 UTC 2012,,,,,,,"0|i0rsz3:",160338,,,,,,,,,,,,,,,,"03/Sep/12 00:00;erans;Proposed patch.

The (somewhat contrived) unit test fails with the current version of the algorithm.
","03/Sep/12 00:01;erans;OK to apply?
","05/Sep/12 14:23;erans;Committed in revision 1381195.","07/Sep/12 15:45;erans;That was not it yet; further improvement committed in revision 1382070, together with a Javadoc update explaining some change wrt the original version of the algorithm.
","09/Sep/12 10:41;erans;Yet another small change in revision 1382441.
",,,,,,,,,,,,,,,,,,,,,,,,,,
Fraction percentageValue rare overflow,MATH-835,12600843,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,snalin,snalin,31/Jul/12 13:09,04/Mar/13 18:53,20/Mar/20 20:33,31/Jul/12 15:01,3.0,,,,,,,3.1,,,0,Fraction,,,,,"The percentageValue() method of the Fraction class works by first multiplying the Fraction by 100, then converting the Fraction to a double. This causes overflows when the numerator is greater than Integer.MAX_VALUE/100, even when the value of the fraction is far below this value.

The patch changes the method to first convert to a double value, and then multiply this value by 100 - the result should be the same, but with less overflows. An addition to the test for the method that covers this bug is also included.",,,,,,,,,,,,,,"31/Jul/12 13:09;snalin;percentageValueOverflow.patch;https://issues.apache.org/jira/secure/attachment/12538547/percentageValueOverflow.patch",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-07-31 15:01:02.099,,,false,,,,,,,,,,,,,,,,,292284,,,Tue Jul 31 15:01:02 UTC 2012,,,,,,,"0|i0rt1b:",160348,,,,,,,,,,,,,,,,"31/Jul/12 15:01;erans;Fixed, as suggested, in revision 1367593.
Thanks for the report.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Front page 3.0 Javadoc link actually goes to 3.1 snapshot,MATH-912,12623363,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,peterfine,peterfine,11/Dec/12 11:49,11/Dec/12 13:52,20/Mar/20 20:33,11/Dec/12 13:52,3.0,,,,,,,,,,0,documentation,,,,,"On the front page of the math site [http://commons.apache.org/math/], the link on the left titled ""Javadoc (3.0 release)"" actually leads to [http://commons.apache.org/math/apidocs/index.html] with the title ""Commons Math 3.1-SNAPSHOT API"".

It should actually lead to the 3.0 docs. 
",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-12-11 13:52:48.729,,,false,,,,,,,,,,,,,,,,,297060,,,Tue Dec 11 13:52:48 UTC 2012,,,,,,,"0|i14l3r:",234882,,,,,,,,,,,,,,,,"11/Dec/12 13:52;erans;I added the missing ""api-3.0"" directory.
Thanks for the report.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
math3 SecantSolver can return Double.INFINITE,MATH-871,12609396,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Cannot Reproduce,,anthonymaidment,anthonymaidment,27/Sep/12 14:19,15/Oct/12 13:23,20/Mar/20 20:33,15/Oct/12 13:23,3.0,,,,,,,,,,0,,,,,,"In SecantSolver.doSolve(), I had a situation in which f0 and f1, from computeObjectiveValue() on lines 77 & 78, were the same value.

Then when it calculates the next appromixation at line 101:
final double x = x1 - ((f1 * (x1 - x0)) / (f1 - f0));
The denominator is then zero, and the next approximation is Double.INFINITE.

I was able to work around this in this particular instance by relaxing the accuracy requirements of the solver, although I haven't yet fully tested the downstream implications of this change.",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-09-28 11:23:44.534,,,false,,,,,,,,,,,,,,,,,241564,,,Mon Oct 15 13:23:27 UTC 2012,,,,,,,"0|i028of:",11007,,,,,,,,,,,,,,,,"28/Sep/12 11:23;erans;Thanks for the report. Could you provide a unit test showing the failure?

At first sight, this seems the result of a inherent weakness of the algorithm, not a bug in the implementation.
With your particular use-case (i.e. a code excerpt), it will be useful to raise the issue on the ""dev"" in order to discuss whether to introduce a check to detect this problem.
","01/Oct/12 06:18;dhendriks;bq. final double x = x1 - ((f1 * (x1 - x0)) / (f1 - f0));

BaseSecantSolver (used for IllinoisSolver, PegasusSolver, and RegulaFalsiSolver), has the same line of code, at line 162 (CM 3.0 release).","01/Oct/12 09:25;erans;bq. In SecantSolver.doSolve(), I had a situation in which f0 and f1, from computeObjectiveValue() on lines 77 & 78, were the same value.

My first comment was a bit hasty.
""f0"" and f1"" cannot have the same value, as that would mean that there is no bracketing, a condition that is checked and, if not satisfied, raises an exception.

The line numbers you refer to seems to indicate that I do not look at the same code: Did you test with a recent snapshot of the library?
Alternately, please provide the use case.

","15/Oct/12 13:23;erans;Please reopen with more precise information.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Cumulative probability and inverse cumulative probability inconsistencies,MATH-692,12527632,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,cwinter,cwinter,18/Oct/11 18:01,23/Jul/12 23:18,20/Mar/20 20:33,02/Feb/12 06:45,1.0,1.1,1.2,2.0,2.1,2.2,3.0,3.0,,,0,,,,,,"There are some inconsistencies in the documentation and implementation of functions regarding cumulative probabilities and inverse cumulative probabilities. More precisely, '<' and '<=' are not used in a consistent way.

Besides I would move the function inverseCumulativeProbability(double) to the interface Distribution. A true inverse of the distribution function does neither exist for Distribution nor for ContinuosDistribution. Thus we need to define the inverse in terms of quantiles anyway, and this can already be done for Distribution.

On the whole I would declare the (inverse) cumulative probability functions in the basic distribution interfaces as follows:

Distribution:
- cumulativeProbability(double x): returns P(X <= x)
- cumulativeProbability(double x0, double x1): returns P(x0 < X <= x1) [see also 1)]
- inverseCumulativeProbability(double p):
  returns the quantile function inf{x in R | P(X<=x) >= p} [see also 2), 3), and 4)]

1) An aternative definition could be P(x0 <= X <= x1). But this requires to put the function probability(double x) or another cumulative probability function into the interface Distribution in order be able to calculate P(x0 <= X <= x1) in AbstractDistribution.
2) This definition is stricter than the definition in ContinuousDistribution, because the definition there does not specify what to do if there are multiple x satisfying P(X<=x) = p.
3) A modification could be defined for p=0: Returning sup{x in R | P(X<=x) = 0} would yield the infimum of the distribution's support instead of a mandatory -infinity.
4) This affects issue MATH-540. I'd prefere the definition from above for the following reasons:
- This definition simplifies inverse transform sampling (as mentioned in the other issue).
- It is the standard textbook definition for the quantile function.
- For integer distributions it has the advantage that the result doesn't change when switching to ""x in Z"", i.e. the result is independent of considering the intergers as sole set or as part of the reals.

ContinuousDistribution:
nothing to be added regarding (inverse) cumulative probability functions

IntegerDistribution:
- cumulativeProbability(int x): returns P(X <= x)
- cumulativeProbability(int x0, int x1): returns P(x0 < X <= x1) [see also 1) above]",,,,,,,,,,,,,,"19/Dec/11 22:47;cwinter;MATH-692_integerDomain_patch1.patch;https://issues.apache.org/jira/secure/attachment/12507992/MATH-692_integerDomain_patch1.patch","08/Nov/11 20:28;cwinter;Math-692_realDomain_patch1.patch;https://issues.apache.org/jira/secure/attachment/12502956/Math-692_realDomain_patch1.patch",,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-10-20 01:13:06.429,,,false,,,,,,,,,,,,,,,,,88883,,,Thu Feb 02 06:45:59 UTC 2012,,,,,,,"0|i0aovb:",60302,,,,,,,,,,,,,,,,"20/Oct/11 01:13;psteitz;Thanks for raising this issue, Christian - especially now as we finalize the 3.0 API.

I am +1 for these changes.  I agree that the inf-based definition of inverse cum is more standard and we are in a position now make the change, so I say lets do it.  I am also +1 on the move of this up to the distribution interface.  The reason we did not include it there originally was that we thought we might implement distributions for which we could not define inverses.  That has not happened in the last 8 years, so I think its safe enough to push it up.

The code, test, user guide and doc changes for this have to be done carefully.  Patches most welcome.

Is everyone else OK with this change?","20/Oct/11 06:09;celestin;I have neither used nor developed this part of CM, so my view on this is of but little value. Having said that, anything improving consistency can only be desirable, especially at this stage. So I'm all for it, and will be soon available (when I'm done on SYMMLQ) for an (novice on these issues) help.

Sébastien","20/Oct/11 06:26;mikl;+1","22/Oct/11 17:10;cwinter;Thanks for the feedback to all. Sébastien, thanks for offering your help. If you like and find time for it, you could implement AbstractDistribution.inverseCumulativeProbability(double p).

I will provide some patches next week, but adjusting AbstractContinuousDistribution.inverseCumulativeProbability(double p) will take some more time.

After thinking a little more about the structure of the interfaces, I'd like to put the function probability(double x) to Distribution anyway (independently of the thought in point 1) above).

Are there any preferences on P(x0 <= X <= x1) or P(x0 < X <= x1) for cumulativeProbability(double x0, double x1)?","22/Oct/11 20:24;psteitz;I am not sure it is really makes sense to add probability(double x) to the Distribution interface.  It would have to be defined as density (referring to the distribution function) to make sense in the continuous case, since defined as p(X = x) it would in most cases be identically 0 for continuous distributions.

Regarding the cum definition, I am fine with P(x0 < X <= x1).","23/Oct/11 08:39;celestin;Happy to help on the inverse cumulative probability. You will have to be patient and forgieving with me, though, as I discover this part of CM.

As for the definition, I think that one of the bounds should be excluded, so that these cumulative probabilities can be summed
P(a < X <= c) = P(a < X <= b) + P(b < X <= c),
even in the case of discrete PDFs.

Whether the lower or upper bound should be excluded is another matter. I usually work with continuous pdfs, so I don't know if there is a common practice in the probability community. If there is none, I would tend to chose the following definition
P(x0 <= X < x1)
(sorry Phil!), because it would be consistent with the way things are usually indexed in java (a[0].. a[a.length-1]). See also {{org.apache.commons.math.util.MultidimensionalCounter}}. Although this type of consistency is not an absolute requirement, I think it is nice for the user to have such simple principle: ""lower bound always included, upper bound always excluded"". Appart from this small point, I really have no objection to any choice.","23/Oct/11 15:22;psteitz;Have a look at the default implementation of cum(x0,x1) now in AbstractDistribution.  I think the incorrectness in the documentation there may have been what triggered Christian to raise this issue.  The equation cum(a,b) = F(b) - F(a) where F is the distribution function is natural and what the impl there is trying to do.  In the discrete case, this equation fails, however, unless you define the cum to exclude the *lower* endpoint.  That's why P(x0 < X <= x1) is a better definition.","23/Oct/11 15:31;celestin;OK, Phil, it makes perfect sense.","23/Oct/11 19:57;cwinter;Good, the definition of cum(x0,x1) will be P(x0 < X <= x1). Phil, you are right: cum(x0,x1) in AbstractDistribution was a reason for raising this issue. Another reason was cum(int x0, int x1) in AbstractIntegerDistribution.

The idea behind probability(double x) is in fact to define it as p(X = x) and to return 0 for continuous distributions. This function would be useful for discrete distributions not inheriting from IntergerDistribution and for distributions being composed of discrete and continuous parts.","23/Oct/11 20:59;psteitz;I guess I am OK with pushing p(x) up.  See related post to follow in commons-dev. ","27/Oct/11 07:07;celestin;Hi Christian,
I've started looking into this issue. As I said, you will have to be patient with me ;).
I can see there already is a default implementation of {{AbstractContinuousDistribution.inverseCumulativeProbability}}. So what exactly would you like me to do? Is this implementation fragile? Would you like me to improve robustness? Provide full testing?

I think there might be issues when the PDF falls down to zero in a range (in which case the cum exhibits a plateau). The returned value might differ from the mathematical definition you proposed. Is this what you want me to work on? Have you already identified other issues?

Best regards,
Sébastien","28/Oct/11 21:36;cwinter;Hi Sébastien,

the problem with the plateau is indeed one issue which needs to be solved.

Additionally, AbstractDistribution will need an implementation of inverseCumulativeProbability. In fact both implementations should be the same except for the solver to be used. Thus inverseCumulativeProbability should be implemented just once in AbstractDistribution, and invoking the solver should be put to a separate procedure so that it can be overridden in AbstractContinuousDistribution.

A third point is the choice of the solvers. For AbstractDistribution we need a solver which works even for discontinuous cdfs (BisectionSolver can do the job, but maybe the implementations of the faster IllinoisSolver, PegasusSolver, BrentSolver, or another solver can cope with discontinuities, too). For AbstractContinuousDistribution it would be beneficial to use a DifferentiableUnivariateRealSolver. However, the NewtonSolver cannot be used due to uncertainty of convergence and an alternative doesn't seem to exist by now. So we have to choose one of the other solvers for now.

As all these points are interdependent, I guess it's best to solve them as a whole. If you like, you can do this.

Best Regards,
Christian","28/Oct/11 22:02;cwinter;Another point for discussion:
I'd like to introduce
getDomainBracket(double p): returns double[]
to AbstractDistribution as helper function for inverseCumulativeProbability. This allows to avoid searching a bracket where a bracket can be specified directly.
The function getDomainBracket could be made abstract (which means to remove getInitialDomain, getDomainLowerBound, and getDomainUpperBound as these functions aren't needed any more), or it could have a default implementation (according to the corresponding part of the current implementation of inverseCumulativeProbability) which uses getInitialDomain, getDomainLowerBound, and getDomainUpperBound. However, getInitialDomain, getDomainLowerBound, and getDomainUpperBound should not be abstract in the latter case. Otherwise a derived class would be forced to implement something it potentially doesn't use. Thus the functions getInitialDomain, getDomainLowerBound, and getDomainUpperBound should have default implementations which either return default values (0, -infinity, +infinity) or throw an exception saying something like ""has to be implemented"".","29/Oct/11 04:23;celestin;Hi Christian,

{quote}
Hi Sébastien,

the problem with the plateau is indeed one issue which needs to be solved.
{quote}
I'm working on it...

{quote}
Additionally, AbstractDistribution will need an implementation of inverseCumulativeProbability. In fact both implementations should be the same except for the solver to be used. Thus inverseCumulativeProbability should be implemented just once in AbstractDistribution, and invoking the solver should be put to a separate procedure so that it can be overridden in AbstractContinuousDistribution.
{quote}
OK, for now, I'm concentrating on making the current impl in {{AbstractContinuousDistribution}} more robust. The other impl should be easier.

{quote}
A third point is the choice of the solvers. For AbstractDistribution we need a solver which works even for discontinuous cdfs (BisectionSolver can do the job, but maybe the implementations of the faster IllinoisSolver, PegasusSolver, BrentSolver, or another solver can cope with discontinuities, too). For AbstractContinuousDistribution it would be beneficial to use a DifferentiableUnivariateRealSolver. However, the NewtonSolver cannot be used due to uncertainty of convergence and an alternative doesn't seem to exist by now. So we have to choose one of the other solvers for now.
{quote}
The current implementation uses a Brent solver. I think the solver itself is only one side of the issue. The other point is the algorithm used to bracket the solution, in order to ensure that the result is consistent with the definition of the cumprob. As for the {{DifferentiableUnivariateRealSolver}}, I'm not too sure. I guess it depends on what is meant by ""continuous distribution"". For me, it means that the random variable takes values in a continuous set, and possibly its distribution is defined by a density. However, in my view, nothing prevents occurences of Dirac functions, in which case the cum sum is only piecewise C1. It's all a matter of definition, of course, and I'll ask the question on the forum to check whether or not people want to allow for such a situation.

{quote}
As all these points are interdependent, I guess it's best to solve them as a whole. If you like, you can do this.

Best Regards,
Christian
{quote}
Yes, I'm very interested.

Best regards,
Sébastien","05/Nov/11 08:29;celestin;Please note that MATH-699 has been created specifically to handle plateaux.

Sébastien","08/Nov/11 20:28;cwinter;Here is the first patch for this issue (unfortunately with some delay). It adjusts the distributions with real domain to the definitions in this issue, and it mainly changes documentations.

I could not move inverseCumulativeProbability(double) up to Distribution because there would be a conflict with IntegerDistribution.inverseCumulativeProbability(double): This method returns int. This problem will be removed by solving issue MATH-703.

The implementation of inverseCumulativeProbability(double) is not changed as Sébastien is working on this.

I will provide the patch for the integer distributions as soon as I have adjusted the test data to the new inequalities and reverified the adjusted test data.","09/Nov/11 07:22;celestin;All,
since I'm already working on this package, I'm happy to commit the patch on behalf of Christian. However, since I'm a relatively new committer, I would feel more confident if one of the ""old, wise committers"" could double check the svn log afterwards.

Best,
Sébastien","09/Nov/11 15:48;psteitz;Hey, that's how it always works :)  

I don't know about ""wise"" but I certainly qualify as ""old"" by any standard, so will have a look once you have reviewed and committed.

Thanks!","10/Nov/11 06:23;celestin;Patch {{Math-692_realDomain_patch1.patch}} (20111108) applied in rev 1200179, with minor modifications (mostly checkstyle fixes).
Thanks Christian!
","04/Dec/11 21:02;cwinter;As mentioned by Sébastien in MATH-699, the implementation of {{IntegerDistribution.inverseCumulativeProbability(double p)}} can benefit from the ideas which came up for {{RealDistribution.inverseCumulativeProbability(double p)}} in that thread.

Thus I will remove {{getDomainLowerBound(double p)}} and {{getDomainUpperBound(double p)}} from the integer distributions. I checked that all current implementations of the lower/upper bound methods provide the whole support of the distribution as starting bracket. This means that using {{getSupportLowerBound()}} and {{getSupportUpperBound()}} for the starting bracket won't degrade the performance of the current distribution implementations. However, a user might want the improve the performance of his distribution implementations by providing a more targeted starting bracket for probability {{p}}. Thus I will swap the solving step to a protected function {{solveInverseCumulativeProbability(double p, int lower, int upper)}}, so that it gets easy to override {{inverseCumulativeProbability}} with an implementation which finds a better starting bracket.

Furthermore, Phil's idea with Chebyshev's inequality can be applied to the generic implementation of {{inverseCumulativeProbability}} in order to get a better starting bracket.","05/Dec/11 06:51;celestin;Hi Christian,
If you agree with that, I suggest that you also take care of MATH-718, as the two issues seem to be very much connected.
Sébastien","15/Dec/11 23:44;cwinter;Hi Sébastien,

my changes in the integer distributions don't solve MATH-718. Instead I found a probably related problem with the Pascal distribution.

The integer distribution patch for this issue still isn't ready. I will provide it next week.

Christian","19/Dec/11 22:47;cwinter;This is the patch which adjusts the integer distributions to the agreements above.

The changes to the test cases for the random generators may be unexpected. But these changes initially were triggered by adjusting {{RandomDataTest.checkNextPoissonConsistency(double)}} to the new contract for integer distributions. Then some random generator tests failed due to chance. While adjusting their seeds, I found some other tests with a high failure probability. Thus I also set some failure probabilities to 0.01 in order to find suitable seeds more quickly.

My next task on this issue is to adjust the user guid.","20/Dec/11 20:27;celestin;Hi Christian,
thanks for this contribution. I am away for a few days, but am very happy to commit this patch as soon as I am back, if you are not in too much of a hurry.
Thanks again,
Sébastien","31/Dec/11 05:25;celestin;Well, we've recently run into some troubles with SVN, but it seems everything is working fine again. Patch {{MATH-692_integerDomain_patch1.patch}} (with minor checkstyle changes) committed in revision {{1226041}}.

Please do not forget to run {{mvn clean; mvn site:site}} and check the reports (in particular, {{checkstyle}}) prior to submitting a patch!

Thanks for this contribution.","31/Dec/11 08:39;celestin;The committed patch actually causes failure of {{Well1024Test}} in {{o.a.c.m.random}}.","31/Dec/11 17:01;cwinter;Thanks for committing the patch, Sébastien. I see you already changed the seed in {{Well1024aTest}}. This hopefully removes the failure.

I'll have a look into Maven to prepare a better patch next time. :-)","31/Dec/11 17:11;celestin;{quote}
I see you already changed the seed in Well1024aTest.
{quote}

Yes I did, but is this really how we want {{Well2004aTest}} to pass?","02/Jan/12 17:51;cwinter;I guess there is no alternative to this way of making probabilistic test cases pass. However, I understand your bad feeling with this kind of failure fixing. The problem is that probabilistic tests are quiet fuzzy: Neither a passed test nor a failed test provides a clear answer whether something is right or wrong in the implementation. There is just a high chance to pass such a test with a correct implementation. The chance for failure increases with an erroneous implementation due to systematic deviations in the generated data. These chances tell whether it is easy to find a seed which passes the tests or not. Thus difficulties in finding a suitable seed are an indicator for problems in the code.","02/Jan/12 18:53;celestin;{quote}
Thus difficulties in finding a suitable seed are an indicator for problems in the code.
{quote}

That's exactly the point I've raised on the mailing-list: out of three seeds (100, 1000 and 1001), only one works. Of course, I would not dare to call that representative statistics, but I'm wondering whether or not we should be worried...","02/Feb/12 06:45;celestin;The issue about selection of an appropriate seed has been raised elsewhere. No definitive answer has been provided so far, so I suggest we consider this issue as solved for the time being."
Truncation issue in KMeansPlusPlusClusterer,MATH-546,12501227,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,npaymer,npaymer,12/Mar/11 04:52,24/Mar/12 16:16,20/Mar/20 20:33,16/Mar/11 12:58,3.0,,,,,,,3.0,,,0,cluster,,,,,"The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable
  int sum = 0;
This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1.

As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.",,,,,,,,,,,,,,"13/Mar/11 13:13;npaymer;MATH-546.txt;https://issues.apache.org/jira/secure/attachment/12473504/MATH-546.txt",,,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-03-15 12:22:16.071,,,false,,,,,,,,,,,,,,,,,150657,,,Tue Mar 15 12:31:46 UTC 2011,,,,,,,"0|i0rtuf:",160479,,,,,,,,,,,,,,,,"13/Mar/11 13:13;npaymer;I've a patch to fix this bug.

This is my first contribution to this project, so apologies if I've screwed something up :)","15/Mar/11 12:22;erans;Fixed in revision 1081744.
Thanks for the report and the patch.

Leaving open until an answer can be provided concerning the ""EmptyClusterStrategy"" question.
","15/Mar/11 12:31;luc;The empty cluster strategy is needed regardless of this bug. It may appear with different conditions and is a feature commonly found in clustering implementations.
This issue can be marked as resolved if the patch has been applied and works.

Thanks to Nate for reporting and fixing the issue, thanks to Gilles for reviewing and applying the patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
CMA-ES optimizer input sigma should not be normalized by user,MATH-702,12530572,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,luc,luc,07/Nov/11 13:47,24/Mar/12 16:16,20/Mar/20 20:33,07/Nov/11 14:13,3.0,,,,,,,3.0,,,0,,,,,,"I am trying to use CMA-ES optimizer with simple boundaries.

It seems the inputSigma parameter should be normalized as it is checked against the [0 - 1] range in the checkParameters private method and as its value defaults to 0.3 if not not set in the initializeCMA private method.

I would have expected this value to be in the same units as the user parameters and to be normalized as part of an internal processing step instead of relying to the user doing this. I think the method need normalized values internally, as per the encode/decode methods in the inner class FitnessFunction suggest.

The optimizer should accept values in the same units as the other parameters and use ""encode"" (or a similar function) to do the normalization. This way, normalization is considered an internal implementation detail.

",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-11-07 19:46:51.517,,,false,,,,,,,,,,,,,,,,,216310,,,Tue Nov 08 21:57:18 UTC 2011,,,,,,,"0|i0aotb:",60293,,,,,,,,,,,,,,,,"07/Nov/11 14:13;luc;Fixed in subversion repository as of r1198741.","07/Nov/11 19:46;Nikolaus.Hansen@lri.fr;good point. However, if the encoding/decoding methods can be defined
coordinate-wise differently or are even non-linear (I am not aware whether
they can be or not), I am not sure you can come up with a sufficiently
reasonable and comprehensible way to apply this transformation to sigma.
sigma is a scalar and positive, so it is a different object than the
encoding/decoding methods are operating on, right?

The interplay between sigma and an encoding is an unfortunately delicate
part in the user interface, but I don't really see a way to make it right
*and* look entirely obvious to the user.


On Mon, 07 Nov 2011 14:48:51 +0100, Luc Maisonobe (Created) (JIRA)



-- 
Science is a way of trying not to fool yourself.

The first principle is that you must not fool yourself, and you
are the easiest person to fool. So you have to be very careful
about that. After you've not fooled yourself, it's easy not to
fool other[ scientist]s. You just have to be honest in a
conventional way after that.
                                         -- Richard P. Feynman

Nikolaus Hansen
INRIA, Research Centre Saclay – Ile-de-France
Machine Learning and Optimization group (TAO)
University Paris-Sud (Orsay)
LRI (UMR 8623), building 490
91405 ORSAY Cedex, France
Phone: +33-1-691-56495, Fax: +33-1-691-54240
URL: http://www.lri.fr/~hansen
","07/Nov/11 19:59;luc;You are right.
for now, encoding/decoding is both liner only and hidden in a private inner class (FitnessFunction), so users only see simple bounds and inside these bounds the transform is linear.
For sure if we come up with a different mapping, we will need to come up with a way to define also the mapping of covariance. One way would be to rely on the Jacobian, but it would be strange to propose mapping function and requiring them to be smooth while the goal function by itself could be highly non-smooth.
So for now, the simple linear mapping seems sufficient to me, and would need improvement only if we change some internals of the class.","08/Nov/11 21:51;Nikolaus.Hansen@lri.fr;unfortunately, the problem already arises with a linear coordinate-wise  
mapping: which coordinate gives the unit where sigma is defined on?

 from a practical viewpoint it is important to consider coordinate-wise  
non-linear mappings. Multidimensional mappings (e.g. with ""covariance"") I  
have never been able to apply successfully.

Cheers,
Niko

On Mon, 07 Nov 2011 21:00:51 +0100, Luc Maisonobe (Commented) (JIRA)  



-- 
Science is a way of trying not to fool yourself.

The first principle is that you must not fool yourself, and you
are the easiest person to fool. So you have to be very careful
about that. After you've not fooled yourself, it's easy not to
fool other[ scientist]s. You just have to be honest in a
conventional way after that.
                                     -- Richard P. Feynman

Nikolaus Hansen
INRIA, Research Centre Saclay – Ile-de-France
Machine Learning and Optimization group (TAO)
University Paris-Sud (Orsay)
LRI (UMR 8623), building 490
91405 ORSAY Cedex, France
Phone: +33-1-691-56495, Fax: +33-1-691-54240
URL: http://www.lri.fr/~hansen
","08/Nov/11 21:57;luc;Does ""coordinate-wise"" means that each coordinate has its dedicated sigma ?
If so, this is what I have set up. The sigma vector was already an array with the same dimension as both the state vector and the lower/upper bounds, so I have simply used upper[i] - lower[i] as a multiplication factor for sigma[i].
Does what I did make sense ?",,,,,,,,,,,,,,,,,,,,,,,,,,
BracketingNthOrderBrentSolver exceeds maxIterationCount while updating always the same boundary,MATH-716,12533416,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,luc,pparraud,pparraud,01/Dec/11 09:36,24/Mar/12 16:16,20/Mar/20 20:33,01/Dec/11 23:25,3.0,,,,,,,3.0,,,0,,,,,,"In some cases, the aging feature in BracketingNthOrderBrentSolver fails.
It attempts to balance the bracketing points by targeting a non-zero value instead of the real root. However, the chosen target is too close too zero, and the inverse polynomial approximation is always on the same side, thus always updates the same bracket.
In the real used case for a large program, I had a bracket point xA = 12500.0, yA = 3.7e-16, agingA = 0, which is the (really good) estimate of the zero on one side of the root and xB = 12500.03, yB = -7.0e-5, agingB = 97. This shows that the bracketing interval is completely unbalanced, and we never succeed to rebalance it as we always updates (xA, yA) and never updates (xB, yB).",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-12-01 09:55:18.756,,,false,,,,,,,,,,,,,,,,,219144,,,Thu Dec 01 23:25:58 UTC 2011,,,,,,,"0|i0aoqf:",60280,,,,,,,,,,,,,,,,"01/Dec/11 09:55;luc;The problem Pascal (who works with me) describes has already been encountered during the development of the algorithm. The solution found at that time seems to be insufficient. What happens is that in order to still gain a few digits while rebalancing the bracketing interval, we base our retargeting on the currently best solution. In this case, we set targetY = -yA/16 and fail to rebalance. Using the other bracket would improve rebalancing but waste evaluations as was observed during development. So its not a perfect solution either.

I think a compromise would be to attempt rebalancing with a progressively more aggressive target. We could start from the current setting (i.e -1/16 of the best bracket), and if we still update the same side, move towards larger targets.","01/Dec/11 22:15;luc;The following simple test reproduces the bad behavior with a trivial function:
{code}
    @Test
    public void testIssue716() {
        BracketingNthOrderBrentSolver solver =
                new BracketingNthOrderBrentSolver(1.0e-12, 1.0e-10, 0.0, 5);
        UnivariateFunction sharpTurn = new UnivariateFunction() {
            public double value(double x) {
                return (2 * x + 1) / (1.0e9 * (x + 1));
            }
        };
        double result = solver.solve(100, sharpTurn, -0.9999999, 30, 15, AllowedSolution.RIGHT_SIDE);
        Assert.assertEquals(0, sharpTurn.value(result), solver.getFunctionValueAccuracy());
        Assert.assertTrue(sharpTurn.value(result) >= 0);
        Assert.assertEquals(-0.5, result, 1.0e-10);
    }
{code}
The test fails with TooManyEvaluationsException. In fact, only the right side of the bracketing interval is updated and very slowly decreases from 15.0 to 14.999677603318897 while the left side of the bracketing interval is stuck at -0.9999999.","01/Dec/11 23:25;luc;Fixed in subversion repository as of r1209307.

Thanks for the report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
in ArrayFielVector i.e. subtract calls wrong constructor,MATH-573,12506690,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,aploese,aploese,10/May/11 08:23,24/Mar/12 16:16,20/Mar/20 20:33,05/Jun/11 15:32,3.0,,,,,,,3.0,,,0,,,,,,"I.E. subtract calls

""return new ArrayFieldVector<T>(out)"" this constructor clones the array...
""return new ArrayFieldVector<T>(field, out, false)"" would be better (preserving field as well)",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-05-10 18:20:24.611,,,false,,,,,,,,,,,,,,,,,71909,,,Sun Jun 05 15:32:33 UTC 2011,,,,,,,"0|i0rton:",160453,,,,,,,,,,,,,,,,"10/May/11 18:20;luc;Fixed in subversion repository as of r1101579.

The same problem was also present in the add methods.

Thanks for reporting the issue and providing the fix.","10/May/11 20:21;aploese;Sorry, but look at mapSubtract and further down in the source you will find plenty.

The same goes for AbstractArrayMatrix... 

Sorry ;-)

","11/May/11 06:59;luc;Thanks to have checked this and sorry for forgetting these occurrences. Could you check again and reopen if I still forgot a few ones ?

Fixed in subversion repository as of r1101763","11/May/11 17:19;aploese;At least copy() and public constructor Array2DRowFieldMatrix<T> add(final Array2DRowFieldMatrix<T> m) in Array2DRowFieldMatrix.java ignores field | there is no contract what should happen with field (whetzer it is to pe basses or not, and if yes after what pattern) - so not really a bug more a hint.

Maybe an other issue: should the ArrayFieldVector v from which data is used, not given the chance to review its internal state by asking v.getDataRef() instead of simply taking v.data (currently just a philosophical question :-)) ?","11/May/11 20:03;luc;Nice catch. I have added a few constructors with the field parameter as of r1102057.

Concerning v.data versus v.getDataRef(), there is no fixed rule. As far as I am concerned, I use directly the attribute as it is shorter and easier to read. Some other [math] developers may do otherwise. I don't think derived class completely discard an attribute from their base class to replace it. However, in these rare cases, using a getter would of course be the right way to go.","12/May/11 06:31;aploese;No, I think more of a derived class which do some fancy cashing and not updating data immediately - as I said just a thought.","12/May/11 09:53;aploese;Array2DRowFieldMatrix.java: 263 | 295","13/May/11 06:47;aploese;I suggest to remove all constructores that not specify whether the data should be copied or not. If you do so you will find many ""wrong"" usages ...

An other point why there is the interface FieldVector when the implementing class (ArrayFieldVector) of FieldVector is leaking out (and in) of ArrayFieldVector on every place - IMHO this is bad coding style.","13/May/11 06:47;aploese;sorry, see my last comment","13/May/11 07:19;aploese; public T[] preMultiply(final T[] v) {

is implemented twice 1st in AbstractFieldMatrix.java and 2nd in Array2DRowFieldMatrix.java","14/May/11 10:05;luc;I have tried to specify field wherever it was possible. Thanks for the tip about removing the constructor and fixing the errors. I have left many calls of the no-field constructors in the tests, for testing purposes, there are no such calls left in the library itself.

We know ArrayFieldVector leaks out in the interface. This was on purpose when people already know they want to use this implementation. They are however free to have different implementations for storage, so the interface is also useful (but not to the same users). In fact, when we have a method that returns a FieldVector in the interface, we should say it returns an ArrayFieldVector in the implementation if we know this is what always happen. Implementing or overriding a method with a narrowed return type is allowed by the Java language. We did not do it on all methods, so I agree we are inconsistent here.

The two implementations of premultiply are different. One uses the generic getEntry methods and the other uses direct array access. At that time, it was done for efficiency. We did not benchmark it recently with new JVMs, but it may not be needed anymore. The usefulness of this overriding is really JVM dependent.","14/May/11 15:05;aploese;OK so fare.

I hope that you don't flame me for nagging you again :-)

Looking at RealArrayVecor and FieldArrayVector I see different implementations of mapSOMETHING ... Maybe check performance, define some best condiing practice and refactor | clean up?

Furthermore I saw the checkDimension() is sometimes called sometimes not? - Just a hint... ","15/May/11 13:22;luc;Don't worry Arne, I'm happy you point out mistakes in our code.

I seem to have forgotten committing the changes I made yesterday about specifying field wherever possible. It's in the repository now.

Doing some benchmarks to check if we should remove or keep different implementations would be a good thing. Would you try to give it a try and report your results, preferably opening a new Jira issue ?

I found checkDimension only in MultivariateSummaryStatistics and checkdimension (small case ""d"") only in tests. Where did you see other use ?","15/May/11 17:46;aploese;In AbstractRealvector

public RealVector add(double[] v) {
there is no check of any dimension (goes for (at least some) other operators as well.

What kind of benchmark do you want (RealVector current implementations v.s. true array|sparse ?) or the operators in map*?

Maybe I could think of an implementations where there is a private interface Storage {
 T getEntrx(int i);
 void setEntry(int i; T e);
 int getDimension();
 int setDimension(Field f, int size);
 int setDimension(T[] v, boolean makeCopy); << I dont like the name copyArray its implemetation specifict but dataIsInmutabe sounds also bad ....
}

this is instatiated with ether An array implentation or a sparse list one could define if the fillstate is more than 50 % switch to array ore something else ... just an idea ....","05/Jun/11 15:32;luc;It now seems everything has finally been sorted out for this issue, after several failed attempts, so marking it as resolved.",,,,,,,,,,,,,,,,
UnivariateRealIntegrator throws ConvergenceException,MATH-669,12523194,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,celestin,celestin,15/Sep/11 14:14,18/Mar/12 09:32,20/Mar/20 20:33,15/Sep/11 14:28,3.0,,,,,,,3.0,,,0,,,,,,"{{ConvergenceException}} is a checked exception, which goes against the developer's guide. It occurs in the {{throws}} clause of some methods in package o.a.c.m.analysis.integration. It seems that these occurences are remnants from previous versions, where exceptions were probably checked. This exception is actually never thrown : it is safe to remove it from the {{throws}} clause.",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,60559,,,Sun Mar 18 09:32:51 UTC 2012,,,,,,,"0|i0aoyv:",60318,,,,,,,,,,,,,,,,"15/Sep/11 14:28;celestin;Done in rev1171111.","18/Mar/12 09:32;celestin;Fixed in 3.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HarmonicCoefficientsGuesser.sortObservations() potentlal NPE warning,MATH-467,12494849,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,sebb,sebb,06/Jan/11 14:49,23/Mar/11 20:30,20/Mar/20 20:33,06/Jan/11 19:43,2.2,3.0,,,,,,,,,0,,,,,,"HarmonicCoefficientsGuesser.sortObservations()

generates an NPE warning from Eclipse which thinks that mI can be null in the while condition.

The code looks like:
{code}
WeightedObservedPoint mI = observations[i];
while ((i >= 0) && (curr.getX() < mI.getX())) {
    observations[i + 1] = mI;
    if (i-- != 0) {
        mI = observations[i];
    } else {
        mI = null;
    }
}
// mI is not used further
{code}

It looks to me as though the ""mI = null"" statement is either redundant or wrong - why would one want to replace one of the observations with null during a sort?",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-01-06 19:43:47.327,,,false,,,,,,,,,,,,,,,,,150596,,,Wed Mar 23 20:30:30 UTC 2011,,,,,,,"0|i0rubb:",160555,,,,,,,,,,,,,,,,"06/Jan/11 19:43;luc;You are right. The else statement is not needed.
Fixed in subversion repository as of r1056034 for branch 2.X and as of r1056035 for trunk","23/Mar/11 20:30;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MathRuntimeException#createInternalError() method loosing the exception ""cause""",MATH-415,12473912,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,axelclk,axelclk,11/Sep/10 17:05,23/Mar/11 20:21,20/Mar/20 20:33,11/Sep/10 17:21,3.0,,,,,,,2.2,,,0,,,,,,"The MathRuntimeException#createInternalError(Throwable cause) method doesn't store the exception ""cause"".
Is this intentionally?

",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-09-11 17:21:33.028,,,false,,,,,,,,,,,,,,,,,150560,,,Wed Mar 23 20:21:06 UTC 2011,,,,,,,"0|i0rulz:",160603,,,,,,,,,,,,,,,,"11/Sep/10 17:21;luc;No, it was an error.
It should now be fixed in subversion repository, as of r996178 fro branch 2.x and as of r996179 for trunk.
Thanks for reporting this","23/Mar/11 20:21;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""CMAESOptimizer"" silently changes invalid input",MATH-879,12611338,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,erans,erans,erans,11/Oct/12 12:33,20/Aug/15 20:04,20/Mar/20 20:33,20/Aug/15 20:04,3.0,,,,,,,4.0,,,0,,,,,,"The ""lambda"" input parameter must be strictly positive. But when it's not the case, an undocumented default is used (cf. line 526).
When a precondition is not satisfied, the code must throw an exception.
Instead of the code unknowingly changing the input, it is rather the documentation that should suggest a good default.

This change would allow to make ""lambda"" a constant (final) field.
",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,247321,,,Thu Aug 20 20:04:27 UTC 2015,,,,,,,"0|i089vb:",46198,,,,,,,,,,,,,,,,"18/Oct/12 13:56;erans;The code is:
{code}
if (lambda <= 0) {
  lambda = 4 + (int) (3 * Math.log(dimension));
}
{code}

This implies that the default is dependent on the number of optimized parameters. Hence, I would suggest that ""lambda"" be specified at the call to ""optimize"", as kind of ""OptimizationData"" (with a more suggestive name, such as ""PopulationSize""), with no default (the value currently used in the code could appear as a _suggestion_ in the documentation).
And passing this parameter through the constructor will be deprecated.

What do you think?
","19/Oct/12 14:23;erans;""PopulationSize"" created in revision 1400108. Old code deprecated.

Further changes postponed until preparation of 4.0 (see code marked with ""XXX"").
","20/Aug/15 20:04;erans;Probably fixed some time ago, as there is no more code marked with ""XXX""...
",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"public method ""laguerre"" should be private",MATH-825,12598934,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,,erans,erans,16/Jul/12 09:16,16/Feb/15 23:04,20/Mar/20 20:33,16/Feb/15 23:04,3.0,,,,,,,4.0,,,0,,,,,,"In class ""LaguerreSolver"" (package ""o.a.c.m.analysis.solvers""), the method ""laguerre"" is public. However, it doesn't make any sense to call it from outside the class (because its argument list does not contain the function whose roots must be computed).
The method should be made private.",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-02-16 23:04:05.37,,,false,,,,,,,,,,,,,,,,,241586,,,Mon Feb 16 23:04:05 UTC 2015,,,,,,,"0|i028tb:",11029,,,,,,,,,,,,,,,,"16/Jul/12 09:19;erans;Since the change is not backward-compatible, the method should be marked as deprecated in 3.1.","16/Jul/12 09:57;erans;bq. [...] the method should be marked as deprecated [...]

Done in revision 1361956.","16/Feb/15 23:04;tn;Fixed in commit 5f47ad718e0ffcbbe6cd2f8d32b2e345c4af3a48.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Failures in ""FastMathTestPerformance"" when testRuns >= 10,000,002",MATH-840,12601475,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,,erans,erans,05/Aug/12 02:35,04/Mar/13 18:53,20/Mar/20 20:33,05/Aug/12 02:38,3.0,,,,,,,3.1,,,0,test,,,,,"Tests for methods ""asin"" and ""acos"" fail because they use
{code}
i / 10000000.0
{code}
as the argument to those methods, where ""i"" goes from 0 to the value of ""testRuns"" minus one (if ""testRuns"" is defined).

A solution is to replace the above with
{code}
i / (double) RUNS
{code}
",,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,292281,,,Sun Aug 05 02:38:14 UTC 2012,,,,,,,"0|i0rt0n:",160345,,,,,,,,,,,,,,,,"05/Aug/12 02:38;erans;Fixed in revision 1369514.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Removing some packages that are not needed in some classes,MATH-539,12500525,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Not A Problem,,miccagiann,miccagiann,05/Mar/11 21:21,23/Mar/11 20:44,20/Mar/20 20:33,06/Mar/11 01:52,3.0,,,,,,,,,,0,cleanup,patch,,,,"Well,

Looking at the junit test files of CM project i noticed that in some of them are imported some classes that are never used so i decided to remove them... Here are the patches!",Ubuntu Linux 10.04 (lucid),,,,,,,,,,,,,"05/Mar/11 21:22;miccagiann;CorrelatedRandomVectorGeneratorTest_Patch.txt;https://issues.apache.org/jira/secure/attachment/12472759/CorrelatedRandomVectorGeneratorTest_Patch.txt","05/Mar/11 21:23;miccagiann;UncorrelatedRandomVectorGeneratorTest_Patch.txt;https://issues.apache.org/jira/secure/attachment/12472760/UncorrelatedRandomVectorGeneratorTest_Patch.txt",,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-03-06 01:52:29.55,,,false,,,,,,,,,,,,,,,,,150651,,,Wed Mar 23 20:44:40 UTC 2011,,,,,,,"0|i0rtvz:",160486,,,,,,,,,,,,,,,,"06/Mar/11 01:52;sebb;These imports are currently needed.

By the way, please don't use Eclipse workspace-relative patches. They only work if everyone uses exactly the same name for projects.

Project-relative patches can be used by anyone.","06/Mar/11 03:04;erans;I don't see those imports in ""trunk"" as of revision 1078400.
","23/Mar/11 20:44;luc;Closing an issue that was not a problem",,,,,,,,,,,,,,,,,,,,,,,,,,,,

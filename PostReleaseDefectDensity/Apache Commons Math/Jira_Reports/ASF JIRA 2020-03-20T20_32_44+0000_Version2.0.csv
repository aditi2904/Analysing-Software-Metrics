Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Reference),Outward issue link (Reference),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Tags),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
MultiDirectional optimzation loops forver if started at the correct solution,MATH-283,12432884,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Blocker,Fixed,luc,michael.nischt,michael.nischt,12/Aug/09 14:51,14/Apr/10 00:32,20/Mar/20 20:32,14/Aug/09 19:25,2.0,,,,,,,2.1,,0,,,,,,,,"MultiDirectional.iterateSimplex loops forever if the starting point is the correct solution.

see the attached test case (testMultiDirectionalCorrectStart) as an example.",none specific for the issue (it's a programming bug),86400,86400,,0%,86400,86400,,,,,,"12/Aug/09 15:02;michael.nischt;MultiDirectionalCorrectStartTest.java;https://issues.apache.org/jira/secure/attachment/12416328/MultiDirectionalCorrectStartTest.java",,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2009-08-14 19:25:15.696,,,false,,,,,,,,,,,,,,,,,34193,,,Fri Aug 14 19:25:15 UTC 2009,,,,,,,"0|i0rven:",160732,,,,,,,,,,,,,,,,"12/Aug/09 15:02;michael.nischt;the failing unit test","14/Aug/09 19:25;luc;fixed in subversion repository as of r804328
thanks for the report and the fix suggestion",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jacobian rank determination in LevenbergMarquardtOptimizer is not numerically robust,MATH-352,12458613,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,geneg102,geneg102,10/Mar/10 03:58,23/Mar/11 20:01,20/Mar/20 20:32,06/Jun/10 14:04,2.0,,,,,,,2.2,,0,commons-math,DifferentiableMultivariateVectorialOptimizer,Levenberg,Marquardt,optimizer,qr,rank,"LevenbergMarquardtOptimizer is designed to handle singular jacobians,  i.e. situations when some of the fitted parameters depend on each other. The check for that condition is in LevenbergMarquardtOptimizer.qrDecomposition uses precise comparison to 0.

    if (ak2 == 0 ) {
                rank = k;
                return;
        }

A correct check would be comparison with a small epsilon. Hard coded 2.2204e-16 is used elsewhere in the same file for similar purpose.

final double QR_RANK_EPS = Math.ulp(1d); //2.220446049250313E-16
....
    if (ak2  < QR_RANK_EPS) {
                rank = k;
                return;
        }

Current exact equality check is not tolerant of the real world poorly conditioned situations. For example I am trying to fit a cylinder into sample 3d points. Although theoretically cylinder has only 5 independent variables, derivatives for optimizing function (signed distance) for such minimal parametrization are complicated and it  it much easier to work with a 7 variable parametrization (3 for axis direction, 3 for axis origin and 1 for radius). This naturally results in rank-deficient jacobian, but because of the numeric errors the actual ak2 values for the dependent rows ( I am seeing values of 1e-18 and less), rank handling code does not kick in.
Keeping these tiny values around then leads to huge corrections for the corresponding very slowly changing parameters, and consequently to numeric errors and instabilities. I have noticed the problem because tiny shift in the initial guess (on the order of 1e-12 in the axis component and origins) resulted in significantly different finally converged answers (origins and radii differing by as much as 0.02) which I tracked to loss of precision due to numeric error with root cause described above.
Providing a cutoff as suggested fixes the issue. After the fix, small perturbations in the initial guess had practically no effect to the converged result - as expected from a robust algorithm.
",commons-math 2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-06-06 14:04:42.316,,,false,,,,,,,,,,,,,,,,,150520,,,Wed Mar 23 20:01:13 UTC 2011,,,,,,,"0|i0ruzr:",160665,,,,,,,,,,,,,,,,"06/Jun/10 14:04;luc;Fixed in subversion repository as of r951864.
A setQRRankingThreshold has been added as proposed in [http://markmail.org/message/p2j76cnwsyehl7u6].
Thanks for reporting the issue.","23/Mar/11 20:01;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign,MATH-343,12457208,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,aktasv,aktasv,23/Feb/10 20:21,03/Apr/10 03:44,20/Mar/20 20:32,23/Feb/10 21:02,2.0,,,,,,,2.1,,0,,,,,,,,"Javadoc for ""public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)"" claims that ""if the values of the function at the three points have the same sign"" an IllegalArgumentException is thrown. This case isn't even checked.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-02-23 21:02:31.079,,,false,,,,,,,,,,,,,,,,,34172,,,Wed Feb 24 11:34:29 UTC 2010,,,,,,,"0|i0rv1r:",160674,,,,,,,,,,,,,,,,"23/Feb/10 21:02;luc;Fixed in subversion repository as of r915517
Thanks for reporting the issue","23/Feb/10 22:56;aktasv;Thanks for the quick turnaround. One comment: I'm not sure whether the check before throwing the IllegalArgumentException is necessary. You can have only the following situations (given that min <= initial <= max and assuming neither min nor max is a root):

    * yMin and yMax have the same sign:
        ** yInitial has a different sign: Handled on line 121 (function is not monotonous between min and max)
        ** yInitial has the same sign: Falls through to line 136 and yMin * yMax > 0 by definition
    * yMin and yMax do not have the same sign:
        ** yInitial has the same sign as yMax: Handled on line 121
        ** yInitial has the same sign as yMin: Handled on line 133

In this case I'd say code between lines 131 and 142 should be replaced by the throw statement on line 137.","24/Feb/10 11:34;luc;You are right.
I have removed the unreachable code and committed it in the subversion repository
Thanks again",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brent solver returns the wrong value if either bracket endpoint is root,MATH-344,12457210,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,aktasv,aktasv,23/Feb/10 20:23,03/Apr/10 03:07,20/Mar/20 20:32,23/Feb/10 21:10,2.0,,,,,,,,,0,,,,,,,,"The solve(final UnivariateRealFunction f, final double min, final double max, final double initial) function returns yMin or yMax if min or max are deemed to be roots, respectively, instead of min or max.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-02-23 21:10:00.408,,,false,,,,,,,,,,,,,,,,,34150,,,Tue Feb 23 21:10:00 UTC 2010,,,,,,,"0|i0rv1j:",160673,,,,,,,,,,,,,,,,"23/Feb/10 21:10;luc;Fixed in subversion repository as of r915522
thanks for reporting the issue",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ODE integrator goes past specified end of integration range,MATH-358,12460137,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,luc,luc,luc,24/Mar/10 17:25,03/Apr/10 03:01,20/Mar/20 20:32,24/Mar/10 22:13,2.0,,,,,,,2.1,,0,,,,,,,,"End of integration range in ODE solving is handled as an event.
In some cases, numerical accuracy in events detection leads to error in events location.
The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.
{code}
  public void testMissedEvent() throws IntegratorException, DerivativeException {
          final double t0 = 1878250320.0000029;
          final double t =  1878250379.9999986;
          FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {
            
            public int getDimension() {
                return 1;
            }
            
            public void computeDerivatives(double t, double[] y, double[] yDot)
                throws DerivativeException {
                yDot[0] = y[0] * 1.0e-6;
            }
        };

        DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,
                                                                               1.0e-10, 1.0e-10);

        double[] y = { 1.0 };
        integrator.setInitialStepSize(60.0);
        double finalT = integrator.integrate(ode, t0, y, t, y);
        Assert.assertEquals(t, finalT, 1.0e-6);
    }

{code}",Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,34176,,,Wed Mar 24 22:13:19 UTC 2010,,,,,,,"0|i0ruyn:",160660,,,,,,,,,,,,,,,,"24/Mar/10 22:13;luc;fixed in subversion repository as of r927202",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractRandomGenerator nextInt() and nextLong() default implementations generate only positive values,MATH-640,12517683,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,psteitz,psteitz,02/Aug/11 19:06,24/Mar/12 16:16,20/Mar/20 20:32,03/Aug/11 04:17,1.1,1.2,2.0,2.1,2.2,,,3.0,,0,,,,,,,,The javadoc for these methods (and what is specified in the RandomGenerator interface) says that all int / long values should be in the range of these methods.  The default implementations provided in this class do not generate negative values.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,66450,,,Wed Aug 03 04:17:43 UTC 2011,,,,,,,"0|i0ap27:",60333,,,,,,,,,,,,,,,,"03/Aug/11 04:17;psteitz;Fixed in r1153338",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"BitStreamGenerators (MersenneTwister, Well generators) do not clear normal deviate cache on setSeed",MATH-723,12534683,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,psteitz,psteitz,11/Dec/11 21:03,24/Mar/12 16:16,20/Mar/20 20:32,11/Dec/11 21:59,2.0,2.1,2.2,,,,,3.0,,0,,,,,,,,"The BitStream generators generate normal deviates (for nextGaussian) in pairs, caching the last value generated. When reseeded, the cache should be cleared; otherwise seeding two generators with the same value is not guaranteed to generate the same sequence.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220404,,,Sun Dec 11 21:59:41 UTC 2011,,,,,,,"0|i0aopr:",60277,,,,,,,,,,,,,,,,"11/Dec/11 21:59;psteitz;Fixed in r1213087.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestUtils is thread-hostile,MATH-505,12497270,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,sebb,sebb,sebb,01/Feb/11 00:28,24/Mar/12 16:16,20/Mar/20 20:32,01/Feb/11 18:58,1.2,2.0,2.1,,,,,3.0,,0,,,,,,,,"TestUtils has several mutable static fields which are not synchronised, or volatile.

If one of the fields is updated by thread A, there is no guarantee that thread B will see the full update - it may see a partially updated object.

Furthermore, at least some of the static fields reference a mutable object, which can be changed whilst another thread is using it.

As far as I can tell, this class must only ever be used by a single thread otherwise the results will be unpredictable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-02-01 00:49:14.878,,,false,,,,,,,,,,,,,,,,,150623,,,Tue Feb 01 15:02:19 UTC 2011,,,,,,,"0|i0ru3j:",160520,,,,,,,,,,,,,,,,"01/Feb/11 00:49;psteitz;What fields, exactly?","01/Feb/11 01:01;sebb;{code}
/** Singleton TTest instance using default implementation. */
private static TTest tTest = new TTestImpl();

/** Singleton ChiSquareTest instance using default implementation. */
private static ChiSquareTest chiSquareTest =
        new ChiSquareTestImpl();

/** Singleton ChiSquareTest instance using default implementation. */
private static UnknownDistributionChiSquareTest unknownDistributionChiSquareTest =
        new ChiSquareTestImpl();

/** Singleton OneWayAnova instance using default implementation. */
private static OneWayAnova oneWayAnova =
        new OneWayAnovaImpl();
{code}

All of the above may be changed by set methods. There is no synch.","01/Feb/11 01:12;psteitz;OK, I was looking at the wrong TestUtils :)

The reason for this strange-looking setup is to allow the implementations to be pluggable at runtime.  ""Hostile"" is a harsh word, but this class is certainly *not* threadsafe.  Ideas / patches to achieve the design goal with less ""hostility"" would be appreciated.

I would have to double-check, but I don't think that there is any test instance state used by the methods in this class. ","01/Feb/11 02:33;sebb;By thread-hostile, I mean that it is not possible in general for two different threads to use the class safely.
If one thread changes any of the static fields, there is no way of knowing how the methods called by the other thread will behave. This is partly because the values are not safely published currently, but even if they were, the threads don't know what settings will be used as they can be changed at any time by another thread.

In general, any class which relies on mutable static state for its behaviour is thread-hostile.
The shared state cannot simultaneously satisfy two threads needing different behaviour.

I think the only safe way for two threads to use the class as it stands is if they both synchronize on the class.
This will ensure safe publication of any field changes, and enforce serial usage which can guarantee the setting that will be used (but the lock will have to be held for the set call as well).

ChiSquareTestImpl has a non-final instance field which means its value won't necessarily be safely published.
The field also has a setter which could be invoked by one thread while another was using it.

TTestImpl is immutable (has no fields), and OneWayAnovaImpl can be made immutable, but other implementations of the interfaces might exist which are not immutable.

The simplest way to make the class thread-safe would be to convert all the methods and fields from static to instance, but I don't know if that is acceptable.","01/Feb/11 05:14;psteitz;Making the methods instance sort of defeats the purpose of the class.  None of the instance data in any of the static singletons is actually used or depended on by the methods of this class.  You are correct though that if one thread changes the impl for one of the singletons while another is using the class, the other could see a different than expected impl.  I think the practical likelihood of this is pretty much nil, as it is hard to imagine an application supplying two different implementations for the tests and wanting different threads to use different impls.  Personally, I would be happy just documenting the fact that the class is not threadsafe and if concurrent threads want to plug in different implementations, they need to synchronize on the class.  If this is not acceptable, my next preference would be to remove the pluggability - i.e., make the singletons final or get rid of them altogether, creating instances as needed for static method calls.  There is no initialization overhead creating the test classes.","01/Feb/11 07:15;joehni;@Phil: Please also keep in mind that M3 supports now (currently optional) parallel execution and it might be no longer a proper assumption that all tests are executed serially.","01/Feb/11 12:53;sebb;There is another possible option, which would be to fix the default implementations, and create new static methods that took an extra parameter for the implementation to be used.

At present, changes to the static fields are not guaranteed to be published correctly. Making them volatile would fix this, but would not help with concurrent access.","01/Feb/11 15:02;psteitz;Thanks, Joerg.  There should be no problems with the unit tests unless and until we introduce different tests that actually test the pluggability.  

I thought about the additional parameter option, Sebb; but that again defeats the purpose of this ""convenience class"" - you might as well just instantiate the implementation and use it.

I think the best solution is to just make the fields final and drop the getters and setters.  This is consistent with StatUtils.  So we should document the ""hostility"" issues in 2.2 and deprecate there and drop in 3.0.",,,,,,,,,,,,,,,,,,,,,,,
ValueServer not deterministic for a fixed random number seed,MATH-654,12520774,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,d.james,d.james,30/Aug/11 17:23,24/Mar/12 16:16,20/Mar/20 20:32,01/Sep/11 00:14,1.1,1.2,2.0,2.1,2.2,,,3.0,,0,,,,,,,,"I have built an agent-based model using the Apache Commons Math library, which has come in handy.

The ValueServer seemed particularly helpful, as explained at:
http://commons.apache.org/math/userguide/random.html

My simulation needs repeatable randomness, so I used this form of the ValueServer constructor:

    ValueServer(RandomData randomData) 
    Construct a ValueServer instance using a RandomData as its source of random data.
    // http://commons.apache.org/math/api-2.2/org/apache/commons/math/random/ValueServer.html

However, in my simulation, I found that the ValueServer did not act deterministically if I supplied the same random number seed.

I have not inspected the source code, but I suspect that the ValueServer is not using the `randomData` generator correctly. If it was, then it should be deterministic.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-08-30 19:00:38.698,,,false,,,,,,,,,,,,,,,,,62977,,,Thu Sep 01 00:14:02 UTC 2011,,,,,,,"0|i0ap0f:",60325,,,,,,,,,,,,,,,,"30/Aug/11 19:00;psteitz;Thanks for reporting this.  I assume you are using DIGEST_MODE.  If this is the case and you are comfortable compiling the code in trunk, the fix for MATH-634 enables a workaround for this.  Using the reseed method added to EmpiricalDistributionImpl in trunk, you can use ValueServer's getEmpiricalDistribution to get the distribution and then invoke reseed.  Unfortunately, this method does not exist in any released version yet.

The problem is that ValueServer#getNextDigest (what it does for getNext in DIGEST_MODE) delegates to EmpiricalDistributionImpl#getNextValue.  EmpiricalDistributionImpl has its own RandomData instance.  To fix this issue, EmpiricalDistirbutionImpl should add a constructor taking a RandomData and ValueServer should provide this.","01/Sep/11 00:14;psteitz;Fixed in r1163875. ValueServer now exposes a reSeed method that when supplied a fixed seed will generate a fixed sequence in any stochastic mode. The RandomDataImpl that it uses internally is passed to the EmpiricalDistributionImpl it creates when used in DIGEST_MODE.  The changes for this issue include an incompatible (vs. 2.x) change: the constructor for EmpiricalDistributionImpl that previously took a RandomData now takes a RandomDataImpl.  The plan for 3.0 is to merge these.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same",MATH-618,12513980,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,13/Jul/11 20:23,24/Mar/12 16:16,20/Mar/20 20:32,14/Jul/11 06:08,1.2,2.0,2.1,2.2,,,,3.0,,0,,,,,,,,"For both Complex add and subtract, the javadoc states that

{code}
     * If either this or <code>rhs</code> has a NaN value in either part,
     * {@link #NaN} is returned; otherwise Inifinite and NaN values are
     * returned in the parts of the result according to the rules for
     * {@link java.lang.Double} arithmetic
{code}

Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,68108,,,Thu Jul 14 06:08:54 UTC 2011,,,,,,,"0|i0ap53:",60346,,,,,,,,,,,,,,,,"14/Jul/11 06:08;psteitz;Fixed in r1146573",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RandomDataImpl.nextInt does not distribute uniformly for negative lower bound,MATH-724,12534797,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,dhendriks,dhendriks,12/Dec/11 15:03,24/Mar/12 16:16,20/Mar/20 20:32,20/Dec/11 21:14,1.0,1.1,1.2,2.0,2.1,2.2,,3.0,,0,,,,,,,,"When using the RandomDataImpl.nextInt function to get a uniform sample in a [lower, upper] interval, when the lower value is less than zero, the output is not uniformly distributed, as the lowest value is practically never returned.

See the attached NextIntUniformTest.java file. It uses a [-3, 5] interval. For several values between 0 and 1, testNextIntUniform1 prints the return value of RandomDataImpl.nextInt (as double and as int). We see that -2 through 5 are returned several times. The -3 value however, is only returned for 0.0, and is thus under-respresented in the integer samples. The output of test method testNextIntUniform2 also clearly shows that value -3 is never sampled.",,,,,,,,,,,,,"15/Dec/11 08:23;dhendriks;NextIntTest3.java;https://issues.apache.org/jira/secure/attachment/12507485/NextIntTest3.java","12/Dec/11 15:04;dhendriks;NextIntUniformTest.java;https://issues.apache.org/jira/secure/attachment/12507005/NextIntUniformTest.java","15/Dec/11 08:23;dhendriks;NextUniformTest3.java;https://issues.apache.org/jira/secure/attachment/12507486/NextUniformTest3.java","14/Dec/11 14:16;dhendriks;math-724-v2.patch;https://issues.apache.org/jira/secure/attachment/12507355/math-724-v2.patch","15/Dec/11 08:23;dhendriks;math-724-v3.patch;https://issues.apache.org/jira/secure/attachment/12507487/math-724-v3.patch","13/Dec/11 08:23;dhendriks;math-724.patch;https://issues.apache.org/jira/secure/attachment/12507147/math-724.patch",,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,2011-12-12 16:49:17.093,,,false,,,,,,,,,,,,,,,,,220481,,,Tue Dec 20 21:14:16 UTC 2011,,,,,,,"0|i0aopj:",60276,,,,,,,,,,,,,,,,"12/Dec/11 15:04;dhendriks;NextIntUniformTest.java: see issue description","12/Dec/11 16:49;psteitz;Thanks for reporting this. The problem is in the rounding, which does not work correctly for negative values.  My first inclination is to test for negative lower bound and just shift the interval in that case.  Any better ideas?","13/Dec/11 08:23;dhendriks;math-724.patch: it first scales the [0..1) interval to [0..length), then discretizes it, and finally shifts it to [lower, upper].

It may be a good idea to also add some tests for cases such as [0,3], [3,5], [-3,5], [-5, -3], and see if the distribution of sampled values is uniform. It seems RandomDataTest.testNextInt does this using chiSquare, but since I'm not familiar with that, I'm not sure how to add more tests for the other lower/upper bound pairs...","13/Dec/11 08:33;dhendriks;I just ran the unit tests with my patch applied, an the following test, in RandomDataTest:

{code:java}
    @Test
    public void testNextIntExtremeValues() {
        int x = randomData.nextInt(Integer.MIN_VALUE, Integer.MAX_VALUE);
        int y = randomData.nextInt(Integer.MIN_VALUE, Integer.MAX_VALUE);
        Assert.assertFalse(x == y);
    }
{code}

fails, as does testNextLongExtremeValues. Both x and y become equal to Integer.MIN_VALUE, making x == y to become true, causing the assertion to fail...","13/Dec/11 09:14;dhendriks;Also note that RandomDataImpl.nextUniform uses a similar scale/shift method to transform the range. It may thus suffer from the same failure in case of extreme values...","14/Dec/11 14:16;dhendriks;math-724-v2.patch: 2nd patch.

 - I think all unit tests work now, including the ones for the Integer.MIN_VALUE to Integer.MAX_VALUE interval.
 - The original problem was that negative values were rounded up by the conversion from double to int, while positive numbers were rounded down. By using floor, we first round the numbers down, and then convert to integer, thus ensuring a proper uniform distribution.
 - Test cases for negative values are still missing... Could someone else add them?
 - RandomDataImpl.nextUniform: I haven't changed this, as the change that I used for integers does not have the desired effect for doubles... This may be caused by the fact that Double.MIN_VALUE is more negative than Double.MAX_VALUE is positive, but I'm not really sure. Maybe it is not even an issue for the nextUniform method?
","14/Dec/11 14:41;erans;bq. [...] the fact that Double.MIN_VALUE is more negative [...]

[Double.Min_VALUE|http://docs.oracle.com/javase/6/docs/api/java/lang/Double.html#MIN_VALUE] is a _positive_ number.
","15/Dec/11 08:23;dhendriks;bq. Double.Min_VALUE is a positive number.

Oops...

OK, I uploaded a third version of the patch (math-724-v3.patch), which also applies the new formula for nextUniform. I included two test files (NextUniformTest3.java and NextIntTest3.java), that show the results for nextInt and nextUniform, for both the old and new formulas. As for as I can see, the new formula works equally well or better in all cases. Also, all existing unit tests pass.
","20/Dec/11 21:14;psteitz;Thanks for reporting and diagnosing this, Dennis.

Slightly modified version of the third patch (just removing unecessary parens), along with tests, committed in r1221490.  The ""negativeToPositiveRange"" tests fail before the fix.  The change to nextUniform is also needed to prevent overflows. I changed the relevant test cases to use the TestUtils chisquare test, which is more straightforward and has better output.  This was added after the original versions of these tests were written.  Others in this class should be similarly updated.  Patches welcome to further tidy the tests, but this issue can be resolved.",,,,,,,,,,,,,,,,,,,,,,
NaN singular value from SVD,MATH-320,12440296,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,dieterv77,dieterv77,10/Nov/09 15:42,23/Jun/11 20:02,20/Mar/20 20:32,31/Dec/09 17:55,2.0,,,,,,,2.1,,0,,,,,,,,"The following jython code
Start code

from org.apache.commons.math.linear import *
 
Alist = [[1.0, 2.0, 3.0],[2.0,3.0,4.0],[3.0,5.0,7.0]]
 
A = Array2DRowRealMatrix(Alist)
 
decomp = SingularValueDecompositionImpl(A)
 
print decomp.getSingularValues()

End code

prints
array('d', [11.218599757513008, 0.3781791648535976, nan])
The last singular value should be something very close to 0 since the matrix
is rank deficient.  When i use the result from getSolver() to solve a system, i end 
up with a bunch of NaNs in the solution.  I assumed i would get back a least squares solution.

Does this SVD implementation require that the matrix be full rank?  If so, then i would expect
an exception to be thrown from the constructor or one of the methods.


","Linux (Ubuntu 9.10) java version ""1.6.0_16""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-11-10 20:23:02.883,,,false,,,,,,,,,,,,,,,,,34206,,,Thu Jun 23 20:02:56 UTC 2011,,,,,,,"0|i0rv6n:",160696,,,,,,,,,,,,,,,,"10/Nov/09 20:23;luc;This is a real new bug, thanks for reporting it.

Before I look more precisely at it, could you do a quick check for me ?

If at the end of the SingularValueDecompositionImpl constructor, around line 118 in the java source file you change from:
{noformat}
  singularValues[i] = Math.sqrt(singularValues[i]);
{noformat}
to
{noformat}
  singularValues[i] = Math.sqrt(Math.max(0, singularValues[i]));
{noformat}

does the problem still appear on singular values and does the solver work properly ?","11/Nov/09 13:37;dieterv77;Yes, making that change fixes the singular values, printing the singular values now gives

array('d', [11.218599757513008, 0.3781791648535976, 0.0])

The unittests for the project still pass as well.

However, now the solve fails with a SinularMatrixException

Traceback (most recent call last):
  File ""testdecomp.py"", line 14, in <module>
    soln = solver.solve([5.0, 6.0,7.0])
	at org.apache.commons.math.linear.SingularValueDecompositionImpl$Solver.solve(SingularValueDecompositionImpl.java:371)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)

org.apache.commons.math.linear.SingularMatrixException: org.apache.commons.math.linear.SingularMatrixException: matrix is singular

This confuses me, i guess i'm assuming incorrectly that if the solve method can solve in the least squares sense, then it should be
able to handle singular matrices.  Is that just a restriction on the current solve methods and if so, are there plans to relax that restriction?

thanks very much for your time","11/Nov/09 14:21;luc;The method should solve the problem in the least square sense.
The fact it does not do it is not a restriction, it's a bug.
I'll have a look at it","17/Dec/09 18:43;axelclk;
Is this a similar problem for the getU() method?

  public void testU() {
      double[][] testMatrix = {
          { 1.0 , 2.0 },
          { 1.0 , 2.0 } };
      SingularValueDecompositionImpl svd =
          new SingularValueDecompositionImpl(MatrixUtils.createRealMatrix(testMatrix));
// wrong result:
      assertEquals(""Array2DRowRealMatrix{{-0.7071067811865472,NaN},{-0.7071067811865475,NaN}}"", svd.getU().toString());
    }","17/Dec/09 21:21;luc;The two issues are probably related.
I'll look at both cases.","30/Dec/09 23:27;luc;A first round on fixing this bug has been committed in the subversion repository as of r894735.
Axel example is confirmed to be an occurrence of the same bug as Dieter example.

The SVD is now computed either as a compact SVD (only positive singular values considered) or as a truncated SVD
(max number of singular values to consider is user-specified). The solver simply applies the pseudo-inverse.

The fix is not considered complete yet because I think that the results provided by the solver are not really the ones
that give the smallest residuals. See for example the commented out parts of testMath320A in
SingularValueSolverTest.  Could you check this, please ?","31/Dec/09 11:57;axelclk;This statement should print the values of the original matrix approximately:
{code:java} 
  System.out.println(svd.getU().multiply(svd.getS()).multiply(svd.getVT()));
{code} 

This is true for
{code:java} 
    public void testMath320A() {
{code} 
but not for
{code:java} 
    public void testMath320B() {
{code} 

For reference values try wolfram alpha:
N[SingularValueDecomposition[{{1,2},{1,2}}]]","31/Dec/09 14:47;luc;Thanks for the hint Axel!
The print statement is even not satisfying for testMath320A, the approximation is really too bad. I would expect about 13 exact figures, not 1 or 2.
The problem seems to be related to matrix U which is not correct. In fact, it is even not unitary (i.e. U^T^.U is not the identity matrix).
I'll look at this.","31/Dec/09 17:55;luc;This should be fixed in subversion repository now (r894908).
Thanks for reporting the bug and sorry for the delay.","23/Jun/11 20:02;gsteri1;Did anyone notice that the 3rd eigenvalue is negative? On my box the eigenvalue is -2.1028862676867717E-14. I am not sure what the fix was, but whatever problems existed still persist. ",,,,,,,,,,,,,,,,,,,,,
Multiple Regression newSampleData methods inconsistently create / omit intercepts,MATH-411,12472796,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,28/Aug/10 22:14,23/Mar/11 20:20,20/Mar/20 20:32,13/Sep/10 02:04,2.0,2.1,,,,,,2.2,,0,,,,,,,,"The newSampleData(double[], nrows, ncols) method used in the unit tests adds a unitary column to the design matrix, resulting in an intercept term being estimated among the regression parameters.  The other newSampleData methods do not do this, forcing users to add the column of ""1""s to estimate models with intercept.  Behavior should be consistent and users should not have to add the column.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-03-23 20:20:06.182,,,false,,,,,,,,,,,,,,,,,150558,,,Wed Mar 23 20:20:06 UTC 2011,,,,,,,"0|i0rumv:",160607,,,,,,,,,,,,,,,,"08/Sep/10 01:30;psteitz;Fixed in r993574.  Modified multiple regression newSample methods to ensure that by default in all cases, regression models are estimated with intercept terms.  Prior to the fix for this issue,  newXSampleData(double[][]), newSampleData(double[], double[][]) and newSampleData(double[], double[][], double[][]) all required columns of ""1's  to be inserted into the x[][] arrays to create a model with an intercept term;while newSampleData(double[], int, int) created a model including an intercept term without requiring the unitary column.  All methods have  been changed to eliminate the need for users to add unitary columns to specify regression models.

Leaving open until MATH-409 is resolved. 
","23/Mar/11 20:20;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Regression API should allow specification of whether or not to estimate intercept term,MATH-409,12472346,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,24/Aug/10 09:55,23/Mar/11 20:19,20/Mar/20 20:32,13/Sep/10 02:02,2.0,2.1,,,,,,2.2,,0,,,,,,,,The OLS and GLS regression APIs should support estimating models including intercepts using design matrices including only variable data.,,,,,,,,,,MATH-411,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-03-23 20:19:13.668,,,false,,,,,,,,,,,,,,,,,150556,,,Wed Mar 23 20:19:13 UTC 2011,,,,,,,"0|i0runb:",160609,,,,,,,,,,,,,,,,"13/Sep/10 02:02;psteitz;Fixed in r996404 (both trunk and 2.x branch)","23/Mar/11 20:19;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GLSMultipleLinearRegression has no nontrivial validation tests,MATH-408,12472222,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,23/Aug/10 03:11,23/Mar/11 20:18,20/Mar/20 20:32,12/Dec/10 21:49,2.0,2.1,,,,,,2.2,,0,,,,,,,,"There are no non-trivial tests verifying the computations for GLSMultipleLinearRegression.  Tests verifying computations against analytically determined models, R or some other reference package / datasets should be added to ensure that the statistics reported by this class are valid.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-03-23 20:18:48.796,,,false,,,,,,,,,,,,,,,,,150555,,,Wed Mar 23 20:18:48 UTC 2011,,,,,,,"0|i0runj:",160610,,,,,,,,,,,,,,,,"12/Dec/10 21:49;psteitz;Added a non-trivial test in r1044935.  While still not really a full verification test, it does at least verify that the GLS impl provided does better than OLS for models with error structure conforming to its covariance matrix.","23/Mar/11 20:18;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation improvements for multiple regression classes,MATH-407,12472221,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,23/Aug/10 03:07,23/Mar/11 20:18,20/Mar/20 20:32,20/Sep/10 01:57,2.0,2.1,,,,,,2.2,,0,,,,,,,,The user guide examples showing how to set up and estimate linear models using OLS and GLS multiple regression need to be updated to reflect changes in the API.  The javadoc for these classes and user guide descriptions also need to be improved to make it clear how to estimate a model with an intercept term.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-03-23 20:18:29.971,,,false,,,,,,,,,,,,,,,,,150554,,,Wed Mar 23 20:18:29 UTC 2011,,,,,,,"0|i0runr:",160611,,,,,,,,,,,,,,,,"20/Sep/10 01:57;psteitz;Fixed in r998761","23/Mar/11 20:18;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PearsonsCorrelation.getCorrelationPValues() precision limited by machine epsilon,MATH-371,12464458,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,kchilds,kchilds,13/May/10 19:48,23/Mar/11 20:05,20/Mar/20 20:32,16/May/10 23:49,2.0,,,,,,,2.1,,0,,,,,,,,"Similar to the issue described in MATH-201, using PearsonsCorrelation.getCorrelationPValues() with many treatments results in p-values that are continuous down to 2.2e-16 but that drop to 0 after that.

In MATH-201, the problem was described as such:
> So in essence, the p-value returned by TTestImpl.tTest() is:
> 
> 1.0 - (cumulativeProbability(t) - cumulativeProbabily(-t))
> 
> For large-ish t-statistics, cumulativeProbabilty(-t) can get quite small, and cumulativeProbabilty(t) can get very close to 1.0. When 
> cumulativeProbability(-t) is less than the machine epsilon, we get p-values equal to zero because:
> 
> 1.0 - 1.0 + 0.0 = 0.0

The solution in MATH-201 was to modify the p-value calculation to this:
> p = 2.0 * cumulativeProbability(-t)

Here, the problem is similar.  From PearsonsCorrelation.getCorrelationPValues():
  p = 2 * (1 - tDistribution.cumulativeProbability(t));

Directly calculating the p-value using identical code as PearsonsCorrelation.getCorrelationPValues(), but with the following change seems to solve the problem:
  p = 2 * (tDistribution.cumulativeProbability(-t));




",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-05-16 14:10:39.057,,,false,,,,,,,,,,,,,,,,,34169,,,Wed Mar 23 20:05:43 UTC 2011,,,,,,,"0|i0ruvr:",160647,,,,,,,,,,,,,,,,"16/May/10 14:10;psteitz;Thanks for reporting this.","16/May/10 23:49;psteitz;Fixed in r944939.  Thanks!","23/Mar/11 20:05;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,
LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it,MATH-362,12461240,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,roman.werpachowski,roman.werpachowski,06/Apr/10 11:38,23/Mar/11 20:02,20/Mar/20 20:32,29/May/10 18:16,2.0,2.1,,,,,,2.2,,0,,,,,,,,LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it. This makes it hard to specify custom stopping criteria for the optimizer.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-04-06 16:49:36.856,,,false,,,,,,,,,,,,,,,,,34181,,,Wed Mar 23 20:02:00 UTC 2011,,,,,,,"0|i0ruxr:",160656,,,,,,,,,,,,,,,,"06/Apr/10 16:49;luc;Ooops. You are right.
The Levenberg-Marquardt optimizer uses specific convergence parameters which can be set by   setInitialStepBoundFactor, setCostRelativeTolerance, setParRelativeTolerance and setOrthoTolerance.
The most important convergence tuning are either setCostRelativeTolerance for a convergence on the cost itself or setParRelativeTolerance for a convergence on the parameters.

I'm not sure how to solve this. Do the existing tuning parameters fit your needs or not ? Some convergence criteria can be expressed with both methods, but not all. Should we keep both setting as alternate methods or should we remove one and rely on the remaining one ?
","06/Apr/10 17:20;roman.werpachowski;I would keep using orthoTolerance as it is used now:

{quote}
292                if (maxCosine <= orthoTolerance) \{
293                    // convergence has been reached
294                    return new VectorialPointValuePair(point, objective);
295                \}
{quote}

and then use costRelativeTolerance & parRelativeTolerance if and only if the convergence checker is null, otherwise use the convergence checker and ignore {costRelativeTolerance, parRelativeTolerance}.

What I am missing now is the ability to bail out if the absolute distance from the target falls below some value (""close enough"").","24/May/10 20:50;mprice;I've spent that last few days trying to find a good curve fitting library for Java and got excited when I learned of Commons Math.  Unfortunately, its curve fitting is very unreliable.  I'm hoping that this bug is what is causing the problems that I'm seeing.  I'm comparing data from NIST and results from DataFitX and it is apparent that Commons Math is not yet up to the task.  My fingers are crossed that its quality in the curve fitting area will be improved in the near future.  Keep up the good work Apache.

I've opened an issue about the problems I'm seeing, https://issues.apache.org/jira/browse/MATH-372","25/May/10 06:37;roman.werpachowski;Double check how you use it, Matt. I have succesfully used this curve fitting in production.","25/May/10 17:49;luc;Matt, could you please describe the problem you encounter more precisely (i.e. with numerical examples) and preferably in a new JIRA issue ? We will check if the two problems are related and link the issues afterwards if it appears they are.

Thanks","25/May/10 19:10;mprice;It's good to see such quick responses.  I'll open a new JIRA issue and spend some time putting together code, data and a detailed description of the problem I'm seeing.  Thanks Apache for all your hard work.

I've opened an issue regarding the problem, https://issues.apache.org/jira/browse/MATH-372","29/May/10 18:16;luc;Fixed in subversion repository as of r949433.
Thanks for reporting the issue","29/May/10 18:27;roman.werpachowski;Thank you.","23/Mar/11 20:02;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,
SimplexSolver fails to solve feasible problem instance ,MATH-351,12458608,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,thomamark,thomamark,10/Mar/10 02:47,23/Mar/11 20:00,20/Mar/20 20:32,28/Sep/10 06:43,2.0,,,,,,,2.2,,0,linear_programming,simplex,,,,,,"SimplexSolver throws an UnboundedSolutionException on a problem instance I can optimally solve with Excel's Solver. I've kept the parameters between the two programs the same as far as I can tell  (i.e. both have a precision/epsilon value of 1e-6 and a maxIterations value of 1000). I will attach a JUnit test  with an example problem on which SimplexSolver fails. I will also attach an Excel spreadsheet wtih the same data and successful Solver setup in place.

I don't know a whole lot about linear programming or Simplex, but the problem I'm attempting to solve does appear to have a fairly sparse coefficient matrix, which may be part of the problem.

It's surprisingly difficult to find a Java-based linear programming library, so I was ecstatic when I found this. Let me know how I can help!

Thanks!","Windows Vista Home Premium Version 6.0 Service Pack 1, Build 6001",,,,,,,,,,,,"01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image001.wmz;https://issues.apache.org/jira/secure/attachment/12440453/ASF.LICENSE.NOT.GRANTED--image001.wmz","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image017.gif;https://issues.apache.org/jira/secure/attachment/12440454/ASF.LICENSE.NOT.GRANTED--image017.gif","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image018.wmz;https://issues.apache.org/jira/secure/attachment/12440455/ASF.LICENSE.NOT.GRANTED--image018.wmz","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image019.gif;https://issues.apache.org/jira/secure/attachment/12440456/ASF.LICENSE.NOT.GRANTED--image019.gif","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image020.wmz;https://issues.apache.org/jira/secure/attachment/12440457/ASF.LICENSE.NOT.GRANTED--image020.wmz","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image021.gif;https://issues.apache.org/jira/secure/attachment/12440458/ASF.LICENSE.NOT.GRANTED--image021.gif","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image022.wmz;https://issues.apache.org/jira/secure/attachment/12440459/ASF.LICENSE.NOT.GRANTED--image022.wmz","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image023.gif;https://issues.apache.org/jira/secure/attachment/12440460/ASF.LICENSE.NOT.GRANTED--image023.gif","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image024.wmz;https://issues.apache.org/jira/secure/attachment/12440461/ASF.LICENSE.NOT.GRANTED--image024.wmz","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image025.gif;https://issues.apache.org/jira/secure/attachment/12440462/ASF.LICENSE.NOT.GRANTED--image025.gif","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image026.wmz;https://issues.apache.org/jira/secure/attachment/12440463/ASF.LICENSE.NOT.GRANTED--image026.wmz","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image027.gif;https://issues.apache.org/jira/secure/attachment/12440464/ASF.LICENSE.NOT.GRANTED--image027.gif","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image028.wmz;https://issues.apache.org/jira/secure/attachment/12440465/ASF.LICENSE.NOT.GRANTED--image028.wmz","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image029.gif;https://issues.apache.org/jira/secure/attachment/12440466/ASF.LICENSE.NOT.GRANTED--image029.gif","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image030.wmz;https://issues.apache.org/jira/secure/attachment/12440467/ASF.LICENSE.NOT.GRANTED--image030.wmz","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image031.gif;https://issues.apache.org/jira/secure/attachment/12440468/ASF.LICENSE.NOT.GRANTED--image031.gif","01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--oledata.mso;https://issues.apache.org/jira/secure/attachment/12440469/ASF.LICENSE.NOT.GRANTED--oledata.mso","10/Mar/10 02:52;thomamark;SimplexFail.xlsx;https://issues.apache.org/jira/secure/attachment/12438360/SimplexFail.xlsx","10/Mar/10 02:52;thomamark;TestSimplexFail.java;https://issues.apache.org/jira/secure/attachment/12438359/TestSimplexFail.java",,19.0,,,,,,,,,,,,,,,,,,,,2010-03-31 08:41:25.099,,,false,,,,,,,,,,,,,,,,,150519,,,Wed Mar 23 20:00:48 UTC 2011,,,,,,,"0|i0ruzz:",160666,,,,,,,,,,,,,,,,"10/Mar/10 02:52;thomamark;The JUnit test showing an example of a problem which fails with an UnboundedSolutionException when using SimplexSolver and the Excel spreadsheet containing the optimal solution. The two together prove (I believe) that there is something wrong with SimplexSolver.","31/Mar/10 08:41;jtas;To put it simply, you have defined a problem of the following form:

max (0*x1 + 0*x2 + 0*x3 +...) subject to the following constraints:

x1 + x2 + ... >= some number

x2 + x3 + ... >= some number

etc...(you have defined inequalities of a similar form)

x1>=0, x2>=0,....

Ofcourse such a problem is unbounded; i.e. the solution is x1 = infinitiy, x2 = infinity,.... etc..
Maybe I don't understand the problem well, but to me it seems that the Excel program gives a wrong answer.

Jurgen
","31/Mar/10 08:59;jtas;I see what you have done wrong in your Java code. The coefficients in your objective function are all zero, while they need to be all one. If you change this, you get the same answer as Excel.

Jurgen","31/Mar/10 16:27;thomamark;Good catch. Not sure how I missed this. I'm still getting an UnboundedSolutionException, even after I set the coefficients all to 1, however. Note that I'm still using the 2.0 code which was noted above to have a bug in it. Are you using 2.1?

Thanks Jurgen!

--Mark","01/Apr/10 07:38;jtas;You are welcome!

 

I am using the trunk version of the code. I also found that 2.0 contained some errors. In the new versions I have not found anything strange. 

 

Interesting to me is the use of epsilon. I have found that the default value of 1e-6 works fine for most problems. However, consider the following problem:

 

 

 

The answer to this problem is trivial to find; i.e.  and .  However, for the case when we get a wrong answer; i.e. for   we find that and  (the second constraint is not satisfied), and even for  no feasible solution could be found. 

 

Too me the results of this very simple problem are obvious. But how do you determine the threshold for epsilon for general problems? Do you analyze the condition number of the constraints matrix?

 

Jurgen

","01/Apr/10 18:19;luc;Jurgen, your last comment on this issue is impossible to read, the images containing the equations are not inlined in the text (at least not with firefox on Linux). Could you rewrite it in simple raw text ?

Mark, do you agree to close the issue as invalid ?","01/Apr/10 20:38;thomamark;I haven't had a chance yet to verify for myself that this works on 2.1, but if it works for you, Luc and you consider the epsilon discussion a separate issue, I'm happy with closing this as invalid (especially if it will lead to an earlier release of 2.1 =).

For other readers, it appears that there were bug fixes for Simplex which weren't in 2.0, but have been incorporated into the trunk (see: http://issues.apache.org/jira/browse/MATH-302). If this is so, we'll want to leave this link in.

Thanks guys and sorry for the mix up.
Mark","28/Sep/10 06:43;luc;solving as invalid as per last comments","23/Mar/11 20:00;luc;Closing issue which was declared invalid long ago",,,,,,,,,,,,,,,,,,,,,,
Complex.ZERO.pow(Complex.ONE) gives NaN in unit tests,MATH-402,12470918,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Won't Fix,,axelclk,axelclk,05/Aug/10 16:39,01/Jan/11 01:26,20/Mar/20 20:32,01/Jan/11 01:26,1.2,2.0,2.1,,,,,2.2,,0,,,,,,,,"Why does this unit test in ComplexTest.java gives NaN?
I expected to get Complex.ZERO as the result?

{code:java} 
   public void testPowZero() {
       TestUtils.assertSame(Complex.NaN,
               Complex.ZERO.pow(Complex.ONE));
...
   }
{code} 

I would suggest something like this for the Complex#pow() method:
{code:java} 
    public Complex pow(Complex x) {
        if (x == null) {
            throw new NullPointerException();
        }
        if (x.imaginary == 0.0) {
          if (real == 0.0 && imaginary == 0.0) {
            if (x.real == 0.0){      	
            	return Complex.ZERO;
            }
          }
          if (x.real == 1.0) {
          	return this;
          }
        }
        return this.log().multiply(x).exp();
    }
{code} 
 ",Issue 15 http://code.google.com/p/symja/issues/detail?id=15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-08-08 12:15:04.237,,,false,,,,,,,,,,,,,,,,,34170,,,Thu Dec 30 14:49:35 UTC 2010,,,,,,,"0|i0ruov:",160616,,,,,,,,,,,,,,,,"07/Aug/10 11:25;axelclk;Sorry for my stupid suggestion in the first post.

It should be something like this:
{code:java}
	public Complex pow(Complex x) {
		if (x == null) {
			throw new NullPointerException();
		}
		if (x.real == 0.0 && x.imaginary == 0.0) {
			if (real == 0.0 && imaginary == 0.0) {
				// 0^0 => NaN
				return Complex.NaN;
			}
			// THIS^0 => 1 for THIS<>0
			return Complex.ONE;
		}
		if (real == 0.0 && imaginary == 0.0) {
			// 0^x => 0 for x<>0
			return Complex.ZERO;
		}
		return this.log().multiply(x).exp();
	}
{code}","08/Aug/10 12:15;psteitz;Thanks for reporting this. If we do make the suggested change, we need to change the formulas in the javadoc to reflect the changed definition.  The value returned is a consequence of how we have defined complex exponentiation, which is spelled out in the javadoc for pow, exp and log.   I am open to changing the definition, but we should make sure that the javadoc is modified to reflect whatever change we make.  Interested in what others think the definition should be.","08/Aug/10 12:26;luc;I know we do not follow C99 in many aspects, but what does C99 standard say about this specific topic ?","08/Aug/10 14:58;psteitz;The latest draft ISO C spec appears now to be public and is here:
﻿http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1494.pdf
The complex specifications are in Annex G.  Does not look like anything has changed since C99x, though I have not done side-by-side comparisons.  It looks to me like what we have in the code now matches the spec; but the spec does not specifically call out (unless I missed it) specialization to real arguments.  What is causing the NaN is the singularity in the log function.   Colt 1.1 looks like it matches our code.  R returns  0 + 0i.
﻿﻿﻿","12/Dec/10 21:51;psteitz;I am leaning toward closing as WONTFIX.  Going once, going twice...","30/Dec/10 14:49;mikl;I'm also leaning towards agreeing with Phil. It also seems like the user itself has implemented the change (see http://code.google.com/p/symja/issues/detail?id=15). Another option is to postpone to 3.0?",,,,,,,,,,,,,,,,,,,,,,,,,
Erf(z) should return 1.0 for z 'large' but  fails with a MaxIterationsExceededException for z > 26.0.,MATH-301,12437655,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,milac,milac,08/Oct/09 22:47,09/Dec/10 11:48,20/Mar/20 20:32,08/Mar/10 22:59,2.0,,,,,,,2.1,,0,,,,,,,,"Erf(z) should return 1.0 for z 'large' but fails with a MaxIterationsExceededException for z > 26.0.

Sample code
-----------------

import org.apache.commons.math.MathException;
import org.apache.commons.math.special.Erf;

public class TestErf {

    public TestErf() {
    }

    public static void main(String[] args) {
        double z = Double.NEGATIVE_INFINITY;
        try {
            for(int i=0; i<100; i++) {
                z = i;
                System.out.println(""z = "" + z + ""  erf(z) = "" + Erf.erf(z));
            }

            System.out.flush();
        } catch (MathException mex) {
            System.out.println(""z failed = "" + z);
            mex.printStackTrace();
        }
    }
}

Output
---------

z = 0.0  erf(z) = 0.0
z = 1.0  erf(z) = 0.842700792949715
z = 2.0  erf(z) = 0.9953222650189528
z = 3.0  erf(z) = 0.9999779095030024
z = 4.0  erf(z) = 0.9999999845827416
z = 5.0  erf(z) = 0.9999999999984622
z = 6.0  erf(z) = 0.9999999999999997
z = 7.0  erf(z) = 1.000000000000001
z = 8.0  erf(z) = 0.9999999999999986
z = 9.0  erf(z) = 1.000000000000003
z = 10.0  erf(z) = 1.0000000000000115
z = 11.0  erf(z) = 1.0000000000000016
z = 12.0  erf(z) = 0.9999999999999941
z = 13.0  erf(z) = 0.9999999999999846
z = 14.0  erf(z) = 1.0000000000000024
z = 15.0  erf(z) = 0.9999999999999805
z = 16.0  erf(z) = 0.9999999999999988
z = 17.0  erf(z) = 0.9999999999999949
z = 18.0  erf(z) = 0.9999999999999907
z = 19.0  erf(z) = 0.9999999999999731
z = 20.0  erf(z) = 0.9999999999999862
z = 21.0  erf(z) = 0.9999999999999721
z = 22.0  erf(z) = 1.000000000000017
z = 23.0  erf(z) = 1.0000000000000577
z = 24.0  erf(z) = 1.000000000000054
z = 25.0  erf(z) = 1.0000000000000262
z = 26.0  erf(z) = 1.0000000000000735
z failed = 27.0
org.apache.commons.math.MaxIterationsExceededException: Maximal number of iterations (10,000) exceeded
        at org.apache.commons.math.special.Gamma.regularizedGammaP(Gamma.java:181)
        at org.apache.commons.math.special.Erf.erf(Erf.java:51)
        at org.fhcrc.math.minimization.TestErf.main(TestErf.java:23)",MacOS and Linux,,,,,,,,,,MATH-282,MATH-456,"29/Oct/09 21:40;njawalkar;erf.txt;https://issues.apache.org/jira/secure/attachment/12423626/erf.txt",,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2009-10-29 21:39:58.0,,,false,,,,,,,,,,,,,,,,,34208,,,Mon Mar 08 22:59:28 UTC 2010,,,,,,,"0|i0rvav:",160715,,,,,,,,,,,,,,,,"29/Oct/09 21:39;njawalkar;Hi,

Here's a patch that uses a different algorithm to calculate erf(x). It's adapted from ""Handbook of Mathematical Functions: with Formulas, Graphs, and Mathematical Tables, by Milton Abramowitz and Irene A. Stegun"".

It no longer throws a MathException if x is too large or small. I've added one test to check erf(x) for 20<=x<40, and all previous tests pass also.","15/Feb/10 18:41;psteitz;This issue and MATH-282 are both caused by problems handling extreme values in Gamma.","20/Feb/10 15:14;psteitz;From commons-user, another example of inaccurate results for extreme erf values:

NormalDistribution normDist = new NormalDistributionImpl(40,1.5)
;
try{
System.out.println(""cummulative probability::
""+normDist.cumulativeProbability(0.908789));
}
catch(MathException e){
e.printStackTrace();
}

*Result:*
cummulative probability:: *-8.104628079763643E-15*","08/Mar/10 22:59;psteitz;Fixed in r920558.",,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexSolver not working as expected?,MATH-286,12433578,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ciaccia,ciaccia,20/Aug/09 15:04,14/Apr/10 00:30,20/Mar/20 20:32,10/Sep/09 08:22,2.0,,,,,,,2.1,,0,,,,,,,,"I guess (but I could be wrong) that SimplexSolver does not always return the optimal solution, nor satisfies all the constraints...

Consider this LP:

max: 0.8 x0 + 0.2 x1 + 0.7 x2 + 0.3 x3 + 0.6 x4 + 0.4 x5;
r1: x0 + x2 + x4 = 23.0;
r2: x1 + x3 + x5 = 23.0;
r3: x0 >= 10.0;
r4: x2 >= 8.0;
r5: x4 >= 5.0;

LPSolve returns 25.8, with x0 = 10.0, x1 = 0.0, x2 = 8.0, x3 = 0.0, x4 = 5.0, x5 = 23.0;

The same LP expressed in Apache commons math is:

LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.6, 0.4 }, 0 );
Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 23.0));
constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 23.0));
constraints.add(new LinearConstraint(new double[] { 1, 0, 0, 0, 0, 0 }, Relationship.GEQ, 10.0));
constraints.add(new LinearConstraint(new double[] { 0, 0, 1, 0, 0, 0 }, Relationship.GEQ, 8.0));
constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 1, 0 }, Relationship.GEQ, 5.0));

RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);

that returns 22.20, with x0 = 15.0, x1 = 23.0, x2 = 8.0, x3 = 0.0, x4 = 0.0, x5 = 0.0;

Is it possible SimplexSolver is buggy that way? The returned value is 22.20 instead of 25.8, and the last constraint (x4 >= 5.0) is not satisfied...

Am I using the interface wrongly?",Java 1.6.0_13 on  Windows XP 32-bit,,,,,,,,,,,,"08/Sep/09 00:16;bmccann;SimplexSolver.patch;https://issues.apache.org/jira/secure/attachment/12418860/SimplexSolver.patch","09/Sep/09 12:08;ciaccia;SimplexSolverTest-Andrea.patch;https://issues.apache.org/jira/secure/attachment/12419049/SimplexSolverTest-Andrea.patch","08/Sep/09 00:16;bmccann;SimplexSolverTest.patch;https://issues.apache.org/jira/secure/attachment/12418861/SimplexSolverTest.patch","21/Aug/09 22:11;bmccann;SimplexSolverTest.patch;https://issues.apache.org/jira/secure/attachment/12417328/SimplexSolverTest.patch","08/Sep/09 00:16;bmccann;SimplexTableau.patch;https://issues.apache.org/jira/secure/attachment/12418862/SimplexTableau.patch","21/Aug/09 22:11;bmccann;SimplexTableau.patch;https://issues.apache.org/jira/secure/attachment/12417327/SimplexTableau.patch","09/Sep/09 01:54;bmccann;patch.zip;https://issues.apache.org/jira/secure/attachment/12419011/patch.zip","31/Aug/09 17:24;bmccann;simplex.txt;https://issues.apache.org/jira/secure/attachment/12418165/simplex.txt",,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2009-08-21 20:25:41.58,,,false,,,,,,,,,,,,,,,,,34144,,,Thu Sep 10 08:22:11 UTC 2009,,,,,,,"0|i0rvdz:",160729,,,,,,,,,,,,,,,,"21/Aug/09 20:25;bmccann;I have confirmed this is an issue.","21/Aug/09 20:38;bmccann;Here's a smaller version of the problem that also fails:

max 0.8 x0 + 0.2 x1 + 0.7 x2 + 0.3 x3
s.t. 
x0 + x2 = 23.0
x1 + x3 = 23.0

    @Test
    public void testMath268() throws OptimizationException {
      LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3 }, 0 );
      Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
      constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0 }, Relationship.EQ, 23.0));
      constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1 }, Relationship.EQ, 23.0));
      
      SimplexSolver solver = new SimplexSolver();
      RealPointValuePair solution = solver.optimize(f, constraints, GoalType.MINIMIZE, true);
      assertEquals(25.3, solution.getValue(), .0000001);
    }

","21/Aug/09 20:50;bmccann;And a yet smaller version that is failing:

max 0.2 x1 + 0.3 x3
s.t. 
x1 + x3 = 23.0

    @Test
    public void testMath268() throws OptimizationException {
      LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.2, 0.3 }, 0 );
      Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
      constraints.add(new LinearConstraint(new double[] { 1, 1 }, Relationship.EQ, 23.0));

      SimplexSolver solver = new SimplexSolver();
      RealPointValuePair solution = solver.optimize(f, constraints, GoalType.MINIMIZE, true);
      assertEquals(6.9, solution.getValue(), .0000001);
    }","21/Aug/09 21:06;ciaccia;Hi Ben,
Your test here above is wrong, the GoalType should be MAXIMIZE and not MINIMIZE...
The MINIMIZE returns 4.6 which is the expected value.","21/Aug/09 21:32;bmccann;Whoops, right you are.  Thanks for catching that.
This should be a better example for debugging:

max 0.2 x1 + 0.3 x3
s.t.
x1 + x3 = 23.0

    @Test
    public void testMath268() throws OptimizationException {
      LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.2, 0.3 }, 0 );
      Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
      constraints.add(new LinearConstraint(new double[] { 1, 1 }, Relationship.EQ, 23.0));

      RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);
      assertEquals(6.9, solution.getValue(), .0000001);
    }

","21/Aug/09 21:38;bmccann;The solution is being calculated correctly, but the wrong thing is being returned.  The bug is in SimplexTableau.getSolution()","21/Aug/09 22:11;bmccann;Here's the fix.
Note that the test case I posted in the comment was named dyslexically.  It should be math286 not math268.","21/Aug/09 23:09;luc;Fixed in subversion repository as of r806753.
Patch applied.
Thanks to Andrea for the report and thanks to Ben for the fix","28/Aug/09 14:12;ciaccia;Hi all,
Maybe I'm doing something wrong extracting the latest version from the SVN repository, but the original issue is still not working on my apache commons math working copy :-(

Here is what I did:
-I extracted the latest version from the SVN repository
-I executed the original problematic LP for this bug and it still fails. Anyway, the simplified version (but maybe incompatible) succeeds...

Could please anyone look at that?
Thanks
Andrea","31/Aug/09 16:41;bmccann;Yes, there is still an error with this example. The bug for the smaller example was fixed, but there's something still affecting the larger one. I haven't been able to figure it out yet, but here's another smaller example that is also failing:

    @Test
    public void testMath286Reopened() throws OptimizationException {
        LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.7 }, 0 );
        Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
        constraints.add(new LinearConstraint(new double[] { 1, 1 }, Relationship.LEQ, 18.0));
        constraints.add(new LinearConstraint(new double[] { 1, 0 }, Relationship.GEQ, 10.0));
        constraints.add(new LinearConstraint(new double[] { 0, 1 }, Relationship.GEQ, 8.0));

        SimplexSolver solver = new SimplexSolver();
        RealPointValuePair solution = solver.optimize(f, constraints, GoalType.MAXIMIZE, true);
        assertEquals(13.6, solution.getValue(), .0000001);
    } ","31/Aug/09 17:24;bmccann;Here are the corresponding Simplex tableaus.  Let me know if you spot the error.","31/Aug/09 19:11;ciaccia;Hi all,
What I wrote before doesn't really make sense... 

Look at this LP simplex solver:

http://www.phpsimplex.com/simplex/page2.php?objetivo=max&x1=0.8&x2=0.7&restricciones=3&variables=2&l=en&x11=1&x12=1&desigualdad1=-1&y1=18&x21=1&x22=0&desigualdad2=1&y2=10&x31=0&x32=1&desigualdad3=1&y3=8&Submit=Continue

why after the first phase is over the coefficients for x1 and x2 are set to 0.8 and 0.7 while here they are 0? Couldn't be the SimplexTableau's discardArtificialVariables doesn't do everything correct?

Just guessing...","01/Sep/09 05:09;bmccann;If I remember correctly phpsimplex formed the tableau a bit differently.  Instead of creating the tableau from the beginning with two objective functions, they only use one at the beginning, and then insert the phase 2 objective function later.  As a result, the tableaus aren't directly comparable.
I emailed Professor Chinneck at Carleton University for some help because I retaught myself a lot of this using a draft of a book he's working on.  He suggested looking into ways to deal with degenerate solutions, so I'll see if I can read up on that a bit:
""You've hit a degeneracy (which means that there are multiple solutions with the same value of objective function). The problem came when, in your second tableau, you pivoted on the third row (corresponding to s1) instead of the last row (corresponding to a2). Understandable since the minimum ratio test came out tied.""
","01/Sep/09 21:39;bmccann;phpsimplex said that when there's a tie, they make the artificial variable leave the basis, which we're not doing and would solve the problem in this case:
http://www.phpsimplex.com/en/simplex_method_theory.htm#anomalous
i'll try putting together a patch, likely this weekend, to do that","08/Sep/09 00:16;bmccann;Happy Labor Day!  To celebrate, here's a new set of patches that adds the ability to deal with degeneracy in the SimplexTableau.
The test includes four tests: the two bugs in MATH-286, the bug from MATH-288, and the bug from MATH-290.  I think they should all go in to prevent a regression in the future.  They all run very quickly and there are a lot of edge cases in the Simplex algorithm, so I'd prefer to be safe.","08/Sep/09 08:42;luc;resolved in subversion repository as of r812390
Benjamin's patches from 2009-09-07 have been committed with very slight changes
(an == test between Integer instances replaced by a call to equals)
thanks for the patches","08/Sep/09 08:51;ciaccia;Hi all,
I'm still not convinced the original problem is correct... Could you please try?

I would add the original LP with 6 variables and 5 restrictions to the test cases, since I'm not completely sure the ""smaller examples"" address exactly the same problem I posted weeks ago.","08/Sep/09 18:24;bmccann;Wow, sorry, I'll take another look and make sure to test against the original problem this time as well.","08/Sep/09 22:35;bmccann;The problem still exists when we are dropping columns moving between phase 1 and phase 2.  I was pointed towards this book for possible solutions (p80-82):
http://www.amazon.com/Linear-Programming-Introduction-Operations-Engineering/dp/0387948333#reader","09/Sep/09 01:54;bmccann;Here's a patch that correctly solves the problem Andrea posted (sorry for not completely fixing the problem with my earlier patches.)  It's mostly refactoring to make the solution easier, which was to drop positive cost non-artificial variables and nonbasic artificial variables at the end of phase 1 instead of all artificial variables.
There are some practical problems with degeneracy and redundant constraints that were not obvious to me from the theory when I first started working on this.  We should really test this a bit more with constraints outnumbering variables and redundant constraints to really be comfortable that all the issues are addressed, but we're certainly pretty close now if not there yet.
Andrea, thanks for the reports and validating fixes (or lack there of).  Luc, thanks for quickly getting these into SVN and letting me know about the problems.","09/Sep/09 08:55;luc;The patch from 2009-09-08 has been applied in subversion repository as of r812831.

After the patch, an existing test (SimplexTableauTest.testdiscardArtificialVariables) failed, so I had to update the expected matrix to prevent this failure. *This should be validated* as I really don't know if the failure were expected after the change or if something wrong occurred. Ben, could you have a look at this ?

This time, I'm not marking the issue as resolved, Andrea I'll wait until you consider it solved by yourself.

Thanks to everybody for their time on this issue, I'm sure we are gradually improving things.","09/Sep/09 12:08;ciaccia;Hi Luc, hi Ben,
This time I can confirm Ben's patch fixes the original problem. :-)

If I find something else related to this bug (or not) I will let you know, but I'm pretty sure this time the SimplexSolver is much more robust it was a couple of weeks ago!

I ""improved"" the JUnit test case for this bug, I attach the patch to this message. There are 5 new assertions that check all the constraints are really satisfied.

Thanks again for all the commitment
Andrea

PS: I don't know if this is somehow related, but I found another bug MATH-293... Ben, please look at that as well.","09/Sep/09 12:25;luc;I applied Andrea's patch in subversion as of r812921, thanks for providing it.
I'll wait for Ben's acknowledgement about the changes I've made in SimplexTableauTest.testdiscardArtificialVariables before marking the issue as resolved.","09/Sep/09 16:38;bmccann;Thanks Luc.  You can close this issue.  A note, we might want to rename testdiscardArtificialVariables to testDropPhase1Objective to match the updated method name.  I overlooked this earlier.  I look at MATH-293.","10/Sep/09 08:22;luc;fixed now (we hope)",,,,,,
NullPointerException in SimplexTableau.initialize,MATH-290,12433932,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ciaccia,ciaccia,25/Aug/09 10:53,14/Apr/10 00:27,20/Mar/20 20:32,27/Aug/09 08:08,2.0,,,,,,,2.1,,0,,,,,,,,"SimplexTableau throws a NullPointerException when no solution can be found instead of a NoFeasibleSolutionException

Here is the code that causes the NullPointerException:

LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 1, 5 }, 0 );
Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
constraints.add(new LinearConstraint(new double[] { 2, 0 }, Relationship.GEQ, -1.0));

RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MINIMIZE, true);

Note: Tested both with Apache Commons Math 2.0 release and SVN trunk",Java 1.6.0_13 on Windows XP 32-bit ,,,,,,,,,,,,"25/Aug/09 22:28;bmccann;SimplexSolverTest.patch;https://issues.apache.org/jira/secure/attachment/12417671/SimplexSolverTest.patch","25/Aug/09 22:28;bmccann;SimplexTableau.patch;https://issues.apache.org/jira/secure/attachment/12417670/SimplexTableau.patch",,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2009-08-25 20:43:17.23,,,false,,,,,,,,,,,,,,,,,34153,,,Thu Aug 27 08:08:17 UTC 2009,,,,,,,"0|i0rvdb:",160726,,,,,,,,,,,,,,,,"25/Aug/09 20:43;bmccann;Thanks for the bug report.  I'm glad to be finding these bugs (hopefully the last of them :o)  That was the point in open sourcing this, so looks like that was a good decision!
Problem here was that when the constraint's right hand side is negative, we need to flip the constraint around so that it is positive.  We were doing that too late during the initialization and it should have been done at the very beginning, so that we only deal with the normalized version instead of both versions.","25/Aug/09 22:28;bmccann;The original patch is sufficient, but I'm attaching a new one that also cleans up some code formatting (weird spacing in SimplexTableau) and also provides a little bit better test.
Andrea, the correct solution for the example you provided is 0.  You restricted the variables to be non-negative and are trying to minimize the objective function, so the best you can do is make both variables 0 for an objective function value of 0.","26/Aug/09 06:35;ciaccia;Hi Ben, what you say in your last comment is not true. There is no feasible solution to my problem...
Maybe I should have expressed it in ""human"" readable format:

min: 1 x + 5 y;
r1: 2 x + 0 y <= -1;
x >= 0;
y >= 0;

x = 0 and y = 0 is not a valid solution since 2 * 0 + 0 * 0 (=0) is not <= -1...

The ""trick"" is that there will be never non-negative variables (in this case [x, y]) that multiplied with non-negative coefficients (here [2, 0]) will produce a negative result.

Do you agree?","26/Aug/09 15:49;bmccann;You originally wrote >= in the constraint and now you're writing <=.  Perhaps that's where the confusion is?  The answer is 0 when you use <=.","26/Aug/09 15:56;ciaccia;Ops... I found the bug with the <= but then I transcribed it wrongly... Sorry.

Another thing I found that is not related to this bug, are the comments in the SimplexTableau. JavaDoc for divideRow was copied from substractRow without being changed ;-)","27/Aug/09 08:08;luc;fixed in subversion repository as of r808313
latest version of patch applied and two tests used (one with <= as the constraint and one with >= as the constraint)
thanks to Andrea for the report
thanks to Ben for the patch",,,,,,,,,,,,,,,,,,,,,,,,,
TestUtils.assertRelativelyEquals() generates misleading error on failure,MATH-292,12434957,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb,sebb,06/Sep/09 22:04,14/Apr/10 00:25,20/Mar/20 20:32,09/Sep/09 10:12,2.0,,,,,,,2.1,,0,,,,,,,,"TestUtils.assertRelativelyEquals() generates misleading error on failure.

For example:

TestUtils.assertRelativelyEquals(1.0, 0.10427661385154971, 1.0e-9)

generates the error message:

junit.framework.AssertionFailedError: expected:<0.0> but was:<0.8957233861484503>

which is not very helpful.",,,,,,,,,,,,,"07/Sep/09 13:20;sebb;MATH-292.patch;https://issues.apache.org/jira/secure/attachment/12418802/MATH-292.patch",,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2009-09-07 18:03:07.151,,,false,,,,,,,,,,,,,,,,,150483,,,Wed Sep 09 10:12:01 UTC 2009,,,,,,,"0|i0rvcv:",160724,,,,,,,,,,,,,,,,"07/Sep/09 13:20;sebb;Possible fix which converts the relative error to an absolute error.","07/Sep/09 18:03;luc;The patch seems good to me.
+1 to commit it","09/Sep/09 10:12;sebb;URL: http://svn.apache.org/viewvc?rev=812871&view=rev
Log:
MATH-292 TestUtils.assertRelativelyEquals() generates misleading error on failure",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Matrix's ""OutOfBoundException"" in SimplexSolver",MATH-293,12435216,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ciaccia,ciaccia,09/Sep/09 14:07,14/Apr/10 00:21,20/Mar/20 20:32,10/Sep/09 08:23,2.0,,,,,,,2.1,,0,,,,,,,,"Hi all,
This bug is somehow related to incident MATH-286, but not necessarily...

Let's say I have an LP and I solve it using SimplexSolver. Then I create a second LP similar to the first one, but with ""stronger"" constraints. The second LP has the following properties:
* the only point in the feasible region for the second LP is the solution returned for the first LP
* the solution returned for the first LP is also the (only possible) solution to the second LP

This shows the problem:

{code:borderStyle=solid}
LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.4, 0.6}, 0 );
Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 30.0));
constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 30.0));
constraints.add(new LinearConstraint(new double[] { 0.8, 0.2, 0.0, 0.0, 0.0, 0.0 }, Relationship.GEQ, 10.0));
constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.7, 0.3, 0.0, 0.0 }, Relationship.GEQ, 10.0));
constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.0, 0.0, 0.4, 0.6 }, Relationship.GEQ, 10.0));

RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);

double valA = 0.8 * solution.getPoint()[0] + 0.2 * solution.getPoint()[1];
double valB = 0.7 * solution.getPoint()[2] + 0.3 * solution.getPoint()[3];
double valC = 0.4 * solution.getPoint()[4] + 0.6 * solution.getPoint()[5];

f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.4, 0.6}, 0 );
constraints = new ArrayList<LinearConstraint>();
constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 30.0));
constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 30.0));
constraints.add(new LinearConstraint(new double[] { 0.8, 0.2, 0.0, 0.0, 0.0, 0.0 }, Relationship.GEQ, valA));
constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.7, 0.3, 0.0, 0.0 }, Relationship.GEQ, valB));
constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.0, 0.0, 0.4, 0.6 }, Relationship.GEQ, valC));

solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);
{code} 

Instead of returning the solution, SimplexSolver throws an Exception:

{noformat} Exception in thread ""main"" org.apache.commons.math.linear.MatrixIndexException: no entry at indices (0, 7) in a 6x7 matrix
	at org.apache.commons.math.linear.Array2DRowRealMatrix.getEntry(Array2DRowRealMatrix.java:356)
	at org.apache.commons.math.optimization.linear.SimplexTableau.getEntry(SimplexTableau.java:408)
	at org.apache.commons.math.optimization.linear.SimplexTableau.getBasicRow(SimplexTableau.java:258)
	at org.apache.commons.math.optimization.linear.SimplexTableau.getSolution(SimplexTableau.java:336)
	at org.apache.commons.math.optimization.linear.SimplexSolver.doOptimize(SimplexSolver.java:182)
	at org.apache.commons.math.optimization.linear.AbstractLinearOptimizer.optimize(AbstractLinearOptimizer.java:106){noformat} 

I was too optimistic with the bug MATH-286 ;-)",java 1.6 on Windows XP 32-Bit,,,,,,,,,,,,"10/Sep/09 07:28;bmccann;SimplexSolverTest.patch;https://issues.apache.org/jira/secure/attachment/12419148/SimplexSolverTest.patch","10/Sep/09 07:28;bmccann;SimplexTableau.patch;https://issues.apache.org/jira/secure/attachment/12419147/SimplexTableau.patch",,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2009-09-09 18:39:26.801,,,false,,,,,,,,,,,,,,,,,34198,,,Thu Sep 10 08:23:11 UTC 2009,,,,,,,"0|i0rvcn:",160723,,,,,,,,,,,,,,,,"09/Sep/09 18:39;bmccann;Here's the standard LP description of the problem:
max .8 x0 + .2 x1 + .7 x2 + .3 x3 + .4 x4 + .6 x5
s.t.
x0 + x2 + x4 = 30
x1 + x3 + x5 = 30
.8 x0 + .2 x1 >= 10
.7 x2 + .3 x3 >= 10
.4 x4 + .6 x5 >= 10","09/Sep/09 19:00;ciaccia;Hi Ben,
Just to be clear, the first LP is solved correctly with the following solution:

Value: 40.57142857142857

x0: 15.714285714285714
x1: 0.0
x2: 14.285714285714286
x3: 0.0
x4: 0.0
x5: 30.0

Then I create new constraints that are satisfied by the solution here above:
c3: 0.8 * x0 + 0.2 * x1 >= 0.8 * 15.714285714285714 ( = 12.571428571428571)
c4: 0.7 * x2 + 0.3 * x3 >= 0.7 * 14.285714285714286 ( = 10.0)
c5: 0.4 * x4 + 0.6 * x5 >= 0.6 * 30.0 ( = 18.0)

Note that by its construction, the solution above satisfies the constraints c3, c4, and c5.

If I try to solve the new LP, a ""OutOfBoundException"" is thrown...","10/Sep/09 07:28;bmccann;Thanks for the bug report Andrea.  I was expecting this problem when I submitted the patch for MATH-286, but didn't have a good test case to work against and verify the fix.  This is largely what I was thinking of when I mentioned we should test some more, but didn't think you'd beat me to it in finding a good example to work off of :o)  When I changed the columns being dropped in the patch for MATH-286 it meant there wasn't always a way to always tell which variable each column of the tableau represented.  This patch creates a mapping between column index and variable label so that when we can read the final solution out of the tableau.  I feel much better about this now.  Thanks!","10/Sep/09 08:23;luc;patch applied in subversion repository as of r813301
thanks to Andrea and Ben",,,,,,,,,,,,,,,,,,,,,,,,,,,
RandomDataImpl.nextPoisson fails for means in range 6.0 - 19.99,MATH-294,12435449,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mcfall,mcfall,11/Sep/09 17:34,14/Apr/10 00:20,20/Mar/20 20:32,12/Oct/09 02:08,1.2,2.0,,,,,,2.1,,0,,,,,,,,"math.random.RandomDataImpl.nextPoisson(double mean) fails frequently (but not always) for values of mean between 6.0 and 19.99 inclusive. For values below 6.0 (where I see there is a branch in the logic) and above 20.0 it seems to be okay (though I've only randomly sampled the space and run a million trials for the values I've tried)

When it fails, the exception is as follows (this for a mean of 6.0)

org.apache.commons.math.MathRuntimeException$4: must have n >= 0 for n!, got n = -2
	at org.apache.commons.math.MathRuntimeException.createIllegalArgumentException(MathRuntimeException.java:282)
	at org.apache.commons.math.util.MathUtils.factorialLog(MathUtils.java:561)
	at org.apache.commons.math.random.RandomDataImpl.nextPoisson(RandomDataImpl.java:434) 

ie MathUtils.factorialLog is being called with a negative input

To reproduce:

    JDKRandomGenerator random = new JDKRandomGenerator();
    random.setSeed(123456);
    RandomData randomData = new RandomDataImpl(random);

    for (int i=0; i< 1000000; i++){
        randomData.nextPoisson(6.0);
    }
",Java 1.6 on mac osX,,,,,,,,,,,,"13/Sep/09 14:45;luc;math-294.patch;https://issues.apache.org/jira/secure/attachment/12419421/math-294.patch",,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2009-09-13 14:45:08.761,,,false,,,,,,,,,,,,,,,,,34180,,,Mon Oct 12 02:08:12 UTC 2009,,,,,,,"0|i0rvcf:",160722,,,,,,,,,,,,,,,,"13/Sep/09 14:45;luc;The failure seems to be related with a wrong detection of out of bound cases.
In the example given the failure occurs at iteration 125 on my computer (Open JDK 1.6, Linux, 64bits). At the beginning of the loop, the u = nextUniform(0.0, c) random drawing leads to a value u smaller than c1. So the first branch of the if is taken and another random drawing is done : z = nextGaussian(0.0, 1.0) which leads to a value for x far below mu (x = -8, mu = 2).

This is detected as w is set to positive infinity, but in fact this is not sufficient. The ""accept"" boolean is still computed despite it will always be false.

The attached patch is an attempt to compute ""accept"" only in some cases and to force it to ""false"" in other cases without computation.

I did *not* check this in subversion because I would like some other people to have a look at it. I am not sure it does work properly because when I compute for example the mean of 200, 100000, 1000000 or 10000000 calls to nextPoisson(6.0), I always get values between 14.7 and 15.2. I have seen the variance of a Poisson distribution is the same as its mean and so could expect a large value, but this still looked strange to me.","15/Sep/09 02:30;psteitz;Lets hold on the patch for now.  We need to understand why the rejection algorithm is failing.  Based on a quick review of the reference in the javadoc (p. 511, X.3), it looks like the following line in the u < c_1 case should not be subtracting 1.0 at the end.

y = -Math.abs(z) * Math.sqrt(mu) - 1.0;

I have not tested the effect dropping the -1 here as I am still working through the algorithm.","28/Sep/09 10:45;psteitz;In r819492, I committed a goodness of fit test (testNextPoissionConistency) for the values generated by nextPoisson.  Currently, it tests only values up to 5.  When this issue is resolved, the upper bound in the test needs to be increased.

I have not been able to rectify the current code so that it succeeds for all values and passes the test.  The implementation does not match the reference in the javadoc and the reference appears to contain errors.  If someone does not beat me to it or supply a patch that passes the test, I will find and implement a different version of the rejection algorithm for means > 6.","12/Oct/09 02:08;psteitz;Fixed in r824214",,,,,,,,,,,,,,,,,,,,,,,,,,,
RandomDataImpl.nextPoisson fails for means in range 6.0 - 19.99,MATH-295,12435450,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,mcfall,mcfall,11/Sep/09 17:40,14/Apr/10 00:18,20/Mar/20 20:32,11/Sep/09 17:44,1.2,2.0,,,,,,,,0,,,,,,,,"RandomDataImpl.nextPoission(double mean) fails frequently (but not every time) from calls with mean >= 6.0 and < 20.0
Below 6.0 and above 20 it seems fine, as far as I can tell by testing values at random.

When it fails, the exception is as follows  - this from calling nextPoisson(6.0)

org.apache.commons.math.MathRuntimeException$4: must have n >= 0 for n!, got n = -2
	at org.apache.commons.math.MathRuntimeException.createIllegalArgumentException(MathRuntimeException.java:282)
	at org.apache.commons.math.util.MathUtils.factorialLog(MathUtils.java:561)
	at org.apache.commons.math.random.RandomDataImpl.nextPoisson(RandomDataImpl.java:434)

ie it's calling MathUtils.factorialLog with a negative argument.

",Java 1.6 on Mac 0S X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,150484,,,Fri Sep 11 17:44:55 UTC 2009,,,,,,,"0|i0rvc7:",160721,,,,,,,,,,,,,,,,"11/Sep/09 17:43;mcfall;Sorry, duplicate of Math 294 (got a 'server down for maintenance' error message when raising 294 but obviously it did get stored)","11/Sep/09 17:44;mcfall;Duplicate of Math 294",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LoessInterpolator.smooth() not working correctly,MATH-296,12435875,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,romeop,romeop,16/Sep/09 21:14,14/Apr/10 00:18,20/Mar/20 20:32,26/Jan/10 21:54,2.0,,,,,,,2.1,,0,,,,,,,,"I have been comparing LoessInterpolator.smooth output with the loessFit output from R (R-project.org, probably the most widely used loess implementation) and have had strangely different numbers. I have created a small set to test the difference and something seems to be wrong with the smooth method but I do no know what and I do not understand the code.
*Example 1*
|x-input: |1.5| 3.0| 6| 8| 12|13| 22| 24|28|31|
|y-input: |3.1|6.1|3.1|2.1|1.4|5.1|5.1|6.1|7.1|7.2|
|Output LoessInterpolator.smooth():|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|
|Output from loessFit() from R: |3.191178027520974|3.0407201231474037|2.7089538903778636|2.7450823274490297|4.388011000549519|4.60078952381848|5.2988217587114805|5.867536388457898|6.7797794777879705|7.444888598397342|

*Example 2 (same x-values, y-values just floored)*
|x-input: |1.5| 3.0| 6| 8| 12|13| 22| 24|28|31|
|y-input: |3|6|3|2|1|5|5|6|7|7|
|Output LoessInterpolator.smooth(): |3|6|3|2|0.9999999999999005|5.0000000000001705|5|5.999999999999972|7|6.999999999999967|
|Output from loessFit() from R: |3.091423927353068|2.9411521572524237|2.60967950675505|2.7421759322272248|4.382996912300442|4.646774316632562|5.225153658563424|5.768301917477015|6.637079139313073|7.270482144410326|

As you see the output is practically the replicated y-input.
At this point this funtionality is critical for us but I could not find any other suitable java-implementation. Help. Maybe this strange behaviour gives someone a clue?

",Java 1.6 on Vista,,,,,,,,,,,,"26/Jan/10 14:27;jkff;MATH-296.2.patch;https://issues.apache.org/jira/secure/attachment/12431424/MATH-296.2.patch","18/Sep/09 20:31;luc;math-296-test.patch;https://issues.apache.org/jira/secure/attachment/12420070/math-296-test.patch","25/Sep/09 16:56;jkff;math-296.patch;https://issues.apache.org/jira/secure/attachment/12420572/math-296.patch",,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2009-09-18 14:52:52.174,,,false,,,,,,,,,,,,,,,,,34191,,,Tue Jan 26 21:54:45 UTC 2010,,,,,,,"0|i0rvbz:",160720,,,,,,,,,,,,,,,,"18/Sep/09 14:52;luc;What are your settings for bandwidth and robustness iterations ?
I have tried your examples and the first one doesn't lead to NaN with the default settings for me.","18/Sep/09 15:54;romeop;My settings are 0.3 and 4 (these are used as defaults in R). 
If I use the standard values I still get:
|3.100000000000003|6.099999999999999|3.0999999999999996|NaN|NaN|NaN|NaN|NaN|7.1|7.85|

Here is my test code for a quick look:

public void testLoessInterpolator() throws MathException{
		dataX = new double[10];
		dataX[0] = 1.5;
		dataX[1] = 3.0;
		dataX[2] = 6;
		dataX[3] = 8;
		dataX[4] = 12;		
		dataX[5] = 13;
		dataX[6] = 22;
		dataX[7] = 24;
		dataX[8] = 28;
		dataX[9] = 31;
		
		
		dataY = new double[10];
		dataY[0] = 3.1;
		dataY[1] = 6.1;
		dataY[2] = 3.1;
		dataY[3] = 2.1;
		dataY[4] = 1.4;		
		dataY[5] = 5.1;
		dataY[6] = 5.1;
		dataY[7] = 6.1;
		dataY[8] = 7.1;
		dataY[9] = 7.2;
		
		LoessInterpolator li = new LoessInterpolator();
		double[] ly = li.smooth(dataX, dataY);
		
		for (int i = 0; i < ly.length; i++) {
			System.out.println(ly[i]);
		}		
	}

Is there any obvious error which I do not see? Is it possible (although highely unlikely) that some java floating point operations are broken on Vista64 (which I am using)?
Can you show your results?","18/Sep/09 18:45;luc;OK. I reproduce exactly your results on a Linux computer with openJDK 1.6 on an AMD64 processor.
I'll have a look at this issue.","18/Sep/09 20:31;luc;The math-296-test.patch is a set of two test cases that reproduce the problem.

Running the first test shows that at the end of iteration 0, the weights for points at indices 4 and 5 are set to 0. At iteration 1, when computing point i=4, the bandwidth is such that ileft=3 and iright=4. For k=3, we get w=0 because dist=4 and denom=1/4. For k=4 and k=5, we get w=0 because robustness[k]=0. So all w are 0, sumWeights=0 and since it is used in some divisions, NaN appears.

I think their should be some protection against this case somewhere. I'll ask the author of the original contribution about this.","22/Sep/09 07:49;jkff;Thanks for reporting the problem, Romeo. Actually, I did not compare the results of my implementation with those of R's: I purely implemented the algorithm described in the paper, and checked that the results are sensible. Unfortunately I will most probably not have time to debug the issue until weekend, but I can provide explanations to you about the code in case the issue is so critical for you that you urge to fix it yourself :)","24/Sep/09 18:22;romeop;Eugene, that is great news. I already tried to fix it but did not get very far. 
First of all, please ignore the sample values that are supposed to be from loessFit in the example above, they are not. I have reviewed my code and I can not reproduce those values. Sorry for that. When I try the examples again I get hardly any changes except for the NaN values (perhaps due to the fact that the values are not very close together?). The testcases should be deleted/changed to reflect this  I have compiled a more appropriate testcase:

|xval|0.1|0.2|0.3|0.4|0.5|0.6|0.7|0.8|0.9|1.0|1.1|1.2|1.3|1.4|1.5|1.6|1.7|1.8|1.9|2.0|
|yval|0.47|0.48|0.55|0.56|-0.08|-0.04|-0.07|-0.07|-0.56|-0.46|-0.56|-0.52|-3.03|-3.08|-3.09|-3.04|3.54|3.46|3.36|3.35|
|result|0.46076343001050907|0.49997135715509317|0.5429801876574236|0.3191000440696382|0.17824135321363765|-0.03999999999999973|-0.0699999999999999|-0.21301261447145203|-0.3260295715150817|-0.45999999999999996|-0.560000000000002|-1.4961752602921834|-2.1317826106042093|-3.0799999999999965|-3.089999999999987|-0.6229499113916326|0.9970283360414918|3.450011252797152|3.3907474604481567|3.336774229999947|
|loessFit|0.4609008580907899|0.4985267031275769|0.5408291873645029|0.308077939113221|0.1754074466314782|-0.0419929152622151|-0.07205683451999362|-0.19661654144034335|-0.31073415047540565|-0.44615854234427305|-0.5567051673879394|-1.4972970376332615|-2.1330233520442317|-3.08|-3.0900000000000043|-0.6208045742447532|0.9823842010251099|3.449395357814414|3.389793487936696|3.3359749840089385|
|weight(see below)|1.0|1.0|1.0|1.0|1.0|1.0|1.0|1.0|1.0|1.0|1.0|1.0|0.0|0.0|1.0|1.0|0.0|0.0|1.0|1.0|
|loessFit with weights|0.4780117093745368|0.4924876444111407|0.48398021996388496|0.3201446090891406|0.17880078999161195|-0.003305611690841502|-0.08829472923805592|-0.20902549447499932|-0.3267558674057103|-0.45542226638134214|-0.518369970805548|-0.5365908808969353|-1.492628917733731|-2.1151751796193703|-3.09|-3.0400000000000005|-2.9899999999999998|0.15500000000000005|1.7524999999999986|3.3499999999999996|


The results do not differ very much and there are no NaN, if the values are close together. Can this be explained by a slightly different algorithm in R?

For easy copy and paste:
double[] yval={0.47, 0.48, 0.55, 0.56,-0.08,-0.04,-0.07,-0.07,-0.56,-0.46,-0.56,-0.52,-3.03,-3.08,-3.09,-3.04, 3.54, 3.46, 3.36, 3.35};
double[] xval={0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0};
double[] weights = {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1};
LoessInterpolator li = new LoessInterpolator(0.3,4);
result = li.smooth(xval, yval);


Another related thing. A very common use of the R-function is with weights (that is how we use it as well): 
*loessFit(yValues, xValues, weights)*
where weights are the values 0 or 1, I dont know if some are using weights between 0 and 1 
If a spot has weight 0 it still gets loessfitted but does not affect his neighbours. This way you can specifiy more relevant points and have the complete data set interpolate around these.

In your implementation you use robustnessWeights and set them initially to 1. Do I understand this correctly that these are actually the same weights as the ones mentioned above?
Would it be possible to add a function with a third parameter and have the weights provided by the user? I tried just passing them trough as a parameter instead of initializing with 1 but it did not do the trick.","25/Sep/09 16:56;jkff;Romeo,
Here is a patch that implements the weighted loess fit (the results agree reasonably with R's implementation).
Also, the patch fixes some numeric instability by introducing an 'accuracy' parameter.

Luc,
Could you please review the patch and probably apply it?","25/Sep/09 18:35;luc;solved in subversion repository as of r818942.
patch applied with slight changes
thanks for the report and thanks for the patch","27/Sep/09 14:12;romeop;Eugene, Luc, thanks. Great news. I have been testing this with some bigger real-life datasets (~800 points) and the results seem to be very close to R).
There is one more problem though but I think I have already found the fix. You can still get NaN-results. 
The reason is that sumWeights can become 0 and there are a couple of divisions with sumWeights. This multiplies to the degree that the complete result set is an array of NaN.
I think (but am not sure) that this happens if you have a long series of 0-weights which is bigger then the amount of points you get through the bandwidth.

I have just added the following lines after these sumWeights divisions:
after: *double meanXSquared = sumXSquared / sumWeights;*
                
                if(sumWeights==0){
                	meanX=0;
                	meanY=0;
                	meanXY=0;
                	meanXSquared=0;
                }
That did the trick for me and the results seem accurate. I will test more during the next week with even more datasets. I hope I do not make any fundamental thinking error.

If you think this is correct please add it to the code.","30/Sep/09 14:25;jkff;Romeo, I am not sure about whether this behavior should be considered correct. 
It essentially means that there is no data inside an approximation window at all, so why should we make the data appear to be zero? 
How about a variant ""express the approximation window in terms of the number of points with non-zero weights""?
","30/Sep/09 22:13;romeop;Sorry, maybe I was a little unclear in my explanation. 
Actually there is data, there are just no weights in the approximation window. Lets say we have 300 point and the first 100 are weighted 0, the others are weighted 1, they are still valid data points. Being weighted 0 does not mean the data point can be deleted, it just means it does not affect his neighbours in terms of normalization (that is how the concept of normalization weights was explained to me)
What happens currently is the first ones become NaN and their neighbours become NaN to the point that *all* points become NaN. This shouldn't happen.
But do not misunderstand me, I am just assuming that the reason is because of the bandwidth as my dataset has this structure, I am not really sure about it. 
I have done some debugging on the code but there are really a lot of loops so I could not pinpoint when and why exactly this is starting to happen as it is not right from the beginning.","01/Oct/09 15:00;jkff;Romeo, have I understood you correctly that you'd like the data points with zero weights to not influence the approximation *coefficients* at all, but to still have computed approximation *values* for them?","07/Oct/09 16:58;romeop;Yes. I think that is correct. No influence on the approximation coefficients but still getting normalized (getting approximation values). 

Here is an except from the userguide for the R-limma package, which uses the loessFit function for some operations:

bq.Any spot quality weights found in RG will be used in the normalization by default. This means for example that spots with zero weight (flagged out) will not influence the normalization of other spots. The use of spot quality weights will not however result in any spots being removed from the data object. Even spots with zero weight will be normalized and will appear in the output object, such spots will simply not have any influence on the other spots.

However I am not sure how to handle the bandwidth. Example: if you have a bandwidth of 0.3, how do you compute the relevant points?
1. Find the 30% in the complete set and the use only the weighted ones inside this set
2. Look at all weighted ones and use 30% of them

To me the first one sounds like the logical one but I am not sure.","20/Oct/09 17:45;jkff;Sorry for the delay again: I just forgot about the issue. Don't hesitate to ping me, I am not a very organized person.

Adding zero-weighted points *must not* change the result in non-zero-weighted points and the approximation coefficients.
Thus, zero-weighted points must not participate in computing the bandwidth, and generally in computing anything at all: that is the *second* approach.

However, seems like R takes the *first* one, because I've implemented the second one and results disagreed with R on the testcase above.

Are you OK with having a more logical implementation that disagrees with R? :)","23/Dec/09 01:19;psteitz;Eugene's reasoning sounds correct to me.  I am inclined to say we document carefully and resolve this issue.  Any objections?","23/Dec/09 19:21;luc;I agree, but would like to know what R produces when a large (read larger than bandwidth) chunk of zero weights occurs.","25/Jan/10 18:07;luc;Coming back to this issue.
Eugene, as you said you implemented the second approach and people agree with your reasoning,
could you provide a patch using this second approach so we can apply it ?","26/Jan/10 14:27;jkff;Here is the patch.","26/Jan/10 21:54;luc;second patch applied as of r903440
thanks for the patch",,,,,,,,,,,,
Eigenvector computation incorrectly returning vectors of NaNs,MATH-297,12436157,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,20/Sep/09 14:33,14/Apr/10 00:16,20/Mar/20 20:32,25/Jan/10 17:58,2.0,,,,,,,2.1,,0,,,,,,,,"As reported by Axel Kramer on commons-dev, the following test case succeeds, but should fail:

{code}
public void testEigenDecomposition() {
    double[][] m = { { 0.0, 1.0, -1.0 }, { 1.0, 1.0, 0.0 }, { -1.0,0.0, 1.0 } };
    RealMatrix rm = new Array2DRowRealMatrix(m);
    assertEquals(rm.toString(),
        ""Array2DRowRealMatrix{{0.0,1.0,-1.0},{1.0,1.0,0.0},{-1.0,0.0,1.0}}"");
    EigenDecompositionImpl ed = new EigenDecompositionImpl(rm,
        MathUtils.SAFE_MIN);
    RealVector rv0 = ed.getEigenvector(0);
    assertEquals(rv0.toString(), ""{(NaN); (NaN); (NaN)}"");
  }
{code}

ed.getRealEigenvalues() returns the correct eigenvalues (2, 1, -1), but all three eigenvectors contain only NaNs.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-10-18 23:33:05.76,,,false,,,,,,,,,,,,,,,,,34221,,,Mon Jan 25 17:58:04 UTC 2010,,,,,,,"0|i0rvbr:",160719,,,,,,,,,,,,,,,,"18/Oct/09 23:33;billbarker;This is now fixed as of R826550.  Unfortunately, I lack karma to resolve this issue, so will have to hope that somebody else will resolve it for me.","19/Oct/09 18:14;axelclk;If I'm expanding my testcase to the snippet below, I'm  now getting an eigenvector with all ""negative values"" at index 1.
I think this should be avoided. 
See also the solution computed by Ted Dunning on the mailing list: 
http://www.mail-archive.com/dev@commons.apache.org/msg12038.html

    double[][] m = { { 0.0, 1.0, -1.0 }, { 1.0, 1.0, 0.0 }, { -1.0, 0.0, 1.0 } };
    RealMatrix rm = new Array2DRowRealMatrix(m);
    assertEquals(rm.toString(),
        ""Array2DRowRealMatrix{{0.0,1.0,-1.0},{1.0,1.0,0.0},{-1.0,0.0,1.0}}"");
    EigenDecompositionImpl ed = new EigenDecompositionImpl(rm,
        MathUtils.SAFE_MIN);
    RealVector rv0 = ed.getEigenvector(0);
    RealVector rv1 = ed.getEigenvector(1);
    RealVector rv2 = ed.getEigenvector(2);
    assertEquals(rv0.toString(), ""{0,58; 0,58; -0,58}"");
    assertEquals(rv1.toString(), ""{-0; -0,71; -0,71}"");
    assertEquals(rv2.toString(), ""{0,82; -0,41; 0,41}"");
","27/Oct/09 03:14;bjohnson;There still seems to be a problem with the decomposition of some matrices.  For example, the decomposition of the identity matrix {noformat} {{1,0},{0,1}}{noformat}
 yields the correct eigenvalues, but NaN for all the eigenvector elements.
Crucially, the ""isIncludedColumn"" in the EigenDecompositionImplTest.java file always returns true (at least on my system) when the calculated eigenvectors have NaN elements, so is useless as a test for this problem.

Also, I discovered this problem when getting NaN values doing an SVD of certain matrices (where each row has only one non-zero value).  Since the SVD algorithm uses the EigenDecompositionImpl code, this seems to be a result of this current  bug.  (And ironically, I just told my students that one reason people love the SVD is that it essentially never fails).





","27/Oct/09 08:43;luc;I will look at this issue one issu MATH-308 has been solved.","27/Oct/09 11:48;bjohnson;Thanks.

The problem seems to be a divide by zero error with the variable diP1 in the following code, but this is probably as far as I'll get in debugging it:
{noformat}
   private void stationaryQuotientDifferenceWithShift(final double[] d, final double[] l,
                                                       final double lambda) {
        final int nM1 = d.length - 1;
        double si = -lambda;
        for (int i = 0, sixI = 0; i < nM1; ++i, sixI += 6) {
            final double di   = d[i];
            final double li   = l[i];
            final double diP1 = di + si;
            final double liP1 = li * di / diP1;
            work[sixI]        = si;
            work[sixI + 1]    = diP1;
            work[sixI + 2]    = liP1;
            si = li * liP1 * si - lambda;
        }
        work[6 * nM1 + 1] = d[nM1] + si;
        work[6 * nM1]     = si;
    }
{noformat}","27/Oct/09 18:39;bjohnson;..., and I note that what I presume to be the original Fortran code (in dlar1v.f of LAPACK) has several sections of code marked:

*        Runs a slower version of the above loop if a NaN is detected

Hope this helps in resolving the issue,

Bruce
","03/Nov/09 07:23;jake.mannix;Doing the most trivial fix possible (dividing by the SAFE_MIN if diPl == 0)  on the line which leads to NaN appears to correctly fix this for the particular case of decomposing diagonal matrices (at least for 2x2 and 3x3 cases I checked with a unit test). 

I'm not sure if this ""fixes"" the problem, because I'm not sure if I really grok the algorithm's implementation in this code.

We could see better if I knew how often this pops up (any matrix whose tri-diagonal form is actually diagonal?)...","03/Nov/09 07:42;jake.mannix;Ok, well sadly it's easy to find an example which *isn't* fixed by just removing that one divide-by-zero: 
{code} { {0, 1, 0}, {1, 0, 0}, {0, 0, 1} } {code} leads to perfectly reasonable eigenvalues (1, 1, -1), but NaN again rears its ugly head because findEigenVectors() assumes that, among other things, that the main diagonal does not start with a zero, and then divides by it.

Not sure what the proper solution is to this, but a non-shifted LDL^t decomposition is a lot easier to understand to me than the other place where the NaN pops up, so maybe I can figure this one out on the plane ride down to ApacheCon tomorrow.","29/Nov/09 21:27;luc;Another step towards the solution has been checked in as of r885268.
Just as Bruce noticed one month ago (thanks), there was an improvement in dstqds and dqds algorithms implemented in DLAR1V that are not in Dhillon's thesis.
The problem is still not completely solved as for example in dimension 3 the decomposition of identity leads to 3 times the same
vector (0, 0, 1) instead of giving (1, 0, 0), (0, 1, 0) and (0, 0, 1).","25/Jan/10 17:58;luc;The original issue as been solved 2009-11-29 as of r885268.
The remaining problem identified in the last comments has been moved in a separate JIRA issue: MATH-333",,,,,,,,,,,,,,,,,,,,,
EmpiriicalDisributionImpl.getUpperBounds does not return upper bounds on data bins,MATH-298,12436158,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,20/Sep/09 14:46,14/Apr/10 00:12,20/Mar/20 20:32,21/Sep/09 01:32,1.0,1.1,1.2,2.0,,,,2.1,,0,,,,,,,,"Per the javadoc, the getUpperBounds method in the EmpiricalDistribution should return upper bounds for the bins used in computing the empirical distribution and the bin statistics.  What the method actually returns is the upper bounds of the subintervals of [0,1] used in generating data following the empirical distribution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,150485,,,Mon Sep 21 01:32:21 UTC 2009,,,,,,,"0|i0rvbj:",160718,,,,,,,,,,,,,,,,"21/Sep/09 01:32;psteitz;Fixed in r817128.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexSolver returns no feasible solution,MATH-299,12436311,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,davidwilcox,davidwilcox,22/Sep/09 14:43,14/Apr/10 00:11,20/Mar/20 20:32,24/Sep/09 22:36,2.0,,,,,,,2.1,,0,,,,,,,,"I am trying to optimize this:

maximize: v

v <= a1*p1 + a2*p2 + a3*p3 + a4*p4
v <= b1*p1 + b2*p2 + b3*p3 + b4*p4
v <= c1*p1 + c2*p2 + c3*p3 + c4*p4
v <= d1*p1 + d2*p2 + d3*p3 + d4*p4

p1 + p2 + p3 + p4 = 1

where a1-d4 are constant specified below by the code (i didn't want to copy and paste them up here. you can look below to see what they are in the objective function). 

		LinearObjectiveFunction f = new LinearObjectiveFunction(		
				new double[] { 1,
						0, 
						0, 0, 0}, 0 );
		Collection<LinearConstraint>  constraints = new ArrayList<LinearConstraint> ();


		constraints.add(new LinearConstraint(new double[] { -1, 
				1.7316145027890766, 
				 1.3584341412980305,
				 0.9305633063383639,
				 1.687117394945513
		},
		Relationship.GEQ, 0));

		constraints.add(new LinearConstraint(new double[] { -1, 
				0.6617060079461883, 
				 1.4862459822191323,
				 0.7692647272328988,
				 0.7329140944025636
		},
		Relationship.GEQ, 0));

		constraints.add(new LinearConstraint(new double[] { -1, 
				1.3255966888982322, 
				286.21607948837584,
				1.135907611434458,
				0.9803367440299271
		},
		Relationship.GEQ, 0));

		constraints.add(new LinearConstraint(new double[] { -1, 
				0.5428682596573682, 
				1.5745685116536952,
				1.4834419186882808,
				1.2884923232048968
		},
		Relationship.GEQ, 0));


		constraints.add(new LinearConstraint(new double[] {0, 1, 1, 1, 1},
				Relationship.EQ, 1));
		RealPointValuePair solution = null;
		try {
		
			solution = new SimplexSolver().optimize(f, constraints,
					GoalType.MAXIMIZE, true);
		}
		catch (OptimizationException e) {
			e.printStackTrace();
		}

I get this error back from the SimplexSolver.

org.apache.commons.math.optimization.linear.NoFeasibleSolutionException: no feasible solution
	at org.apache.commons.math.optimization.linear.SimplexSolver.solvePhase1(SimplexSolver.java:177)
	at org.apache.commons.math.optimization.linear.SimplexSolver.doOptimize(SimplexSolver.java:187)
	at org.apache.commons.math.optimization.linear.AbstractLinearOptimizer.optimize(AbstractLinearOptimizer.java:106)
	at Runner.main(Runner.java:101)

One interesting thing to note is that if you round all the numbers to the nearest 100's place, it works. If you keep it with the floating point precision shown here, it doesn't.
		",Windows XP commons Math 2.0 jre 1.6.0.16,28800,28800,,0%,28800,28800,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-09-22 18:43:43.038,,,false,,,,,,,,,,,,,,,,,34188,,,Thu Sep 24 22:36:26 UTC 2009,,,,,,,"0|i0rvbb:",160717,,,,,,,,,,,,,,,,"22/Sep/09 18:43;luc;I have tried to solve your problem with lp_solve to check whether it find a solution. Here is what I have set up using lp_solve lp format:
{noformat:title=math-299.lp}
max: v;

-v + 1.7316145027890766 p1 +   1.3584341412980305 p2 + 0.9305633063383639 p3 + 1.687117394945513  p4 >= 0;
-v + 0.6617060079461883 p1 +   1.4862459822191323 p2 + 0.7692647272328988 p3 + 0.7329140944025636 p4 >= 0;
-v + 1.3255966888982322 p1 + 286.21607948837584   p2 + 1.135907611434458  p3 + 0.9803367440299271 p4 >= 0;
-v + 0.5428682596573682 p1 +   1.5745685116536952 p2 + 1.4834419186882808 p3 + 1.2884923232048968 p4 >= 0;

p1 + p2 + p3 + p4 = 1;

free p1, p2, p3, p4;

{noformat}

Running lp_solve on this file returns the following error message:
{noformat}
%  lp_solve < math-299.lp
This problem is unbounded
{noformat}

Could you check the constants and check if the problem really has a feasible finite solution ?","22/Sep/09 21:21;bmccann;We might be better than lp_solve on this one!  This worked fine for me and gave me the same answer as Lindo (1.398257)
David, did you use Commons Math 2.0?  The seemed to work fine for me when I used Commons Math compiled from the Subversion repository.  I'd recommend using the Subversion version because we've fixed numerous bugs that were present in the 2.0 release.

    @Test
    public void testMath299() throws OptimizationException {
        LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 1, 0, 0, 0, 0}, 0 );
        Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint> ();
        constraints.add(new LinearConstraint(new double[] { -1, 1.7316145027890766, 1.3584341412980305, 0.9305633063383639, 1.687117394945513 }, Relationship.GEQ, 0));
        constraints.add(new LinearConstraint(new double[] { -1, 0.6617060079461883, 1.4862459822191323, 0.7692647272328988, 0.7329140944025636 }, Relationship.GEQ, 0));
        constraints.add(new LinearConstraint(new double[] { -1, 1.3255966888982322, 286.21607948837584, 1.135907611434458, 0.9803367440299271 }, Relationship.GEQ, 0));
        constraints.add(new LinearConstraint(new double[] { -1, 0.5428682596573682, 1.5745685116536952, 1.4834419186882808, 1.2884923232048968 }, Relationship.GEQ, 0));
        constraints.add(new LinearConstraint(new double[] {0, 1, 1, 1, 1}, Relationship.EQ, 1));
        RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);
        Assert.assertEquals(1.398257, solution.getValue(), .0001);
    }
","24/Sep/09 18:15;luc;David, do you consider the version in the subversion repository fixes this bug ?
Can we change its state to resolved ?","24/Sep/09 19:01;davidwilcox;I changed to the subversion repository and it seems fixed.


","24/Sep/09 22:36;luc;already fixed in subversion by previous changes",,,,,,,,,,,,,,,,,,,,,,,,,,
Bugs in Simplex Implementation,MATH-302,12438085,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,cwinter,cwinter,14/Oct/09 13:58,14/Apr/10 00:09,20/Mar/20 20:32,15/Oct/09 08:56,2.0,,,,,,,2.1,,0,,,,,,,,"Simplex routine may return infeasible solution:
{code:title=Bug1.java|borderstyle=solid}
import java.util.ArrayList;
import org.apache.commons.math.linear.ArrayRealVector;
import org.apache.commons.math.optimization.GoalType;
import org.apache.commons.math.optimization.OptimizationException;
import org.apache.commons.math.optimization.linear.*;

public class Bug1 {
    
    public static void main(String[] args) throws OptimizationException {
        
        LinearObjectiveFunction c = new LinearObjectiveFunction(new double[7], 0.0d);
        
        ArrayList<LinearConstraint> cnsts = new ArrayList<LinearConstraint>(5);
        LinearConstraint cnst;
        cnst = new LinearConstraint(new double[] {1.00d, 1.00d, 0.00d, 0.00d, 0.0d, 0.00d, 0.00d}, Relationship.EQ, 1.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.00d, 1.00d, 1.00d, 1.0d, 0.00d, 0.00d}, Relationship.EQ, 1.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.00d, 0.00d, 0.00d, 0.0d, 1.00d, 1.00d}, Relationship.EQ, 1.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.54d, 0.00d, 0.34d, 0.00d, 0.0d, 0.12d, 0.00d}, Relationship.EQ, 0.54d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.54d, 0.00d, 0.34d, 0.0d, 0.00d, 0.12d}, Relationship.EQ, 0.34d);
        cnsts.add(cnst);
        System.out.println(""Constraints:"");
        for(LinearConstraint con : cnsts) {
            System.out.println(con.getCoefficients().toString() + "" "" + con.getRelationship() + "" "" + con.getValue());
        }
        
        SimplexSolver simplex = new SimplexSolver();
        double[] sol = simplex.optimize(c, cnsts, GoalType.MINIMIZE, true).getPointRef();
        System.out.println(""Solution:\n"" + new ArrayRealVector(sol));
        System.out.println(""Third constraint is violated!"");
    }
}
{code}

or may find no solution where some exist:
{code:title=Bug1.java|borderstyle=solid}
import java.util.ArrayList;
import org.apache.commons.math.linear.ArrayRealVector;
import org.apache.commons.math.optimization.GoalType;
import org.apache.commons.math.optimization.OptimizationException;
import org.apache.commons.math.optimization.linear.*;

public class Bug2 {
    
    public static void main(String[] args) throws OptimizationException {
        
        LinearObjectiveFunction c = new LinearObjectiveFunction(new double[13], 0.0d);
        
        ArrayList<LinearConstraint> cnsts = new ArrayList<LinearConstraint>(5);
        LinearConstraint cnst;
        cnst = new LinearConstraint(new double[] {1.00d, 1.00d, 1.0d, 0.00d, 0.00d, 0.00d, 0.0d, 0.0d, 0.0d, 0.0d, 0.00d, 0.00d, 0.0d}, Relationship.EQ, 1.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.00d, 0.0d, 1.00d, 1.00d, 1.00d, 1.0d, 0.0d, 0.0d, 0.0d, 0.00d, 0.00d, 0.0d}, Relationship.EQ, 1.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.00d, 0.0d, 0.00d, 0.00d, 0.00d, 0.0d, 1.0d, 1.0d, 1.0d, 0.00d, 0.00d, 0.0d}, Relationship.EQ, 1.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.00d, 0.0d, 0.00d, 0.00d, 0.00d, 0.0d, 0.0d, 0.0d, 0.0d, 1.00d, 1.00d, 1.0d}, Relationship.EQ, 1.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.54d, 0.00d, 0.0d, 0.32d, 0.00d, 0.00d, 0.0d, 0.1d, 0.0d, 0.0d, 0.02d, 0.00d, 0.0d}, Relationship.EQ, 0.54d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.54d, 0.0d, 0.00d, 0.32d, 0.00d, 0.0d, 0.0d, 0.1d, 0.0d, 0.00d, 0.02d, 0.0d}, Relationship.EQ, 0.32d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.00d, 0.0d, 0.00d, 0.00d, 0.32d, 0.0d, 0.0d, 0.0d, 0.0d, 0.00d, 0.00d, 0.0d}, Relationship.EQ, 0.1d);
        cnsts.add(cnst);
        System.out.println(""Constraints:"");
        for(LinearConstraint con : cnsts) {
            System.out.println(con.getCoefficients().toString() + "" "" + con.getRelationship() + "" "" + con.getValue());
        }
        
        System.out.println(""verifying a known solution:"");
        ArrayRealVector sol = new ArrayRealVector(new double[] {4.0d/9.0d, 5.0d/9.0d, 0.0d, 11.0d/16.0d, 0.0d, 5.0d/16.0d, 0.0d, 4.0d/5.0d, 0.0d, 1.0d/5.0d, 0.0d, 1.0d, 0.0d});
        System.out.println(""sol = "" + sol);
        for(LinearConstraint con : cnsts) {
            System.out.println(sol.dotProduct(con.getCoefficients()) + "" = "" + con.getValue());
        }
        
        SimplexSolver simplex = new SimplexSolver();
        double[] newsol = simplex.optimize(c, cnsts, GoalType.MINIMIZE, true).getPointRef();
        System.out.println(""Solution:\n"" + new ArrayRealVector(newsol));
    }
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-10-14 20:16:39.548,,,false,,,,,,,,,,,,,,,,,34159,,,Thu Oct 15 08:56:56 UTC 2009,,,,,,,"0|i0rvan:",160714,,,,,,,,,,,,,,,,"14/Oct/09 20:16;bmccann;These are likely bugs that have already been fixed in the Subversion repository.  Can you try again with the latest version compiled from SVN?

I've verified that the first problem gives valid output with the SVN version:
Constraints:
{1; 1; 0; 0; 0; 0; 0} = 1.0
{0; 0; 1; 1; 1; 0; 0} = 1.0
{0; 0; 0; 0; 0; 1; 1} = 1.0
{0.54; 0; 0.34; 0; 0; 0.12; 0} = 0.54
{0; 0.54; 0; 0.34; 0; 0; 0.12} = 0.34
Solution:
{0.37; 0.63; 0.65; 0; 0.35; 1; 0}


I haven't completely checked the second one, but it appears to be correct as well with the SVN version:
Constraints:
{1; 1; 1; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0} = 1.0
{0; 0; 0; 1; 1; 1; 1; 0; 0; 0; 0; 0; 0} = 1.0
{0; 0; 0; 0; 0; 0; 0; 1; 1; 1; 0; 0; 0} = 1.0
{0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 1; 1; 1} = 1.0
{0.54; 0; 0; 0.32; 0; 0; 0; 0.1; 0; 0; 0.02; 0; 0} = 0.54
{0; 0.54; 0; 0; 0.32; 0; 0; 0; 0.1; 0; 0; 0.02; 0} = 0.32
{0; 0; 0; 0; 0; 0.32; 0; 0; 0; 0; 0; 0; 0} = 0.1
Solution:
[0.3703703703703704, 0.5925925925925926, 0.037037037037037035, 0.6875, 0.0, 0.3125, 0.0, 0.9999999999999999, 0.0, 0.0, 1.0, 0.0, 0.0]","15/Oct/09 07:41;cwinter;Alright. It works with the version from SVN.
Thanks.","15/Oct/09 08:56;luc;Already fixed in subversion repository by previous changes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CurveFitter.fit(ParametricRealFunction, double[]) used with LevenbergMarquardtOptimizer throws ArrayIndexOutOfBoundsException when double[] length > 1 (AbstractLeastSquaresOptimizer.java:187)",MATH-303,12438498,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,ddrummond,ddrummond,19/Oct/09 21:20,14/Apr/10 00:07,20/Mar/20 20:32,27/Dec/09 17:19,2.0,,,,,,,2.1,,0,,,,,,,,"CurveFitter.fit(ParametricRealFunction, double[]) throws ArrayIndexOutOfBoundsException at AbstractLeastSquaresOptimizer.java:187 when used with the  LevenbergMarquardtOptimizer  and the length of the initial guess array is greater than 1.  The code will run if the initialGuess array is of length 1, but then CurveFitter.fit() just returns the same value as the initialGuess array (I'll file this as a separate issue).  Here is my example code:
{code:title=CurveFitter with LevenbergMarquardtOptimizer and SimpleInverseFunction|borderStyle=solid}
  LevenbergMarquardtOptimizer optimizer = new LevenbergMarquardtOptimizer();
  CurveFitter fitter = new CurveFitter(optimizer);
  fitter.addObservedPoint(2.805d, 0.6934785852953367d);
  fitter.addObservedPoint(2.74333333333333d, 0.6306772025518496d);
  fitter.addObservedPoint(1.655d, 0.9474675497289684);
  fitter.addObservedPoint(1.725d, 0.9013594835804194d);
  SimpleInverseFunction sif = new SimpleInverseFunction(); // Class provided below
  double[] initialguess = new double[2];
  initialguess[0] = 1.0d;
  initialguess[1] = .5d;
  double[] bestCoefficients = fitter.fit(sif, initialguess); // <---- throws exception here

    /**
     * This is my implementation of ParametricRealFunction
     * Implements y = ax^-1 + b for use with an Apache CurveFitter implementation
      */
    private class SimpleInverseFunction implements ParametricRealFunction
    {
        public double value(double x, double[] doubles) throws FunctionEvaluationException
        {
            //y = ax^-1 + b
            //""double[] must include at least 1 but not more than 2 coefficients.""
            if(doubles == null || doubles.length ==0 || doubles.length > 2) throw new FunctionEvaluationException(doubles);
            double a = doubles[0];
            double b = 0;
            if(doubles.length >= 2) b = doubles[1];
            return a * Math.pow(x, -1d) + b;
        }
        public double[] gradient(double x, double[] doubles) throws FunctionEvaluationException
        {
            //derivative: -ax^-2
            //""double[] must include at least 1 but not more than 2 coefficients.""
            if(doubles == null || doubles.length ==0 || doubles.length > 2) throw new FunctionEvaluationException(doubles);
            double a = doubles[0];
            double b = 0;
            if(doubles.length >= 2) b = doubles[1];
            double derivative = -a * Math.pow(x, -2d);
            double[]gradientVector = new double[1];
            gradientVector[0] = derivative;
            return gradientVector; 
        }
    }

{code} 

This is the resulting stack trace:

java.lang.ArrayIndexOutOfBoundsException: 1
	at org.apache.commons.math.optimization.general.AbstractLeastSquaresOptimizer.updateJacobian(AbstractLeastSquaresOptimizer.java:187)
	at org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizer.doOptimize(LevenbergMarquardtOptimizer.java:241)
	at org.apache.commons.math.optimization.general.AbstractLeastSquaresOptimizer.optimize(AbstractLeastSquaresOptimizer.java:346)
	at org.apache.commons.math.optimization.fitting.CurveFitter.fit(CurveFitter.java:134)
	at com.yieldsoftware.analyticstest.tasks.ppcbidder.CurveFittingTest.testFitnessRankCurveIntercept(CurveFittingTest.java:181)","Java, Linux Ubuntu 9.04 (64 bit)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-12-27 17:19:25.907,,,false,,,,,,,,,,,,,,,,,150487,,,Sun Dec 27 17:19:25 UTC 2009,,,,,,,"0|i0rvaf:",160713,,,,,,,,,,,,,,,,"27/Dec/09 17:19;luc;The problem is not in the solver but in the implementation of the gradient method in your SimpleInverseFunction class. The length of the returned array must match the length of the second argument to the method (which is called parameters in the interface and doubles in your class). In your implementation, the array always has length 1 since it is created by statement:
{code}
double[]gradientVector = new double[1];
{code}

Also note that the value of the gradient is wrong. The gradient vector is computed with respect to the parameters (which is the reason why lengths must match), not with respect to the independent variable x. So for a function with two parameters p[0] / x + p[1], the gradient is { 1/x, 1 } and not { -p[0]/x^2, 0 }.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CurveFitter.fit(ParametricRealFunction, double[]) always returns the same value as the initial guess when used with the LevenbergMarquardtOptimizer",MATH-304,12438499,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,ddrummond,ddrummond,19/Oct/09 21:29,14/Apr/10 00:06,20/Mar/20 20:32,27/Dec/09 17:33,2.0,,,,,,,2.1,,0,,,,,,,,"CurveFitter.fit(ParametricRealFunction, double[]) always returns the same value as the initial guess when used with the LevenbergMarquardtOptimizer and the length of the initial guess array is 1.  Here is my example code:

{code:title=CurveFitter with LevenbergMarquardtOptimizer|borderStyle=solid}
  LevenbergMarquardtOptimizer optimizer = new LevenbergMarquardtOptimizer();
  CurveFitter fitter = new CurveFitter(optimizer);
  fitter.addObservedPoint(2.805d, 0.6934785852953367d);
  fitter.addObservedPoint(2.74333333333333d, 0.6306772025518496d);
  fitter.addObservedPoint(1.655d, 0.9474675497289684);
  fitter.addObservedPoint(1.725d, 0.9013594835804194d);
  SimpleInverseFunction sif = new SimpleInverseFunction(); // Class provided below
  double[] initialguess = new double[1];
  initialguess[0] = 1.0d;
  double[] bestCoefficients = fitter.fit(sif, initialguess); // <---- ALWAYS RETURNS A VALUE OF initialguess !

    /**
     * This is my implementation of ParametricRealFunction
     * Implements y = ax^-1 + b for use with an Apache CurveFitter implementation
      */
    private class SimpleInverseFunction implements ParametricRealFunction
    {
        public double value(double x, double[] doubles) throws FunctionEvaluationException
        {
            //y = ax^-1 + b
            //""double[] must include at least 1 but not more than 2 coefficients.""
            if(doubles == null || doubles.length ==0 || doubles.length > 2) throw new FunctionEvaluationException(doubles);
            double a = doubles[0];
            double b = 0;
            if(doubles.length >= 2) b = doubles[1];
            return a * Math.pow(x, -1d) + b;
        }
        public double[] gradient(double x, double[] doubles) throws FunctionEvaluationException
        {
            //derivative: -ax^-2
            //""double[] must include at least 1 but not more than 2 coefficients.""
            if(doubles == null || doubles.length ==0 || doubles.length > 2) throw new FunctionEvaluationException(doubles);
            double a = doubles[0];
            double b = 0;
            if(doubles.length >= 2) b = doubles[1];
            double derivative = -a * Math.pow(x, -2d);
            double[]gradientVector = new double[1];
            gradientVector[0] = derivative;
            return gradientVector; 
        }
    }
{code} 
","Java, Ubuntu 9.04 (64 bit)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-12-27 17:33:58.715,,,false,,,,,,,,,,,,,,,,,150488,,,Sun Dec 27 17:33:58 UTC 2009,,,,,,,"0|i0rva7:",160712,,,,,,,,,,,,,,,,"27/Dec/09 17:33;luc;The problem is not in the solver but in the implementation of the gradient method in your SimpleInverseFunction class. The value of the gradient is wrong. The gradient vector is computed with respect to the parameters (which is the reason why lengths must match), not with respect to the independent variable x. So for a function with one parameter p[0] / x, the gradient is { 1/x } and not { -p[0]/x^2 }.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Method 'divide' in class 'Complex' uses a false formula for a special case resulting in erroneous division by zero.,MATH-306,12438998,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,bowman2001,bowman2001,24/Oct/09 14:23,14/Apr/10 00:05,20/Mar/20 20:32,27/Oct/09 01:33,1.1,1.2,2.0,,,,,2.1,,0,,,,,,,,"The formula that 'divide' wants to implement is

( a + bi )  /  ( c + di )  =  ( ac + bd + ( bc - ad ) i )  /  ( c^2 + d^2 )

as correctly written in the description.

When c == 0.0 this leads to the special case

( a + bi )  /  di  = ( b / d ) - ( a / d ) i

But the corresponding code is:

if (c == 0.0) {
    return createComplex(imaginary/d, -real/c);
}

The bug is the last division -real/c, which should obviously be -real/d.",all,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-10-24 15:39:35.426,,,false,,,,,,,,,,,,,,,,,150489,,,Tue Oct 27 01:33:42 UTC 2009,,,,,,,"0|i0rv9r:",160710,,,,,,,,,,,,,,,,"24/Oct/09 14:31;bowman2001;Other layout and an additional formula conversion for better readability","24/Oct/09 15:39;psteitz;Thanks for reviewing the code.

The code is misleading, but should not lead to incorrect results.  The line you refer to above is actually in a block that will never get executed - so should be deleted.
 {code}
        if (Math.abs(c) < Math.abs(d)) {
            if (d == 0.0) {  <---- impossible to have abs(c) < 0
                return createComplex(real/c, imaginary/c);
            }
            double q = c / d;
            double denominator = c * q + d;
            return createComplex((real * q + imaginary) / denominator,
                (imaginary * q - real) / denominator);
        } else {
            if (c == 0.0) {  <-- to get here, we need c = d = 0,  but this is handled above.
                return createComplex(imaginary/d, -real/c);  <-- incorrect fmla is harmless because never executed.
            }
{code}


Interesting that static analyzers did not catch this as dead code.  Unless I am missing something, both of the if(* == 0) tests in the block above can be removed (with no effect on results).

Appreciate comments on this, but leaning toward code cleanup and closing as invalid.
","24/Oct/09 16:15;bowman2001;Yes, it looks like a twofold check for an already solved problem. By splitting the code up for the two cases
1. Math.abs(c) < Math.abs(d) 
2. the other way around
the denominator of q is always non-zero, which would habe been a problem and may have been the reason to apply those extra checks.

The special cases c = 0 and d = 0 lead always to q = 0, in the respective code parts. 
Which is no problem, because there is no division by q. 
Effective code, if the extra checks are omitted.

Thank you Phil, I encourage the code cleanup you proposed.","24/Oct/09 17:10;psteitz;Thanks, and thanks again for reviewing the code.   We really appreciate that.","27/Oct/09 01:33;psteitz;Removed dead code in r830044.",,,,,,,,,,,,,,,,,,,,,,,,,,
BigReal/Fieldelement divide without setting a proper scale -> exception: no exact representable decimal result,MATH-307,12438999,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,joan,joan,24/Oct/09 14:36,14/Apr/10 00:03,20/Mar/20 20:32,24/Jan/10 10:35,2.0,,,,,,,2.1,,0,,,,,,,,"BigReal implements the methode divide of Fieldelement. The problem is that there is no scale defined for the BigDecimal so the class will throw an error when the outcome is not a representable decimal result. 
(Exception: no exact representable decimal result)

The workaround for me was to copy the BigReal and set the scale and roundingMode the same as version 1.2.

Maybe is it possible to set the scale in FieldMatrix and implements it also a divide(BigReal b, int scale, int roundMode) ?? 

",independent,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-12-27 20:04:40.185,,,false,,,,,,,,,,,,,,,,,150490,,,Sun Jan 24 10:35:39 UTC 2010,,,,,,,"0|i0rv9j:",160709,,,,,,,,,,,,,,,,"27/Dec/09 20:04;luc;I have added rounding mode and scale fields in BigReal in subversion tree as of r894107.
Could you please check if this solves the issue for you ?","27/Dec/09 21:00;joan;
Thank you for your reply and effort.

If this method is replaced for the implementation of scale and roundingMode it will certainly work:

old method:

  public BigReal divide(BigReal a) throws ArithmeticException {
       return new BigReal(d.divide(a.d));
   }

suggested method:

  public BigReal divide(BigReal a) throws ArithmeticException {
       return new BigReal(d.divide(a.d, scale, roundingMode));
   }

Joan ","27/Dec/09 21:00;joan;
Thank you for your reply and effort.

If this method is replaced for the implementation of scale and 
roundingMode it will certainly work:

   public BigReal divide(BigReal a) throws ArithmeticException {
        return new BigReal(d.divide(a.d));
    }

is replaced by:

   public BigReal divide(BigReal a) throws ArithmeticException {
        return new BigReal(d.divide(a.d, scale, roundingMode));
    }

Joan

","27/Dec/09 21:08;luc;Oops. Sorry for having forgotten this ... I've checked it in now.","24/Jan/10 10:35;luc;Fixed since 2009-12-27 as of r894109",,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundException in EigenDecompositionImpl,MATH-308,12439042,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,25/Oct/09 22:25,14/Apr/10 00:02,20/Mar/20 20:32,03/Nov/09 22:19,2.0,,,,,,,2.1,,0,,,,,,,,"The following test triggers an ArrayIndexOutOfBoundException:

{code:java}
    public void testMath308() {

        double[] mainTridiagonal = {
            22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437
        };
        double[] secondaryTridiagonal = {
            13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225
        };

        // the reference values have been computed using routine DSTEMR
        // from the fortran library LAPACK version 3.2.1
        double[] refEigenValues = {
            14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002
        };
        RealVector[] refEigenVectors = {
            new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),
            new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),
            new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),
            new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),
            new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })
        };

        // the following line triggers the exception
        EigenDecomposition decomposition =
            new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);

        double[] eigenValues = decomposition.getRealEigenvalues();
        for (int i = 0; i < refEigenValues.length; ++i) {
            assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);
            if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {
                assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);
            } else {
                assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);
            }
        }

    }
{code}

Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:

{noformat}
java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545)
	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072)
	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894)
	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658)
	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246)
	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205)
	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)
{noformat}

I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.",linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,34207,,,Tue Nov 03 22:19:04 UTC 2009,,,,,,,"0|i0rv9b:",160708,,,,,,,,,,,,,,,,"03/Nov/09 22:19;luc;fixed in subversion repository as of r832577
Many thanks to Dimitri who debugged this with a careful step by step comparison between the original lapack fortran and our translation in Java.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChiSquaredDistributionImpl.cumulativeProbability > 1,MATH-282,12432549,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,akiezun,akiezun,07/Aug/09 18:24,13/Apr/10 10:26,20/Mar/20 20:32,08/Mar/10 23:00,1.0,1.1,1.2,2.0,,,,2.1,,0,,,,,,,,"Calling 
new ChiSquaredDistributionImpl(1.0).cumulativeProbability(66.41528551683048)

returns 1.000000000000004, which is bogus (should never be > 1)",called from Scala code,,,,,,,,,,,,"01/Mar/10 11:50;psteitz;distributions.patch;https://issues.apache.org/jira/secure/attachment/12437474/distributions.patch","08/Aug/09 16:05;luc;math-282.patch;https://issues.apache.org/jira/secure/attachment/12415937/math-282.patch",,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2009-08-08 16:05:08.945,,,false,,,,,,,,,,,,,,,,,34161,,,Mon Mar 08 23:00:26 UTC 2010,,,,,,,"0|i0rvev:",160733,,,,,,,,,,,,,,,,"08/Aug/09 16:05;luc;The problem seems to be related to numerical accuracy.
The computation here involves lots of iterations in the Gamma.regularizedGammaP method (115 iterations on my computer) and the final expression involves several transcendant functions (exp, log and logGamma).

The proposed patch simply filters out the values exceeding 1.0 in the gamma distribution implementation which is used by Chi squared distribution.

I am not sure this is the best fix to this issue, so I won't commit it myself. I prefer someone else could verify it.","09/Aug/09 18:07;psteitz;I am working on getting a bound in terms of the arguments for the Gamma distribution similar to what we do for the Normal distribution, so we can return 1 when we know that the tail probability is going to be bounded above by a sufficiently small number - say, 10E-20.   ","10/Aug/09 01:20;psteitz;Unfortunately, from R and [1], it looks like the value in the issue report should be ~ 1 - 10^-13, and I don't like the idea of top-coding with such a tight bound. Need to look at the regularized gamma approximation.

[1] ""The Moment Bound Is Tighter Than Chernoff's Bound for Positive Tail Probabilities"", Thomas K Philips, Randolph Nelson,  American Statistician 1995 vol. 49 (May, 1995) pp. 175-178","11/Oct/09 21:53;psteitz;The problem in Gamma.regularizedGammaP reported here is also causing incorrect results from PoissonDistributionImpl#cumulativeProbability.  The (currently disabled) test case, testCumulativeProbabilitySpecial() in PoissonDistributionTest illustrates the problem.  For some (not all) large lambda and some (not all) x in (0.9 * lambda, lambda), NaN or zero cumulative probabilities are returned.","22/Feb/10 12:04;psteitz;I have narrowed this down to two issues in Gamma

*  choice of when to use the series expansion vs continued fraction in computing the regularized gamma functions - changing from (a >= 1.0 && x > a) to (x > a + 1) as criteria for when to use continued fraction reduces incidence of NaN values returned and improves accuracy for some arguments.
*  handling the case when the continued fraction diverges.  I am still working on convincing myself that divergence is expected in failing test cases, in which case, logic can be changed to catch the continued fraction divergence and fall back to the series approximation in that case.  This will require refactoring the regularizedGammaP and Q functions to encapsulate the series / fraction computation instead of calling one another based on argument test.
","01/Mar/10 11:50;psteitz;The attached patch resolves this issue as well as MATH-301 and the problems described above with the Poisson distribution.  The patch bundles quite a few changes, some of which are not strictly necessary to resolve these issues.  The following is a summary.

* BrentSolver has been changed to expose its configured absolute accuracy.   This solver is used by the default inverse cum implementation in AbstractContinuousDistribution and the hard-coded setting (1E-6) was limiting accuracy in inverse cumulative probability estimates.  AbstractContinuousDistribution was changed to allow distributions to set this value and NormalDistributionImpl was changed to set it to 1E-9 by default and allow users to configure it via a constructor argument.   If all are happy with this change, I will similarly change other distributions to override the new getSolverAbsoluteAccuracy method and add constructors to configure the value.

* AbstractContinuousDistribution and AbstractIntegerDistribution inverseCumulativeProbability methods have been modified to check for NaN values returned by cumulativeProbability and throw MathExceptions when this happens.

* The criteria for choosing between the Lanczos series and continued fraction expansion when computing regularized gamma functions has been changed to (x >= a + 1).  When using the series approximation (regularizedGammaP), divergence to infinity is checked and when this happens, 1 is returned. 

* When scaling continued fractions to (try to) avoid divergence to infinity, the larger of a and b is used as a scale factor and the attempt to scale is repeated up to 5 times, using successive powers of the scale factor.  

* The maximum number of iterations used in estimating cumulative probabilities for PoissonDistributionImpl has been decreased from Integer.MAX_VALUE to 10000000 and made configurable.

Review and comment much appreciated.  One thing that I would like improve is to get decent top-coding in place in terms of the arguments to the regularized gamma functions.  The Poisson inverse cum tests take a very long time now because for very large values of x, the continued fractions are taking a long time to converge.  This is needless computation, as the value returned is 1.  We should be able to analytically determine bounds here.","01/Mar/10 19:25;luc;This looks fine to me (congrats for the error messages translations).
I am a little puzzled by the MathException thrown in some methods to be caught in the same method and wrapped in a FunctionEvaluationException. Could the MathException be a FunctionEvaluationException from the start (even if other MathException can be thrown and need to be wrapped by themselves) ?","01/Mar/10 20:18;psteitz;Thanks, Luc! 

Here is the perhaps strange logic explaining the odd exception nesting that you pointed out.  When a cumulativeProbability function returns NaN in the context of estimating inverse cum, the immediate exception is really a bad-value-returned exception, not a FunctionEvaluationException - there is no exception encountered evaluating the function, it just returns a bad value - so I code it as MathException.  The exception that the caller gets is FunctionEvaluationException, because there is in fact an error evaluating the inverse cum.  Wrapped inside is the MathException with the message indicating that NaN was returned by a cum activation.   

I guess it comes down to how we view FunctionEvaluationException and in particular is it appropriate to throw when NaN is returned by a method that logically should not return NaNs.  Thinking some more about this, I think so, so I will change the patch to throw FunctionEvaluationException in the first incidence.

Thanks again for looking at this carefully.  I am glad I got the translations right - the one I was worried about was as much English as French - ""diverge to NaN"" makes me cringe a little ;)","08/Mar/10 23:00;psteitz;Fixed in r920558",,,,,,,,,,,,,,,,,,,,,,
wrong result in eigen decomposition,MATH-318,12440038,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,luc,luc,06/Nov/09 15:09,03/Apr/10 20:49,20/Mar/20 20:32,06/Nov/09 15:12,2.0,,,,,,,2.1,,0,,,,,,,,"Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0
{code}
    public void testMathpbx02() {

        double[] mainTridiagonal = {
        	  7484.860960227216, 18405.28129035345, 13855.225609560746,
        	 10016.708722343366, 559.8117399576674, 6750.190788301587, 
        	    71.21428769782159
        };
        double[] secondaryTridiagonal = {
        	 -4175.088570476366,1975.7955858241994,5193.178422374075, 
        	  1995.286659169179,75.34535882933804,-234.0808002076056
        };

        // the reference values have been computed using routine DSTEMR
        // from the fortran library LAPACK version 3.2.1
        double[] refEigenValues = {
        		20654.744890306974412,16828.208208485466457,
        		6893.155912634994820,6757.083016675340332,
        		5887.799885688558788,64.309089923240379,
        		57.992628792736340
        };
        RealVector[] refEigenVectors = {
        		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),
        		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),
        		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),
        		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),
        		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),
        		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),
        		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})
        };

        // the following line triggers the exception
        EigenDecomposition decomposition =
            new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);

        double[] eigenValues = decomposition.getRealEigenvalues();
        for (int i = 0; i < refEigenValues.length; ++i) {
            assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);
            if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {
                assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);
            } else {
                assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);
            }
        }

    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,34177,,,Fri Nov 06 15:12:47 UTC 2009,,,,,,,"0|i0rv73:",160698,,,,,,,,,,,,,,,,"06/Nov/09 15:12;luc;fixed in subversion repository as of r833433.
Thanks again to Dimitri would found and fixed this bug.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"during ODE integration, the last event in a pair of very close event may not be detected",MATH-322,12442588,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,06/Dec/09 23:01,03/Apr/10 20:48,20/Mar/20 20:32,06/Dec/09 23:06,2.0,,,,,,,2.1,,0,,,,,,,,"When an events follows a previous one very closely, it may be ignored. The occurrence of the bug depends on the side of the bracketing interval that was selected. For example consider a switching function that is increasing around first event around t = 90, reaches its maximum and is decreasing around the second event around t = 135. If an integration step spans from 67.5 and 112.5, the switching function values at start and end of step will  have opposite signs, so the first event will be detected. The solver will find the event really occurs at 90.0 and will therefore truncate the step at 90.0. The next step will start from where the first step ends, i.e. it will start at 90.0. Let's say this step spans from 90.0 to 153.0. The switching function switches once again in this step.

If the solver for the first event converged to a value slightly before 90.0 (say 89.9999999), then the switch will not be detected because g(89.9999999) and g(153.0) are both negative.

This bug was introduced as of r781157 (2009-06-02) when special handling of events very close to step start was added.",All,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,34217,,,Sun Dec 06 23:06:04 UTC 2009,,,,,,,"0|i0rv6f:",160695,,,,,,,,,,,,,,,,"06/Dec/09 23:06;luc;fixed in subversion repository as of r887794",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways),MATH-326,12444282,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,jake.mannix,jake.mannix,29/Dec/09 00:09,03/Apr/10 20:37,20/Mar/20 20:32,29/Dec/09 12:26,2.0,,,,,,,2.1,,0,,,,,,,,"the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries.

The current implementation in ArrayRealVector has a typo:

{code}
    public double getLInfNorm() {
        double max = 0;
        for (double a : data) {
            max += Math.max(max, Math.abs(a));
        }
        return max;
    }
{code}

the += should just be an =.

There is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test, not a test for correctness).

Worse, the implementation in OpenMapRealVector is not even positive semi-definite:

{code}   
    public double getLInfNorm() {
        double max = 0;
        Iterator iter = entries.iterator();
        while (iter.hasNext()) {
            iter.advance();
            max += iter.value();
        }
        return max;
    }
{code}

I would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():

{code}
  public double getLInfNorm() {
    double norm = 0;
    Iterator<Entry> it = sparseIterator();
    Entry e;
    while(it.hasNext() && (e = it.next()) != null) {
      norm = Math.max(norm, Math.abs(e.getValue()));
    }
    return norm;
  }
{code}

Unit tests with negative valued vectors would be helpful to check for this kind of thing in the future.",all,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-12-29 12:26:59.335,,,false,,,,,,,,,,,,,,,,,34173,,,Tue Dec 29 12:26:59 UTC 2009,,,,,,,"0|i0rv5j:",160691,,,,,,,,,,,,,,,,"29/Dec/09 12:26;luc;Fixed in subversion repository as of r894367.
For consistency, the implementation of L1 and L2 norms have also been pushed upward in the abstract class.
Thanks for reporting and providing a fix to this bug",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong parameter for first step size guess for Embedded Runge Kutta methods,MATH-338,12446898,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,vincent.morand,vincent.morand,28/Jan/10 10:59,03/Apr/10 03:54,20/Mar/20 20:32,28/Jan/10 15:17,2.0,,,,,,,2.1,,0,,,,,,,,"In a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator.

Here, DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...)

The problem comes from the array ""scale"" that is used as a parameter in the call off initializeStep(..)

Following the theory described by Hairer in his book ""Solving Ordinary Differential Equations 1 : Nonstiff Problems"", the scaling should be :

sci = Atol i + |y0i| * Rtoli

Whereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli

Note that the Gragg-Bulirsch-Stoer integrator uses the good implementation ""sci = Atol i + |y0i| * Rtoli  "" when he performs the call to the same method initializeStep(..)

In the method initializeStep, the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user.
But in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...)


To fix the bug, one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator
For exemple :

 final double[] scale= new double[y0.length];;
          
          if (vecAbsoluteTolerance == null) {
              for (int i = 0; i < scale.length; ++i) {
                final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));
                scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * yi;
              }
            } else {
              for (int i = 0; i < scale.length; ++i) {
                final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));
                scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yi;
              }
            }
          
          hNew = initializeStep(equations, forward, getOrder(), scale,
                           stepStart, y, yDotK[0], yTmp, yDotK[1]);



Sorry for the length of this message, looking forward to hearing from you soon

Vincent Morand




",Eclipse sous Red Hat 5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-01-28 15:17:46.543,,,false,,,,,,,,,,,,,,,,,150511,,,Thu Jan 28 15:17:46 UTC 2010,,,,,,,"0|i0rv2v:",160679,,,,,,,,,,,,,,,,"28/Jan/10 15:17;luc;Fixed in subversion repository as of r904112.
Note that I have changed slightly the fix you proposed: the call to Math.max was not needed because both arguments were the same (in GBS integrator, they are different).
Note that instead of letting the integrator guess the first step by itself, you can provide it yourself by calling setInitialStepSize. This setting must be done before the call to integrate, which is called by Orekit propagate method if you happen to use Orekit for your application ;-) For such applications, an initial step of the order of magnitude of 1/100 of the keplerian period is a fair bet, it will be adjusted by the integrator if inconsistent with your accuracy settings.

Thanks for the report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BigFraction numerator constrainted by int size during multiplication,MATH-340,12455227,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mjtaylor,mjtaylor,03/Feb/10 20:17,03/Apr/10 03:45,20/Mar/20 20:32,03/Feb/10 21:21,2.0,,,,,,,2.1,,0,,,,,,,,"When multiplying two BigFraction objects with numerators larger than will fit in an java-primitive int the result of BigFraction.ZERO is incorrectly returned.


Test Case:

	        BigFraction fractionA = new BigFraction(0.00131);
	        BigFraction fractionB = new BigFraction(.37).reciprocal();
	        BigFraction errorResult = fractionA.multiply(fractionB);
	        System.out.println(""Error Result: "" + errorResult);
		BigFraction correctResult = new BigFraction(fractionA.getNumerator().multiply(fractionB.getNumerator()), fractionA.getDenominator().multiply(fractionB.getDenominator()));
	        System.out.println(""Correct Result: "" + correctResult);
",All,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-02-03 21:21:06.956,,,false,,,,,,,,,,,,,,,,,34197,,,Wed Feb 03 21:21:06 UTC 2010,,,,,,,"0|i0rv2f:",160677,,,,,,,,,,,,,,,,"03/Feb/10 21:21;luc;Fixed in subversion repository as of r906251.
Thanks for the report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SVD crashes when applied to a strongly rectangular matrix (typical case of least-squares problem),MATH-342,12456954,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,dimpbx,dimpbx,dimpbx,21/Feb/10 21:41,03/Apr/10 03:45,20/Mar/20 20:32,21/Feb/10 21:50,2.0,,,,,,,2.1,,0,,,,,,,,"When SVD is applied to a strongly rectangular matrix (number of rows way larger than number of columns, typical case of least-squares problem), finite precision arithmetics shows up:
 - in EigenDecompositionImpl.isSymmetric: a by-definition symmetric matrix returns false;
 - in EigenDecompositionImpl.findEigenVectors: too many iterations exception ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,150513,,,Sun Feb 21 21:50:20 UTC 2010,,,,,,,"0|i0rv1z:",160675,,,,,,,,,,,,,,,,"21/Feb/10 21:50;dimpbx;The two identified troublesome behaviors of EigenDecomposition are corrected.  Besides the regular unit tests, the two classes SingularValueDecompositionimpl and EigenDecompositionImpl have now been successfully tested over 300k+ systems coming from some astronomical application.  No crash reported!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Brent solver shouldn't need strict ordering of min, max and initial",MATH-347,12457428,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,aktasv,aktasv,25/Feb/10 14:33,03/Apr/10 03:06,20/Mar/20 20:32,01/Mar/10 19:39,2.0,,,,,,,,,0,,,,,,,,"The ""solve(final UnivariateRealFunction f, final double min, final double max, final double initial)"" function calls verifySequence() which enforces a strict ordering of min, max and initial parameters. I can't see why that is necessary - the rest of solve() seems to be able to handle ""initial == min"" and ""initial == min"" cases just fine. In fact, the JavaDoc suggests setting initial to min when not known but that is not possible at the moment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-03-01 19:39:24.285,,,false,,,,,,,,,,,,,,,,,150515,,,Mon Mar 01 19:39:24 UTC 2010,,,,,,,"0|i0rv0v:",160670,,,,,,,,,,,,,,,,"25/Feb/10 14:36;aktasv;Also would like to add that when this constraint is removed it should be possible to make the ""solve(final UnivariateRealFunction f, final double min, final double max)"" call the other solve() function without any additional logic.
 
","01/Mar/10 19:39;luc;Fixed in subversion repository as of r917668.
Thanks for reporting the issue",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compiler dependency in RandomDataImpl.getPoisson(double mean),MATH-354,12459572,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,gmicha,gmicha,18/Mar/10 21:08,03/Apr/10 03:05,20/Mar/20 20:32,18/Mar/10 22:34,2.0,,,,,,,2.1,,0,,,,,,,,"Hi,

in RandomDataImpl.getPoisson(double mean) I got the following problem in the case mean>= 6.0:

in the branch if (u <= c1):

if (x < -mu)
    w = Double.POSITIVE_INFINITY;

implicits that (int) (mu+ x) < 0

I found that for some compiler/run-time environments the subsequent update of the ""accept"" value then fails, as by the right hand side of the comparison leads to an Exception in MathUtils.factorialLog((int) (mu + x)). Some compiler/jre combinations, however, skip evaluating the right side as by isInfinity(w).

To ensure stability, I currently worked around by an explicit if(Double.isInfinity(w)) branch, however, I would like to ask whether there is a more elegant way to ensure general functionality of that method.

Thank you, micha. ",jre1.5/jre1.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-03-18 22:34:20.331,,,false,,,,,,,,,,,,,,,,,34157,,,Thu Mar 18 22:34:20 UTC 2010,,,,,,,"0|i0ruzb:",160663,,,,,,,,,,,,,,,,"18/Mar/10 22:34;psteitz;The method has been rewritten in version 2.1.  The current version no longer includes the compiler-dependent code referenced in this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Regression in package ""regression""",MATH-350,12458567,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Incomplete,,erans,erans,09/Mar/10 18:21,12/Mar/10 12:27,20/Mar/20 20:32,12/Mar/10 12:27,2.0,,,,,,,,,0,,,,,,,,"There is a regression in class ""OLSMultipleLinearRegression"".",,,,,,,,,,,,,"11/Mar/10 11:14;erans;OLSRegressionCompare20To21Test.java;https://issues.apache.org/jira/secure/attachment/12438501/OLSRegressionCompare20To21Test.java",,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,150518,,,Fri Mar 12 12:27:35 UTC 2010,,,,,,,"0|i0rv07:",160667,,,,,,,,,,,,,,,,"11/Mar/10 11:14;erans;Unit test from the user who discovered the issue. Copying it into the
  src/test/java/org/apache/commons/math/stat/regression
directory of the CM source tree, and running the tests will result in the following output:

{noformat}
Running org.apache.commons.math.stat.regression.OLSRegressionCompare20To21Test
Test parameters
Test variance
Expected 2.7104787386850834E-26 Actual 1.091646231028372E-25
Constructed a polynomial of degree 2
System will model various degrees of polynomial and use an Ftest to find the best model


Testing trend of degree 1
OLSRegressionCompare20To21Test: model statistic: 1545.9235701200896 threshold 3.9381110982233225
Model with degree 1 is better - keep testing higher order models

Testing trend of degree 2
OLSRegressionCompare20To21Test: model statistic: 8.9522618877706E31 threshold 3.939126144339758
Model with degree 2 is better - keep testing higher order models

Testing trend of degree 3
OLSRegressionCompare20To21Test: model statistic: 109.12572361790322 threshold 3.940162735266305
Model with degree 3 is better - keep testing higher order models

Testing trend of degree 4
OLSRegressionCompare20To21Test: model statistic: -40.823878611285174 threshold 3.941221564253479
Model with degree 4 is rejected - keeping simple model and exiting
Best model found with degree = 3
Coeff 0 = 3.000000000000148
Coeff 1 = 1.2000000000000006
Coeff 2 = 0.3399999999999999
Coeff 3 = 1.0632735348660702E-18
Test residuals
Test standard errors
Test hat matrix
Test parameter variance
{noformat}

So, although the last coefficient is nearly zero, it is conceptually wrong to return a fit with a polynomial of degree 3 whereas the input data was generated from a polynomial of degree 2.
[CM 2.0 behaved properly in this respect.]
","11/Mar/10 11:24;erans;Examining the history:
{noformat}
r825925 | luc | 2009-10-16 17:11:47 +0200 (Fri, 16 Oct 2009) | 1 line

replaced custom linear solve computation by use of the linear package features
{noformat}

The diff shows:
{noformat}
-        return solveUpperTriangular(qr.getR(), qr.getQ().transpose().operate(Y));
+        return qr.getSolver().solve(Y);
{noformat}
","12/Mar/10 12:27;erans;The initial reporter identified changes (of the order of 1e-13) in the values computed by the current code and those from 2.0. However we cannot assert which ones are ""better"" at this point.
The provided test does not clearly points to some deficiency in the CM source code.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test for firsst Derivative in PolynomialFunction ERROR,MATH-341,12455491,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,rfball,rfball,05/Feb/10 20:32,09/Feb/10 23:21,20/Mar/20 20:32,09/Feb/10 20:08,2.0,,,,,,,,,0,,,,,,,,"I have written the attached test using our data for generating a curve function

However the first derivative test fails see: testfirstDerivativeComparisonFullPower

Either my test is in error or there is a bug in PolynomialFunction class.


Roger Ball
Creoss Business Solutions ",,,,,,,,,,,,,"05/Feb/10 20:34;rfball;FirstDerivativePolyNomTest.java;https://issues.apache.org/jira/secure/attachment/12435005/FirstDerivativePolyNomTest.java",,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2010-02-05 21:50:33.314,,,false,,,,,,,,,,,,,,,,,34204,,,Tue Feb 09 23:21:27 UTC 2010,,,,,,,"0|i0rv27:",160676,,,,,,,,,,,,,,,,"05/Feb/10 20:34;rfball;FirstDerivativePolyNomTest.java","05/Feb/10 21:50;luc;I think there are no bugs here.
The first part of the test (method testfirstDerivativeComparison) runs without error.
The second part of the test (method testfirstDerivativeComparisonFullPower) prints coefficients that do not match.

However, the coefficients computed seem wrong to me because
* the getCoefficients method returns an array in increasing degree order (i.e. coeff[0] is the constant term)
* the coefficients array length is d+1 when d is the degree

This implies that the g_coeff array which should contain the coefficients of the derivative must have length myPolyNom.degree() and not myPolyNom.degree()-1 and the following loop should match.  Also the computation of the coefficients of the derivative shoud be:

  g_coeff[i] = f_coeff[i+1] * (i + 1);

instead of

  g_coeff[i] = f_coeff[i]*(myPolyNom.degree()-i);

With these changes, the automatic computation of derivative works.

A first comment on this case is that using polynomial fitting for such functions with large numbers of points shows very large Gibbs oscillations near the interval ends. One way to see this is to draw the curve from the sample points and from evaluation of myPolyNom with x varying from 5 to 30 with a 0.1 step. You will see that the polynomial fits the sample points perfectly, but near interval ends it has HUGE oscillations.

Another comment is that extracting the coefficients from the lagrangian form should be used with caution. Unfortunately, this is written only in the protected computationCoefficients() method javadoc, not in the public getCoefficients() method javadoc ... The computation is ill-conditioned and in fact the coefficients returned in your case are really bad. This could be seen by evaluating the original lagrangian form polynomial and the one reconstructed from the coefficients. The original does match the expected points, the reconstructed one does not.

So I think there is only a documentation problem: we should warn the user about extracting coefficients in this implementation.

Do you agree with this analysis ?","09/Feb/10 17:59;rfball;Regarding: 
""So I think there is only a documentation problem: we should warn the user about extracting coefficients in this implementation.

Do you agree with this analysis ?""

Certainly a documentation problem. However, if the coefficients do not result in a curve expression that is valid of the entire range of values for which the curve expression is derived then the curve expression is not really useful in a practical sense. I also the erractic occillations in curve. These occillations are HUGE when comparred to the data. Therefore the expression is not all together valid.","09/Feb/10 20:00;luc;I will fix the documentation.

The coefficients are valid when the number of points is reduced (for example consider only the 8 or 10 last points, it should work). Also note that polynomials in lagrangian form should really stay in this form and should not be converted to the canonical sum of monomials. It IS ill-conditioned so attempting this for high degree is not advised.

The oscillations you see are not an implementation problem, the mathematical solution is really like this. This is a theoretical problem known as Runge's phenomenon. See for example [http://demonstrations.wolfram.com/RungesPhenomenon/] and especially the high degree versions at the bottom of the page. You will see huge interpolation error at the end of the interval. When using polynomial interpolation, increasing the degree and the number of points but keeping them equidistant does NOT reduce maximal error (in fact for some functions error will tend towards infinity). The expression perfectly fits the sample points you provide but it has no information of what to do between them.

Polynomial interpolation should really be used with care. If you want smoother behavior, you should use several lower degrees polynomials each covering a subrange of your data.
","09/Feb/10 20:08;luc;added a warning in getCoefficients() method documentation
fixed in subversion repository as of r908190
thanks for reporting the problem","09/Feb/10 21:57;rfball;Looking at ""See for example http://demonstrations.wolfram.com/RungesPhenomenon/ "" it seems that if you select Chebyshev sample points the error drops dramatically all along the curve. Is this an option in the apache math library?","09/Feb/10 23:21;luc;You select the points yourself as you provides the x and y arrays, so yes, you can choos points at Chebyshev abscissas.
The array even don't need to be sorted. However, you cannot have two points with the same x value.",,,,,,,,,,,,,,,,,,,,,,,,
NPE in  KMeansPlusPlusClusterer unittest,MATH-305,12438781,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,erikvaningen,erikvaningen,22/Oct/09 06:35,30/Nov/09 08:12,20/Mar/20 20:32,27/Nov/09 21:46,2.0,,,,,,,2.1,,0,,,,,,,,"When running this unittest, I am facing this NPE:
java.lang.NullPointerException
	at org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91)

This is the unittest:


package org.fao.fisheries.chronicles.calcuation.cluster;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;

import java.util.Arrays;
import java.util.List;
import java.util.Random;

import org.apache.commons.math.stat.clustering.Cluster;
import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint;
import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer;
import org.fao.fisheries.chronicles.input.CsvImportProcess;
import org.fao.fisheries.chronicles.input.Top200Csv;
import org.junit.Test;

public class ClusterAnalysisTest {


	@Test
	public void testPerformClusterAnalysis2() {
		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>(
				new Random(1746432956321l));
		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] {
				new EuclideanIntegerPoint(new int[] { 1959, 325100 }),
				new EuclideanIntegerPoint(new int[] { 1960, 373200 }), };
		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1);
		assertEquals(1, clusters.size());

	}

}
","java 6, eclipse, apache commons math trunk",14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-11-27 16:57:43.788,,,false,,,,,,,,,,,,,,,,,34189,,,Mon Nov 30 08:12:50 UTC 2009,,,,,,,"0|i0rv9z:",160711,,,,,,,,,,,,,,,,"27/Nov/09 16:57;psteitz;Thanks for reporting this. ","27/Nov/09 21:46;psteitz;The problem was due to overflow in MathUtils.distance() due to bad typing.  Fixed in r885027.","30/Nov/09 08:12;erikvaningen;I have tested the fix and I can confirm that it is working in my environment. Thanks a lot!",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implementation of GeneticAlgorithm.nextGeneration() is wrong,MATH-207,12396580,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,david.stefka,david.stefka,21/May/08 21:50,07/Aug/09 09:20,20/Mar/20 20:32,30/Jun/09 00:44,2.0,,,,,,,2.0,,0,,,,,,,,"The implementation of GeneticAlgorithm.nextGeneration() is wrong, since the only way how a Chromosome can get into the new generation is by mutation. 

Enclosed, I am sending a patch for this.",,600,600,,0%,600,600,,,,,,"29/Sep/08 11:05;david.stefka@gmail.com;ASF.LICENSE.NOT.GRANTED--genetics_impl.zip;https://issues.apache.org/jira/secure/attachment/12391126/ASF.LICENSE.NOT.GRANTED--genetics_impl.zip","26/Mar/09 13:50;david.stefka@gmail.com;geneticAlgorithms.zip;https://issues.apache.org/jira/secure/attachment/12403714/geneticAlgorithms.zip","01/Mar/09 19:41;bmccann;geneticalgorithm.patch;https://issues.apache.org/jira/secure/attachment/12401205/geneticalgorithm.patch","21/May/08 21:51;david.stefka;patch;https://issues.apache.org/jira/secure/attachment/12382518/patch",,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2008-09-27 18:09:43.157,,,false,,,,,,,,,,,,,,,,,150422,,,Fri Aug 07 09:20:08 UTC 2009,,,,,,,"0|i0rvvb:",160807,,,,,,,,,,,,,,,,"21/May/08 21:51;david.stefka;Patch for the bug","27/Sep/08 18:09;psteitz;Patch applied to MATH_2_0 branch in r699704.

Leaving open because I think we need to either make nextGeneration public or protected or make it configurable.  Interested in comments on this.","29/Sep/08 11:05;david.stefka@gmail.com;Dear Phil,

if you are interested in the topic of genetic algorithms, I am sending a 
simple implementation of GA using the commons-math framework. If you find any 
of the ideas in it useful, you may use it.

Best regards,
David Stefka


-- 
David Stefka
david.stefka@gmail.com (personal mail)

","27/Feb/09 02:07;chengas123;I think that making nextGeneration public would be reasonable.
A few other thoughts about the API:
All the setters should be changed to constructor arguments.  If you do not call each of the setters then you are in an illegal state.  You avoid the problem of having some call evolve before the mutation policy and crossover policy are set if you just make them required in the constructor.
For crossOverRate and mutationRate we should specify that the input is expected to be between 0 and 1 and throw an exception when given invalid input.  It's currently not clear in the javadocs what is expected.","01/Mar/09 19:41;bmccann;Patch to improve the API.","09/Mar/09 18:50;bmccann;Any thoughts on the patch submitted to clean up the API?","18/Mar/09 12:50;david.stefka@gmail.com;I think the ideas to clean up the API are okay. However, I don't know whether the patch is against the latest version. Things are starting to be a bit messy... :)","19/Mar/09 02:00;psteitz;Sorry for delay in reviewing these patches.  I like both the implementation (David's first patch) and the API improvements and will apply them both if I can get answers to the following.

1) David - pls confirm that you can grant the code in your patch.  It is a borderline case whether or not we need a code grant.  Given that it just implements the commons-math ga framework, I think it is OK to commit without a grant; but since you did not include ASL license headers in the patch, I need you to confirm that you own the code and can freely contribute it under the terms of the Apache Contributor License Agreement.  If you don't mind, it would be good to file a CLA.

2) David's patch looks like it has some JDK 1.6 dependencies.  Math 2.0 targets 1.5+, so these need to be removed.  I can do this; but a revised patch with these removed and ASL headers would be appreciated.

3) Ben - why expose fields as protected?  

Thanks for the patches!","19/Mar/09 15:46;david.stefka@gmail.com;ad 1) what do you mean by ""file a CLA""? Should I include the license header (e.g. in GeneticAlgorithm.java) to every file in the framework? I do own the code and I agree with the Apache license (http://www.apache.org/licenses/LICENSE-2.0)
ad 2) okay, i will try to make it Java 1.5+ compatible

BTW, I am currently working on a better way of representing permutations, so do not include the original patch now. I will post here a revised version of the implementation in a week or so.","19/Mar/09 20:18;psteitz;Thanks, David!

All files should include headers like what you see in GenericAlgorithm.java or any other apache java source file.  As long as you own the code and are willing to attach those headers, we should be OK.   A Contributors Licence Agreement (CLA) is a good thing to have on file and will be required in any case should you become a committer.  Have a look at the ""Contributor License Agreements"" section at http://www.apache.org/licenses/ to find the form.  The form itself includes info on how to submit it.

Thanks again for your conttribution.
","26/Mar/09 13:48;david.stefka@gmail.com;Hi again,

I have finished work on better representation of permutations in GA, so I am sending the implementation in a .zip file. The code includes:

 * new implementation of RandomKey -- chromosome representing permutations; the old PermutationChromosome implementation has been removed
 * some minor improvements (chromosome's representation is held in an unmodifiable list instead of an array, better implementation of generic issues, renaming of several classes, etc.)
 * GeneticAlgorithm has a static Random instance, which is used for all random numbers in the implementation. This is useful for debugging, because if the Random is initialized to a particular seed, the behavior can be reproduced. If a similar mechanism is already somewhere in the
 * the code should be java 1.5 compatible
 * all the files should include the Apache license headers

I have also filled and signed the CLA and sent it to secretary@apache.org","26/Mar/09 13:50;david.stefka@gmail.com;New implementation of basic GA algorithms","04/May/09 20:08;chengas123;Sorry for my delayed response.  JIRA didn't email me on any of these updates for whatever reason.
There's no real need to make the fields protected in my patch.  Private would be fine.  I've just gotten in the habit of frequently using protected to allow easier subclassing and unit testing, but those fields all have public getters, so no harm in making them private.","14/Jun/09 19:08;psteitz;Committed a slightly modified version of David's last patch in r784604.

Other than minor javadoc/formatting changes to make checkstyle happy, I also made the shared source of randomness pluggable.

I am not 100% happy with the static randomGenerator attached to GeneticAlgorithm, though I understand and support the need to ensure reproducibility for some applications.  Comments / suggestions for better ways to do this welcome.

Leaving open as we need to update the user guide to complete this.","30/Jun/09 00:44;psteitz;User guide updated in r789511","07/Aug/09 09:20;luc;closing resolved issue for 2.0 release",,,,,,,,,,,,,,,
Basic variable is not found correctly in simplex tableau,MATH-273,12426998,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,bmccann,bmccann,03/Jun/09 05:11,07/Aug/09 09:15,20/Mar/20 20:32,03/Jun/09 09:07,2.0,,,,,,,2.0,,0,,,,,,,,"The last patch to SimplexTableau caused an automated test suite I'm running at work to go down a new code path and uncover what is hopefully the last bug remaining in the Simplex code.
SimplexTableau was assuming an entry in the tableau had to be nonzero to indicate a basic variable, which is incorrect - the entry should have a value equal to 1.",,,,,,,,,,,,,"03/Jun/09 05:12;bmccann;SimplexSolverTest.patch;https://issues.apache.org/jira/secure/attachment/12409728/SimplexSolverTest.patch","03/Jun/09 05:12;bmccann;SimplexTableau.patch;https://issues.apache.org/jira/secure/attachment/12409727/SimplexTableau.patch",,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2009-06-03 09:07:06.044,,,false,,,,,,,,,,,,,,,,,34190,,,Fri Aug 07 09:15:43 UTC 2009,,,,,,,"0|i0rvgv:",160742,,,,,,,,,,,,,,,,"03/Jun/09 05:12;bmccann;Here's the patch.","03/Jun/09 09:07;luc;fixed in subversion repository as of r781304
patch applied
thanks","07/Aug/09 09:15;luc;closing resolved issue for 2.0 release",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplex Solver arrives at incorrect solution,MATH-272,12426720,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,bmccann,bmccann,30/May/09 01:01,07/Aug/09 09:15,20/Mar/20 20:32,02/Jun/09 19:38,2.0,,,,,,,2.0,,0,,,,,,,,I have reduced the problem reported to me down to a minimal test case which I will attach.,,,,,,,,,,,,,"30/May/09 01:02;bmccann;SimplexSolvetTest.txt;https://issues.apache.org/jira/secure/attachment/12409441/SimplexSolvetTest.txt","30/May/09 03:12;bmccann;SimplexTableau.patch;https://issues.apache.org/jira/secure/attachment/12409449/SimplexTableau.patch",,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2009-06-02 19:38:46.461,,,false,,,,,,,,,,,,,,,,,34175,,,Fri Aug 07 09:15:24 UTC 2009,,,,,,,"0|i0rvh3:",160743,,,,,,,,,,,,,,,,"30/May/09 01:02;bmccann;Test case to be added to SimplexSolverTest.  It is currently failing and should be fixed.","30/May/09 03:12;bmccann;Previously, there was a bug where we could set one of a number of variables equal to some value.  We were setting all the variables instead of choosing one.  When I patched that bug, I did it incorrectly.  This is a correct implementation, which causes the old bug and the attached test to both pass.","02/Jun/09 19:38;luc;fixed in subversion repository as of r781135
patch applied
thanks","02/Jun/09 19:53;bmccann;Thanks!



","07/Aug/09 09:15;luc;closing resolved issue for 2.0 release",,,,,,,,,,,,,,,,,,,,,,,,,,
"MathUtils.binomialCoefficient(n,k) fails for large results",MATH-241,12412642,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,chsemrau,chsemrau,16/Jan/09 23:34,23/Apr/09 02:26,20/Mar/20 20:32,19/Jan/09 23:53,2.0,,,,,,,2.0,,0,,,,,,,,"Probably due to rounding errors, MathUtils.binomialCoefficient(n,k) fails for results near Long.MAX_VALUE.

The existence of failures can be demonstrated by testing the recursive property:

{noformat}
         assertEquals(MathUtils.binomialCoefficient(65,32) + MathUtils.binomialCoefficient(65,33),
                 MathUtils.binomialCoefficient(66,33));
{noformat}

Or by directly using the (externally calculated and hopefully correct) expected value:

{noformat}
         assertEquals(7219428434016265740L, MathUtils.binomialCoefficient(66,33));
{noformat}

I suggest a nonrecursive test implementation along the lines of

{code:title=MathUtilsTest.java|borderStyle=solid}
    /**
     * Exact implementation using BigInteger and the explicit formula
     * (n, k) == ((k-1)*...*n) / (1*...*(n-k))
     */
	public static long binomialCoefficient(int n, int k) {
		if (k == 0 || k == n)
			return 1;
		BigInteger result = BigInteger.ONE;
		for (int i = k + 1; i <= n; i++) {
			result = result.multiply(BigInteger.valueOf(i));
		}
		for (int i = 1; i <= n - k; i++) {
			result = result.divide(BigInteger.valueOf(i));
		}
		if (result.compareTo(BigInteger.valueOf(Long.MAX_VALUE)) > 0) {
			throw new ArithmeticException(
                                ""Binomial coefficient overflow: "" + n + "", "" + k);
		}
		return result.longValue();
	}
{code} 

Which would allow you to test the expected values directly:

{noformat}
         assertEquals(binomialCoefficient(66,33), MathUtils.binomialCoefficient(66,33));
{noformat}
",,,,,,,,,,,,,"19/Jan/09 18:40;psteitz;binomialPatch.txt;https://issues.apache.org/jira/secure/attachment/12398252/binomialPatch.txt","19/Jan/09 22:05;chsemrau;binomialPatch_cs.txt;https://issues.apache.org/jira/secure/attachment/12398265/binomialPatch_cs.txt",,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2009-01-19 18:40:57.164,,,false,,,,,,,,,,,,,,,,,34152,,,Mon Jan 19 23:53:48 UTC 2009,,,,,,,"0|i0rvnz:",160774,,,,,,,,,,,,,,,,"19/Jan/09 18:40;psteitz;First, thanks for reporting this.  Due to log/exp rounding and double/long conversion, the current code returns bad values for many long-representable values, starting as low as n = 48.  The returned value can be off by as much as 200,000.  The error in binomial(66, 29) is 214,880.  All b(n,k) for n < 48 are exact.

Attached is a patch that ensures accuracy up to n = 200 (specified as a constant) and allows the user to force exact computation for values beyond this if desired.  For n <= 200, the implementation works like an unwound recursive implementation.   I also improved the accuracy of the double-valued and log versions.   The latter perform better than the current implementations, but the long-valued version is approximately 8x slower than the current version.  I did not benchmark the BigInteger version, but suspect that would be slower still.  The most accurate (for n <= 200) non-recursive formula that I could find is the one that I implemented in the double version.

I also investigated overflow behavior and added tests to confirm correctness.  As stated in the API doc, overflows start at n = 67.  For n = 200,  values of k less than 14 or greater than 186 can still be computed without overflow; but all others throw ArithmeticException.

I would appreciate feedback on the patch and any better ideas on how to fix the problem.","19/Jan/09 22:05;chsemrau;Attached is my version of a new binomialCoefficient function.","19/Jan/09 22:16;chsemrau;I think the recursive computation of Pascal's triangle (even with caching or dynamic programming) is unnecessarily complicated except to ensure correct values.

The attached patch ensures accuracy for all values that can be represented as a long integer, with a running time proportional to k*log(k) (assuming gcd(i,j) takes log(j) steps). It should be faster than the current version for n <= 61, but for n > 61 my version computes as much as k different gcd values, which might be slower.

I did not modify the double and log version, but your patch can be applied to these.","19/Jan/09 23:53;psteitz;Applied second patch along with changes to double, log versions from first patch in r735879.  Many thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Cumulative probability and inverse cumulative probability inconsistencies,MATH-692,12527632,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,cwinter,cwinter,18/Oct/11 18:01,23/Jul/12 23:18,20/Mar/20 20:32,02/Feb/12 06:45,1.0,1.1,1.2,2.0,2.1,2.2,3.0,3.0,,0,,,,,,,,"There are some inconsistencies in the documentation and implementation of functions regarding cumulative probabilities and inverse cumulative probabilities. More precisely, '<' and '<=' are not used in a consistent way.

Besides I would move the function inverseCumulativeProbability(double) to the interface Distribution. A true inverse of the distribution function does neither exist for Distribution nor for ContinuosDistribution. Thus we need to define the inverse in terms of quantiles anyway, and this can already be done for Distribution.

On the whole I would declare the (inverse) cumulative probability functions in the basic distribution interfaces as follows:

Distribution:
- cumulativeProbability(double x): returns P(X <= x)
- cumulativeProbability(double x0, double x1): returns P(x0 < X <= x1) [see also 1)]
- inverseCumulativeProbability(double p):
  returns the quantile function inf{x in R | P(X<=x) >= p} [see also 2), 3), and 4)]

1) An aternative definition could be P(x0 <= X <= x1). But this requires to put the function probability(double x) or another cumulative probability function into the interface Distribution in order be able to calculate P(x0 <= X <= x1) in AbstractDistribution.
2) This definition is stricter than the definition in ContinuousDistribution, because the definition there does not specify what to do if there are multiple x satisfying P(X<=x) = p.
3) A modification could be defined for p=0: Returning sup{x in R | P(X<=x) = 0} would yield the infimum of the distribution's support instead of a mandatory -infinity.
4) This affects issue MATH-540. I'd prefere the definition from above for the following reasons:
- This definition simplifies inverse transform sampling (as mentioned in the other issue).
- It is the standard textbook definition for the quantile function.
- For integer distributions it has the advantage that the result doesn't change when switching to ""x in Z"", i.e. the result is independent of considering the intergers as sole set or as part of the reals.

ContinuousDistribution:
nothing to be added regarding (inverse) cumulative probability functions

IntegerDistribution:
- cumulativeProbability(int x): returns P(X <= x)
- cumulativeProbability(int x0, int x1): returns P(x0 < X <= x1) [see also 1) above]",,,,,,,,,,,,,"19/Dec/11 22:47;cwinter;MATH-692_integerDomain_patch1.patch;https://issues.apache.org/jira/secure/attachment/12507992/MATH-692_integerDomain_patch1.patch","08/Nov/11 20:28;cwinter;Math-692_realDomain_patch1.patch;https://issues.apache.org/jira/secure/attachment/12502956/Math-692_realDomain_patch1.patch",,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-10-20 01:13:06.429,,,false,,,,,,,,,,,,,,,,,88883,,,Thu Feb 02 06:45:59 UTC 2012,,,,,,,"0|i0aovb:",60302,,,,,,,,,,,,,,,,"20/Oct/11 01:13;psteitz;Thanks for raising this issue, Christian - especially now as we finalize the 3.0 API.

I am +1 for these changes.  I agree that the inf-based definition of inverse cum is more standard and we are in a position now make the change, so I say lets do it.  I am also +1 on the move of this up to the distribution interface.  The reason we did not include it there originally was that we thought we might implement distributions for which we could not define inverses.  That has not happened in the last 8 years, so I think its safe enough to push it up.

The code, test, user guide and doc changes for this have to be done carefully.  Patches most welcome.

Is everyone else OK with this change?","20/Oct/11 06:09;celestin;I have neither used nor developed this part of CM, so my view on this is of but little value. Having said that, anything improving consistency can only be desirable, especially at this stage. So I'm all for it, and will be soon available (when I'm done on SYMMLQ) for an (novice on these issues) help.

Sébastien","20/Oct/11 06:26;mikl;+1","22/Oct/11 17:10;cwinter;Thanks for the feedback to all. Sébastien, thanks for offering your help. If you like and find time for it, you could implement AbstractDistribution.inverseCumulativeProbability(double p).

I will provide some patches next week, but adjusting AbstractContinuousDistribution.inverseCumulativeProbability(double p) will take some more time.

After thinking a little more about the structure of the interfaces, I'd like to put the function probability(double x) to Distribution anyway (independently of the thought in point 1) above).

Are there any preferences on P(x0 <= X <= x1) or P(x0 < X <= x1) for cumulativeProbability(double x0, double x1)?","22/Oct/11 20:24;psteitz;I am not sure it is really makes sense to add probability(double x) to the Distribution interface.  It would have to be defined as density (referring to the distribution function) to make sense in the continuous case, since defined as p(X = x) it would in most cases be identically 0 for continuous distributions.

Regarding the cum definition, I am fine with P(x0 < X <= x1).","23/Oct/11 08:39;celestin;Happy to help on the inverse cumulative probability. You will have to be patient and forgieving with me, though, as I discover this part of CM.

As for the definition, I think that one of the bounds should be excluded, so that these cumulative probabilities can be summed
P(a < X <= c) = P(a < X <= b) + P(b < X <= c),
even in the case of discrete PDFs.

Whether the lower or upper bound should be excluded is another matter. I usually work with continuous pdfs, so I don't know if there is a common practice in the probability community. If there is none, I would tend to chose the following definition
P(x0 <= X < x1)
(sorry Phil!), because it would be consistent with the way things are usually indexed in java (a[0].. a[a.length-1]). See also {{org.apache.commons.math.util.MultidimensionalCounter}}. Although this type of consistency is not an absolute requirement, I think it is nice for the user to have such simple principle: ""lower bound always included, upper bound always excluded"". Appart from this small point, I really have no objection to any choice.","23/Oct/11 15:22;psteitz;Have a look at the default implementation of cum(x0,x1) now in AbstractDistribution.  I think the incorrectness in the documentation there may have been what triggered Christian to raise this issue.  The equation cum(a,b) = F(b) - F(a) where F is the distribution function is natural and what the impl there is trying to do.  In the discrete case, this equation fails, however, unless you define the cum to exclude the *lower* endpoint.  That's why P(x0 < X <= x1) is a better definition.","23/Oct/11 15:31;celestin;OK, Phil, it makes perfect sense.","23/Oct/11 19:57;cwinter;Good, the definition of cum(x0,x1) will be P(x0 < X <= x1). Phil, you are right: cum(x0,x1) in AbstractDistribution was a reason for raising this issue. Another reason was cum(int x0, int x1) in AbstractIntegerDistribution.

The idea behind probability(double x) is in fact to define it as p(X = x) and to return 0 for continuous distributions. This function would be useful for discrete distributions not inheriting from IntergerDistribution and for distributions being composed of discrete and continuous parts.","23/Oct/11 20:59;psteitz;I guess I am OK with pushing p(x) up.  See related post to follow in commons-dev. ","27/Oct/11 07:07;celestin;Hi Christian,
I've started looking into this issue. As I said, you will have to be patient with me ;).
I can see there already is a default implementation of {{AbstractContinuousDistribution.inverseCumulativeProbability}}. So what exactly would you like me to do? Is this implementation fragile? Would you like me to improve robustness? Provide full testing?

I think there might be issues when the PDF falls down to zero in a range (in which case the cum exhibits a plateau). The returned value might differ from the mathematical definition you proposed. Is this what you want me to work on? Have you already identified other issues?

Best regards,
Sébastien","28/Oct/11 21:36;cwinter;Hi Sébastien,

the problem with the plateau is indeed one issue which needs to be solved.

Additionally, AbstractDistribution will need an implementation of inverseCumulativeProbability. In fact both implementations should be the same except for the solver to be used. Thus inverseCumulativeProbability should be implemented just once in AbstractDistribution, and invoking the solver should be put to a separate procedure so that it can be overridden in AbstractContinuousDistribution.

A third point is the choice of the solvers. For AbstractDistribution we need a solver which works even for discontinuous cdfs (BisectionSolver can do the job, but maybe the implementations of the faster IllinoisSolver, PegasusSolver, BrentSolver, or another solver can cope with discontinuities, too). For AbstractContinuousDistribution it would be beneficial to use a DifferentiableUnivariateRealSolver. However, the NewtonSolver cannot be used due to uncertainty of convergence and an alternative doesn't seem to exist by now. So we have to choose one of the other solvers for now.

As all these points are interdependent, I guess it's best to solve them as a whole. If you like, you can do this.

Best Regards,
Christian","28/Oct/11 22:02;cwinter;Another point for discussion:
I'd like to introduce
getDomainBracket(double p): returns double[]
to AbstractDistribution as helper function for inverseCumulativeProbability. This allows to avoid searching a bracket where a bracket can be specified directly.
The function getDomainBracket could be made abstract (which means to remove getInitialDomain, getDomainLowerBound, and getDomainUpperBound as these functions aren't needed any more), or it could have a default implementation (according to the corresponding part of the current implementation of inverseCumulativeProbability) which uses getInitialDomain, getDomainLowerBound, and getDomainUpperBound. However, getInitialDomain, getDomainLowerBound, and getDomainUpperBound should not be abstract in the latter case. Otherwise a derived class would be forced to implement something it potentially doesn't use. Thus the functions getInitialDomain, getDomainLowerBound, and getDomainUpperBound should have default implementations which either return default values (0, -infinity, +infinity) or throw an exception saying something like ""has to be implemented"".","29/Oct/11 04:23;celestin;Hi Christian,

{quote}
Hi Sébastien,

the problem with the plateau is indeed one issue which needs to be solved.
{quote}
I'm working on it...

{quote}
Additionally, AbstractDistribution will need an implementation of inverseCumulativeProbability. In fact both implementations should be the same except for the solver to be used. Thus inverseCumulativeProbability should be implemented just once in AbstractDistribution, and invoking the solver should be put to a separate procedure so that it can be overridden in AbstractContinuousDistribution.
{quote}
OK, for now, I'm concentrating on making the current impl in {{AbstractContinuousDistribution}} more robust. The other impl should be easier.

{quote}
A third point is the choice of the solvers. For AbstractDistribution we need a solver which works even for discontinuous cdfs (BisectionSolver can do the job, but maybe the implementations of the faster IllinoisSolver, PegasusSolver, BrentSolver, or another solver can cope with discontinuities, too). For AbstractContinuousDistribution it would be beneficial to use a DifferentiableUnivariateRealSolver. However, the NewtonSolver cannot be used due to uncertainty of convergence and an alternative doesn't seem to exist by now. So we have to choose one of the other solvers for now.
{quote}
The current implementation uses a Brent solver. I think the solver itself is only one side of the issue. The other point is the algorithm used to bracket the solution, in order to ensure that the result is consistent with the definition of the cumprob. As for the {{DifferentiableUnivariateRealSolver}}, I'm not too sure. I guess it depends on what is meant by ""continuous distribution"". For me, it means that the random variable takes values in a continuous set, and possibly its distribution is defined by a density. However, in my view, nothing prevents occurences of Dirac functions, in which case the cum sum is only piecewise C1. It's all a matter of definition, of course, and I'll ask the question on the forum to check whether or not people want to allow for such a situation.

{quote}
As all these points are interdependent, I guess it's best to solve them as a whole. If you like, you can do this.

Best Regards,
Christian
{quote}
Yes, I'm very interested.

Best regards,
Sébastien","05/Nov/11 08:29;celestin;Please note that MATH-699 has been created specifically to handle plateaux.

Sébastien","08/Nov/11 20:28;cwinter;Here is the first patch for this issue (unfortunately with some delay). It adjusts the distributions with real domain to the definitions in this issue, and it mainly changes documentations.

I could not move inverseCumulativeProbability(double) up to Distribution because there would be a conflict with IntegerDistribution.inverseCumulativeProbability(double): This method returns int. This problem will be removed by solving issue MATH-703.

The implementation of inverseCumulativeProbability(double) is not changed as Sébastien is working on this.

I will provide the patch for the integer distributions as soon as I have adjusted the test data to the new inequalities and reverified the adjusted test data.","09/Nov/11 07:22;celestin;All,
since I'm already working on this package, I'm happy to commit the patch on behalf of Christian. However, since I'm a relatively new committer, I would feel more confident if one of the ""old, wise committers"" could double check the svn log afterwards.

Best,
Sébastien","09/Nov/11 15:48;psteitz;Hey, that's how it always works :)  

I don't know about ""wise"" but I certainly qualify as ""old"" by any standard, so will have a look once you have reviewed and committed.

Thanks!","10/Nov/11 06:23;celestin;Patch {{Math-692_realDomain_patch1.patch}} (20111108) applied in rev 1200179, with minor modifications (mostly checkstyle fixes).
Thanks Christian!
","04/Dec/11 21:02;cwinter;As mentioned by Sébastien in MATH-699, the implementation of {{IntegerDistribution.inverseCumulativeProbability(double p)}} can benefit from the ideas which came up for {{RealDistribution.inverseCumulativeProbability(double p)}} in that thread.

Thus I will remove {{getDomainLowerBound(double p)}} and {{getDomainUpperBound(double p)}} from the integer distributions. I checked that all current implementations of the lower/upper bound methods provide the whole support of the distribution as starting bracket. This means that using {{getSupportLowerBound()}} and {{getSupportUpperBound()}} for the starting bracket won't degrade the performance of the current distribution implementations. However, a user might want the improve the performance of his distribution implementations by providing a more targeted starting bracket for probability {{p}}. Thus I will swap the solving step to a protected function {{solveInverseCumulativeProbability(double p, int lower, int upper)}}, so that it gets easy to override {{inverseCumulativeProbability}} with an implementation which finds a better starting bracket.

Furthermore, Phil's idea with Chebyshev's inequality can be applied to the generic implementation of {{inverseCumulativeProbability}} in order to get a better starting bracket.","05/Dec/11 06:51;celestin;Hi Christian,
If you agree with that, I suggest that you also take care of MATH-718, as the two issues seem to be very much connected.
Sébastien","15/Dec/11 23:44;cwinter;Hi Sébastien,

my changes in the integer distributions don't solve MATH-718. Instead I found a probably related problem with the Pascal distribution.

The integer distribution patch for this issue still isn't ready. I will provide it next week.

Christian","19/Dec/11 22:47;cwinter;This is the patch which adjusts the integer distributions to the agreements above.

The changes to the test cases for the random generators may be unexpected. But these changes initially were triggered by adjusting {{RandomDataTest.checkNextPoissonConsistency(double)}} to the new contract for integer distributions. Then some random generator tests failed due to chance. While adjusting their seeds, I found some other tests with a high failure probability. Thus I also set some failure probabilities to 0.01 in order to find suitable seeds more quickly.

My next task on this issue is to adjust the user guid.","20/Dec/11 20:27;celestin;Hi Christian,
thanks for this contribution. I am away for a few days, but am very happy to commit this patch as soon as I am back, if you are not in too much of a hurry.
Thanks again,
Sébastien","31/Dec/11 05:25;celestin;Well, we've recently run into some troubles with SVN, but it seems everything is working fine again. Patch {{MATH-692_integerDomain_patch1.patch}} (with minor checkstyle changes) committed in revision {{1226041}}.

Please do not forget to run {{mvn clean; mvn site:site}} and check the reports (in particular, {{checkstyle}}) prior to submitting a patch!

Thanks for this contribution.","31/Dec/11 08:39;celestin;The committed patch actually causes failure of {{Well1024Test}} in {{o.a.c.m.random}}.","31/Dec/11 17:01;cwinter;Thanks for committing the patch, Sébastien. I see you already changed the seed in {{Well1024aTest}}. This hopefully removes the failure.

I'll have a look into Maven to prepare a better patch next time. :-)","31/Dec/11 17:11;celestin;{quote}
I see you already changed the seed in Well1024aTest.
{quote}

Yes I did, but is this really how we want {{Well2004aTest}} to pass?","02/Jan/12 17:51;cwinter;I guess there is no alternative to this way of making probabilistic test cases pass. However, I understand your bad feeling with this kind of failure fixing. The problem is that probabilistic tests are quiet fuzzy: Neither a passed test nor a failed test provides a clear answer whether something is right or wrong in the implementation. There is just a high chance to pass such a test with a correct implementation. The chance for failure increases with an erroneous implementation due to systematic deviations in the generated data. These chances tell whether it is easy to find a seed which passes the tests or not. Thus difficulties in finding a suitable seed are an indicator for problems in the code.","02/Jan/12 18:53;celestin;{quote}
Thus difficulties in finding a suitable seed are an indicator for problems in the code.
{quote}

That's exactly the point I've raised on the mailing-list: out of three seeds (100, 1000 and 1001), only one works. Of course, I would not dare to call that representative statistics, but I'm wondering whether or not we should be worried...","02/Feb/12 06:45;celestin;The issue about selection of an appropriate seed has been raised elsewhere. No definitive answer has been provided so far, so I suggest we consider this issue as solved for the time being."
Exceptions in genetics package or not consistent with the rest of [math],MATH-575,12507228,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,tn,psteitz,psteitz,14/May/11 16:40,24/Mar/12 16:16,20/Mar/20 20:32,02/Feb/12 11:12,2.0,2.1,2.2,,,,,3.0,,0,,,,,,,,"InvalidRepresentationException is checked and non-localized.  This exception should be placed in the [math] hierarchy.  The AbstractListChromosome constructor also throws a non-localised IAE, which should be replaced by an appropriate [math] exception.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-01-23 22:30:24.512,,,false,,,,,,,,,,,,,,,,,2219,,,Thu Feb 02 11:12:52 UTC 2012,,,,,,,"0|i0rto7:",160451,,,,,,,,,,,,,,,,"23/Jan/12 22:30;tn;Phil started to work on this issue in r1135025.

In r1235038 additional cleanups have been performed:

 - add localized messages for all exceptions
 - add @throws to javadoc where appropriate
 - add final to method parameters

What is missing:

 - Phil mentioned that InvalidRepresentationException should be placed into [math], although I am not sure why, as it is not used outside the genetics package
 - add more custom exception classes specific to the genetics package (optional). By now mostly MathIllegalArgumentException or other appropriate ones have been used.","23/Jan/12 23:16;erans;Thanks for working on this, but before you do start to make modifications, please assign the issue to yourself!

For the changes themselves, I don't agree with the creation of those many localized messages: We have been trying to rationalize and reduce the number of those, by removing duplicates and combining several ones to convey the full explanation of the problem. See my reply to the commit message.
","24/Jan/12 10:18;tn;Fixed in r1235197.

Thanks for your suggestions!","02/Feb/12 10:34;erans;Thomas,
Could please check whether this issue is resolved? And if it is, mark it so? Thanks.
","02/Feb/12 10:47;tn;As from the original issue description, Phil intended to move the InvalidRepresentationException to the general o.a.c.m.exceptions package. I am not sure about this, that's why I kept it aside for the time being. If we agree on keeping it in the genetics package we can resolve this issue.","02/Feb/12 11:08;erans;Phil had always been opposed to having all exceptions grouped in their own package; so I doubt that he meant to move that one over there... ;-)
Here, the description just indicates that the exception should become _unchecked_ and that the ""detailed message"" should be an element from the ""LocalizedFormats"" enum (i.e. derive from one of the base CM exceptions).
","02/Feb/12 11:12;tn;Ah ok, that makes it clear. When reading hierarchy I was just thinking in terms of packages rather than class hierarchy.

Thus, I resolve this issue.",,,,,,,,,,,,,,,,,,,,,,,,
MathUtils round method should propagate rather than wrap Runitme exceptions,MATH-555,12503246,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,psteitz,psteitz,04/Apr/11 04:13,24/Mar/12 16:16,20/Mar/20 20:32,04/Apr/11 04:53,2.0,2.1,2.2,,,,,3.0,,0,,,,,,,,"MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,150661,,,Mon Apr 04 04:53:13 UTC 2011,,,,,,,"0|i0rtsf:",160470,,,,,,,,,,,,,,,,"04/Apr/11 04:53;psteitz;Fixed in trunk in r1088473",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The static field ChiSquareTestImpl.distribution serves no purpose,MATH-506,12497371,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,psteitz,sebb,sebb,01/Feb/11 18:38,24/Mar/12 16:16,20/Mar/20 20:32,20/Aug/11 21:14,1.2,2.0,2.1,2.2,,,,3.0,,0,,,,,,,,"The static field ChiSquareTestImpl.distribution serves no purpose.

There is a setter for it, but in every case where the field is used, it is first overwritten with a new value.

The field and the setter should be removed, and the methods that create a new instance should create a local variable instead.

For Math 2.1, the field can be removed and the setter deprecated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-02-05 02:56:06.34,,,false,,,,,,,,,,,,,,,,,64321,,,Sat Aug 20 21:14:57 UTC 2011,,,,,,,"0|i0ru3b:",160519,,,,,,,,,,,,,,,,"05/Feb/11 02:56;psteitz;Agreed.  Since the fix for MATH-349 this instance field is unnecessary.","05/Feb/11 03:16;psteitz;See the discussion in MATH-349 where it was decided to remove the distribution pluggability in 3.0.  In 2.x, the distribution is pluggable and the instance field is useful.  The 3.0 code in trunk removes the pluggability and makes the field useless.","05/Feb/11 12:42;sebb;Sorry - I thought I had checked the 2.x implementation as well, but obviously not, as it does use the field.

However, we should still deprecate the setter in 2.2, as it is removed in 3.0 - OK?","05/Feb/11 13:23;sebb;Just tried removing the field and setter in 3.0, and found that the constructors rely on the setter (which is a separate bug, as the setter is not final - but easily fixable if required).

The fix for MATH-349 merely removed deprecated code.

It replaced ""distribution.setDegreesOfFreedom(dof)"" with ""distribution = new ChiSquaredDistributionImpl(dof)"" which is how the field became useless.

There are two constructors which still create values for the distribution field.

I don't know enough about the Math to know whether there would be any use cases for having additional methods that used a distribution provided by the class instance, rather than calculated by the individual methods (as at present).

If there is no need for external provision of the distribution degree of freedom, then the constructor with parameter can be dropped.

Otherwise, we need to add some methods that can use the provided distribution (which should be a final instance field).

In any case, I think the setter needs to be dropped from 3.x","05/Feb/11 14:52;psteitz;The instance field was there originally so that different ChiSquareDistribution implementations could be provided at construction time or via a setter (making the underlying ChiSquareDistribution pluggable).  MATH-349 pointed to a different problem related to mutability of implementation instances.  The simplest solution to both problems is to eliminate the pluggability, which the change in MATH-349 does for this class.  The degrees of freedom are always computed from the data, so there is no need for the constructor that takes a distribution instance as argument.  Both the constructor and setter can be deprecated in 2.2 and removed in 3.0 unless we want to keep pluggability, which would require

1) making the distribution field final (so removing the setter)
2) copying, rather than referencing the actual parameter provided to the constructor

I am on the fence on this.  Maybe others can chime in (next week :)","05/Feb/11 15:30;sebb;OK, I see now, thanks!","20/Aug/11 21:14;psteitz;I removed the field (hence eliminating pluggability) in r1159916.  As of 3.0, the distribution classes are immutable, so to support pluggability a factory or class name rather than a distribution instance would have to be provided.  There is only one implementation provided by [math], so I do not see this as worth the effort and complexity to retain.",,,,,,,,,,,,,,,,,,,,,,,,
Strange deprecations in API,MATH-719,12534055,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Later,,pbloem,pbloem,06/Dec/11 17:07,24/Mar/12 16:16,20/Mar/20 20:32,23/Jan/12 11:28,2.0,2.1,2.2,,,,,,,0,api-change,deprecated,,,,,,"Sorry if this doesn't belong here. I couldn't find any sort of mailing list or other feedback mechanism on the website.

RealMatrix has some very odd deprecations. In particular inverse(), getDeterminant() and isSingular(). The last has the message:

bq. Deprecated. as of release 2.0, replaced by the boolean negation of new LUDecompositionImpl(m).getSolver().isNonSingular()

That's an implementation, not an interface. The whole point of having an interface is that 
* I can query whether a matrix is singular withou having to know about LUDecompositions
* You guys can change the implementation of isSingular() if something better pops up without us guys having to change our code.

I'm not using these methods now, because they're deprecated, but I've basically recreated them in as static methods in a utility class. Wouldn't it be much better to just put code from the deprecation message into the method and remove the deprecation?

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-12-06 18:50:10.323,,,false,,,,,,,,,,,,,,,,,219781,,,Mon Jan 23 11:28:07 UTC 2012,,,,,,,"0|i0rtev:",160409,,,,,,,,,,,,,,,,"06/Dec/11 18:50;erans;bq. Sorry if this doesn't belong here.

Indeed, you'd better bring this kind of issue to the ""dev"" ML. :)
The more so that there have been recent discussions about changing the matrix API and decisions ought to be made quite soon now.
","06/Dec/11 20:07;pbloem;Ah, so there is a mailing list. I guess I should have looked a little harder. I'll bring it up there.","23/Jan/12 11:28;erans;It is unlikely that we can come up with a new design before the release of v3.0.
This must be thoroughly discussed first on the ""dev"" ML, together with other matrix interface issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect exception specification in AbstractLeastSquaresOptimizer.optimize(),MATH-443,12480321,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Duplicate,,roman.werpachowski,roman.werpachowski,18/Nov/10 10:59,24/Mar/12 16:16,20/Mar/20 20:32,20/Nov/10 21:18,2.0,2.1,,,,,,,,0,,,,,,,,"AbstractLeastSquaresOptimizer.optimize() declares the following exceptions: FunctionEvaluationException, OptimizationException, IllegalArgumentException. However, it also throws MaxIterationsExceededException. I think that instead of OptimizationException, ConvergenceException should be declared.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-11-18 19:34:20.356,,,false,,,,,,,,,,,,,,,,,150582,,,Sat Nov 20 21:18:45 UTC 2010,,,,,,,"0|i0rugf:",160578,,,,,,,,,,,,,,,,"18/Nov/10 19:34;luc;The exception hierarchy is currently completely overhauled, many exceptions have disappeared and a bunch of new exceptions have been created, many of them being unchecked exceptions.
The method you cite now throws only unchecked exceptions.
Could you have a look at the subversion repository and see if either the 2.X branch or the trunk (which is used for work on 3.0 version) do fit your needs ?
Any comments on this ongoing work before we release it is welcome.","18/Nov/10 19:39;roman.werpachowski;My personal preference is for checked exceptions, but I can live with unchecked, as long as they are documented and I know what to catch in my code.","18/Nov/10 19:46;luc;I understand.
It has been a looooong debate ;-)","20/Nov/10 21:18;luc;this is currently handled in MATH-195, which is nearly done now.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent API in Frequency,MATH-260,12422791,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,sebb,sebb,14/Apr/09 13:38,14/Apr/10 00:39,20/Mar/20 20:32,30/Dec/09 20:12,2.0,,,,,,,2.1,,0,,,,,,,,"The overloaded Frequency methods are not consistent in the parameter types that they handle.

addValue() has an Integer version which converts the parameter to a Long, and then calls addValue(Object).

The various getxxx() methods all handle Integer parameters as an Object.

Seems to me that it would be better to treat Integer consistently.

But perhaps there is a good reason for having an addValue(Integer) method but no getxxx(Integer) methods?
If so, then it would be helpful to document this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-04-15 01:14:36.248,,,false,,,,,,,,,,,,,,,,,150462,,,Wed Dec 30 20:12:52 UTC 2009,,,,,,,"0|i0rvjr:",160755,,,,,,,,,,,,,,,,"15/Apr/09 01:14;psteitz;the getXxx methods handle Integer arguments by converting them to longs.  See, eg. the test case, testIntegerValues()","15/Apr/09 10:23;sebb;Yes, I know that the getXXX(Object) methods handle Integer specially; but so does the addValue(Object) method - it has to, in case an Integer is passed as an Object.

But why is there an addValue(Integer) method? What does it achieve?
There are no corresponding getXXX(Integer) methods.","27/Dec/09 17:56;luc;Note: the addValue(Object) has been deprecated as of 2.0 and replaced by addValue(Comparable).
I think we could also deprecate or even remove addValue(Integer) since addValue(Comparable) is sufficient for all purposes. Removing the method would probably not harm any users if they can recompile their code (but it would harm if they cannot recompile, of course).","30/Dec/09 20:12;psteitz;addValue(Integer) has been deprecated, marked for removal in 3.0",,,,,,,,,,,,,,,,,,,,,,,,,,,
nextExponential parameter check bug - patch supplied,MATH-309,12439175,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,mikl,mikl,27/Oct/09 13:20,03/Apr/10 20:52,20/Mar/20 20:32,31/Oct/09 02:33,1.0,1.1,1.2,2.0,,,,2.1,,0,,,,,,,,"Index: src/main/java/org/apache/commons/math/random/RandomDataImpl.java
===================================================================
--- src/main/java/org/apache/commons/math/random/RandomDataImpl.java	(revision 830102)
+++ src/main/java/org/apache/commons/math/random/RandomDataImpl.java	(working copy)
@@ -462,7 +462,7 @@
      * @return the random Exponential value
      */
     public double nextExponential(double mean) {
-        if (mean < 0.0) {
+        if (mean <= 0.0) {
             throw MathRuntimeException.createIllegalArgumentException(
                   ""mean must be positive ({0})"", mean);
         }",Ubuntu 9.04,300,300,,0%,300,300,,,,,,"27/Oct/09 13:21;mikl;patch_random_exp;https://issues.apache.org/jira/secure/attachment/12423317/patch_random_exp",,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2009-10-31 02:33:15.66,,,false,,,,,,,,,,,,,,,,,34214,,,Sat Oct 31 02:33:15 UTC 2009,,,,,,,"0|i0rv93:",160707,,,,,,,,,,,,,,,,"31/Oct/09 02:33;psteitz;Fixed in r831510.  Thanks for reporting this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable) ",MATH-329,12445486,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,williausrohr,williausrohr,14/Jan/10 08:01,03/Apr/10 20:35,20/Mar/20 20:32,16/Jan/10 20:02,2.0,,,,,,,2.1,,0,,,,,,,,"Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change

Frequency.java

   /**
      * Returns the percentage of values that are equal to v
     * @deprecated replaced by {@link #getPct(Comparable)} as of 2.0
     */
    @Deprecated
    public double getPct(Object v) {
        return getCumPct((Comparable<?>) v);
    }",,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-01-16 20:02:15.55,,,false,,,,,,,,,,,,,,,,,34195,,,Sat Jan 16 20:02:15 UTC 2010,,,,,,,"0|i0rv4v:",160688,,,,,,,,,,,,,,,,"16/Jan/10 20:02;psteitz;Fixed in r900016.  Thanks for reporting this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultipleLinearRegression - test for minimum number of samples,MATH-279,12428442,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,luc,mbjorkegren,mbjorkegren,20/Jun/09 07:13,07/Aug/09 09:18,20/Mar/20 20:32,05/Jul/09 13:31,2.0,,,,,,,2.0,,0,,,,,,,,"It's currently possible to pass in so few rows (samples) that there isn't a row for each column (predictor).  Does this look like the right thing to do?

{code}
Index: AbstractMultipleLinearRegression.java
===================================================================
--- AbstractMultipleLinearRegression.java       (revision 786758)
+++ AbstractMultipleLinearRegression.java       (working copy)
@@ -91,6 +91,9 @@
                   ""dimension mismatch {0} != {1}"",
                   (x == null) ? 0 : x.length,
                   (y == null) ? 0 : y.length);
+        } else if (x[0].length > x.length){
+            throw MathRuntimeException.createIllegalArgumentException(
+                    ""not enough data ("" + x.length + "" rows) for this many predictors ("" + x[0].length + "" predictors)"");
         }
     }
 {code}

",,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-07-05 13:31:29.287,,,false,,,,,,,,,,,,,,,,,150478,,,Fri Aug 07 09:18:13 UTC 2009,,,,,,,"0|i0rvfj:",160736,,,,,,,,,,,,,,,,"05/Jul/09 13:31;luc;fixed in subversion repository as of r791244
patch was only slightly modified to avoid a NPE
thanks for the patch","07/Aug/09 09:18;luc;closing resolved issue for 2.0 release",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testing for symmetric positive definite matrix in CholeskyDecomposition,MATH-274,12427143,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,luc,rossetti,rossetti,04/Jun/09 19:24,07/Aug/09 09:16,20/Mar/20 20:32,04/Jun/09 20:11,2.0,,,,,,,2.0,,0,,,,,,,,"I used this matrix:

        double[][] cv = {
            {0.40434286, 0.09376327, 0.30328980, 0.04909388},
            {0.09376327, 0.10400408, 0.07137959, 0.04762857},
            {0.30328980, 0.07137959, 0.30458776, 0.04882449},
            {0.04909388, 0.04762857, 0.04882449, 0.07543265}
        };

And it works fine, because it is symmetric positive definite

I tried this matrix:

        double[][] cv = {
            {0.40434286, -0.09376327, 0.30328980, 0.04909388},
            {-0.09376327, 0.10400408, 0.07137959, 0.04762857},
            {0.30328980, 0.07137959, 0.30458776, 0.04882449},
            {0.04909388, 0.04762857, 0.04882449, 0.07543265}
        };

And it should throw an exception but it does not.  I tested the matrix in R and R's cholesky decomposition method returns that the matrix is not symmetric positive definite.

Obviously your code is not catching this appropriately.

By the way (in my opinion) the use of exceptions to check these conditions is not the best design or use for exceptions.  If you are going to force the use to try and catch these exceptions at least provide methods  to test the conditions prior to the possibility of the exception.  

","Mac OS X, NetBeans",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-06-04 20:11:26.076,,,false,,,,,,,,,,,,,,,,,34201,,,Fri Aug 07 09:16:16 UTC 2009,,,,,,,"0|i0rvgn:",160741,,,,,,,,,,,,,,,,"04/Jun/09 20:11;luc;fixed in subversion repository as of r 781845
Concerning the exception, it is not possible to check the matrix without trying to decompose it, so providing an external check would be a waste as it would already do almost everything. In fact, it was exactly the reason for the bug: the check was done too early on the raw matrix, not on the matrix after some changes have been made to its elements.
thanks for the report","04/Jun/09 20:57;rossetti;Luc,

Thanks for fixing the error.

You are missing my point.  You should provide the user the opportunity  
to check the condition, rather than having to rely on exceptions.  The  
client can make the decision whether it is a waste or not.

And, we will agree to disagree on how the library is using exceptions.

Regards,
Manuel



-----------------------------------------------------
Manuel D. Rossetti, Ph.D., P.E.
Associate Professor of Industrial Engineering
University of Arkansas
Department of Industrial Engineering
4207 Bell Engineering Center
Fayetteville, AR 72701
Phone: (479) 575-6756
Fax: (479) 575-8431
email: rossetti@uark.edu
www: www.uark.edu/~rossetti



","07/Aug/09 09:16;luc;closing resolved issue for 2.0 release",,,,,,,,,,,,,,,,,,,,,,,,,,,,
MathUtils.factorial(n) fails for n >= 17,MATH-240,12412638,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,psteitz,chsemrau,chsemrau,16/Jan/09 22:43,23/Apr/09 02:26,20/Mar/20 20:32,19/Jan/09 19:43,2.0,,,,,,,2.0,,0,,,,,,,,"The result of MathUtils.factorial( n ) for n = 17, 18, 19 is wrong, probably because of rounding errors in the double calculations.

Replace the first line of MathUtilsTest.testFactorial() by

        for (int i = 1; i <= 20; i++) {

to check all valid arguments for the long result and see the failure.

I suggest implementing a simple loop to multiply the long result - or even using a precomputed long[] - instead of adding logarithms.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-01-19 19:43:52.673,,,false,,,,,,,,,,,,,,,,,34155,,,Mon Jan 19 19:43:52 UTC 2009,,,,,,,"0|i0rvo7:",160775,,,,,,,,,,,,,,,,"19/Jan/09 19:43;psteitz;Thanks for reporting this.  Fixed in r735781.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MathUtils.gcd(u, v) fails when u and v both contain a high power of 2",MATH-238,12412633,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,chsemrau,chsemrau,16/Jan/09 22:24,23/Apr/09 02:26,20/Mar/20 20:32,16/Jan/09 23:07,2.0,,,,,,,,,0,,,,,,,,"The test at the beginning of MathUtils.gcd(u, v) for arguments equal to zero fails when u and v contain high enough powers of 2 so that their product overflows to zero.

        assertEquals(3 * (1<<15), MathUtils.gcd(3 * (1<<20), 9 * (1<<15)));

Fix: Replace the test at the start of MathUtils.gcd()

        if (u * v == 0) {

by

        if (u == 0 || v == 0) {
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2009-01-16 23:07:17.149,,,false,,,,,,,,,,,,,,,,,34222,,,Fri Jan 16 23:07:17 UTC 2009,,,,,,,"0|i0rvon:",160777,,,,,,,,,,,,,,,,"16/Jan/09 23:07;luc;fixed in trunk as of r73517

thanks for the report",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
